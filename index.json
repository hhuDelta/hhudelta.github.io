[{"authors":["Baoxi Jia"],"categories":null,"content":"I received my B.E. degree in compute artificial intelligence from Qufu normal University in 2024. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few-shot Learning and Disentangled Representation Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0480a8f6c1877e7d4a8da99090e57f5a","permalink":"https://hhudelta.github.io/author/baoxi-jia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/baoxi-jia/","section":"authors","summary":"I received my B.E. degree in compute artificial intelligence from Qufu normal University in 2024. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few-shot Learning and Disentangled Representation Learning.\n","tags":null,"title":"Baoxi Jia","type":"authors"},{"authors":["Guang Yang"],"categories":null,"content":"I received my B.E.degree in Jiangsu University of Science and Technology in 2024. I am currently working toward M.E.degree as amember of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include LLM and Few-Shot Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"193cb2e91ba01e00601634748311c026","permalink":"https://hhudelta.github.io/author/guang-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guang-yang/","section":"authors","summary":"I received my B.E.degree in Jiangsu University of Science and Technology in 2024. I am currently working toward M.E.degree as amember of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include LLM and Few-Shot Learning.\n","tags":null,"title":"Guang Yang","type":"authors"},{"authors":["Hongyi Lu"],"categories":null,"content":"I am a 2023 undergraduate graduate of Hohai University majoring in Electronic Information Engineering. I am applying for a master’s degree in Electronic Information Computer Science and am very interested in computer vision and image segmentation. Therefore, I joined Professor Wu Yirui’s research group.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8d69f871b347160e9a6ba8fdf442c995","permalink":"https://hhudelta.github.io/author/hongyi-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hongyi-lu/","section":"authors","summary":"I am a 2023 undergraduate graduate of Hohai University majoring in Electronic Information Engineering. I am applying for a master’s degree in Electronic Information Computer Science and am very interested in computer vision and image segmentation. Therefore, I joined Professor Wu Yirui’s research group.\n","tags":null,"title":"Hongyi Lu","type":"authors"},{"authors":["Fuchen Ma"],"categories":null,"content":"I received my B.E. degree in Computer Science and Technology from Hohai University in 2024. l am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests is Machine Unlearning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dc8b8561264cbfca141c0966aff8b0ce","permalink":"https://hhudelta.github.io/author/fuchen-ma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/fuchen-ma/","section":"authors","summary":"I received my B.E. degree in Computer Science and Technology from Hohai University in 2024. l am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests is Machine Unlearning.\n","tags":null,"title":"Fuchen Ma","type":"authors"},{"authors":["Hao Wang"],"categories":null,"content":"I received my B.E. degree in Computer Science and Technology from Hohai University in 2024. l am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests is semantic segmentation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"81312ddab37221f82f5b8600183cf2e7","permalink":"https://hhudelta.github.io/author/hao-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hao-wang/","section":"authors","summary":"I received my B.E. degree in Computer Science and Technology from Hohai University in 2024. l am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests is semantic segmentation.\n","tags":null,"title":"Hao Wang","type":"authors"},{"authors":["Ziye Wang"],"categories":null,"content":"I received my B.E. degree in Computer Science and Technology from Hohai University in 2024. l am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests is Transfer Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7e1e1c851319c8ddab53c174133dccfe","permalink":"https://hhudelta.github.io/author/ziye-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ziye-wang/","section":"authors","summary":"I received my B.E. degree in Computer Science and Technology from Hohai University in 2024. l am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests is Transfer Learning.\n","tags":null,"title":"Ziye Wang","type":"authors"},{"authors":["Yuhang Xia"],"categories":null,"content":"I received Bachelor of Management degree in Engineering Management from Nanjing Audit University in 2019. I am currently studying for my Master’s degree and i am a member of the Delta Lab under Professor Yirui Wu.\n","date":1733788800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1733788800,"objectID":"946465037dee3da1b9db6783121c63e7","permalink":"https://hhudelta.github.io/author/yuhang-xia/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/author/yuhang-xia/","section":"authors","summary":"I received Bachelor of Management degree in Engineering Management from Nanjing Audit University in 2019. I am currently studying for my Master’s degree and i am a member of the Delta Lab under Professor Yirui Wu.\n","tags":null,"title":"Yuhang Xia","type":"authors"},{"authors":["Ao Geng"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Anhui University of Science and Technology in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few-shot Learning and object detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"eb9e895946f1c2290c8907b9675566d5","permalink":"https://hhudelta.github.io/author/ao-geng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ao-geng/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Anhui University of Science and Technology in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few-shot Learning and object detection.\n","tags":null,"title":"Ao Geng","type":"authors"},{"authors":["Cheng Zhen"],"categories":null,"content":"I received my B.E. degree in Software Engineering from Jiangsu University of Science and Technology in 2023. I am currently studying for my Master’s degree and am a member of the DeltaV Lab under Professor Yirui Wu. I often post my projects on Github. If you are interested, you can communicate with me through email or post an issue.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d361335ccb1bfb38be467b6f4a90bd30","permalink":"https://hhudelta.github.io/author/cheng-zhen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cheng-zhen/","section":"authors","summary":"I received my B.E. degree in Software Engineering from Jiangsu University of Science and Technology in 2023. I am currently studying for my Master’s degree and am a member of the DeltaV Lab under Professor Yirui Wu. I often post my projects on Github. If you are interested, you can communicate with me through email or post an issue.\n","tags":null,"title":"Cheng Zhen","type":"authors"},{"authors":["Jianzhou Wang"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Nanjing Forestry University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Transformer and Incremental Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6a713745217eb980fa430474cb55490e","permalink":"https://hhudelta.github.io/author/jianzhou-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianzhou-wang/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Nanjing Forestry University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Transformer and Incremental Learning.\n","tags":null,"title":"Jianzhou Wang","type":"authors"},{"authors":["Shijia Qiao"],"categories":null,"content":"I received my B.E. degree in Computer Science and Technology from North China University of Water Resources and Electric Power in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests include Forgetting learning and causal inference.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"33f2eba52c39e990cb01cff17ade1e25","permalink":"https://hhudelta.github.io/author/shijia-qiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shijia-qiao/","section":"authors","summary":"I received my B.E. degree in Computer Science and Technology from North China University of Water Resources and Electric Power in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests include Forgetting learning and causal inference.\n","tags":null,"title":"Shijia Qiao","type":"authors"},{"authors":["Xiu Jin"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Nanjing Tech University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e3c7b59182007f56c34ea2f9d3d4fcaa","permalink":"https://hhudelta.github.io/author/xiu-jin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiu-jin/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Nanjing Tech University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu.\n","tags":null,"title":"Xiu Jin","type":"authors"},{"authors":["Haiyan Sun"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Shandong Normal University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few shot Learning and Graph Neural Network. I think machines can take a lot of clues from 2D images and make inferences based on them, just like Sherlock Holmes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"837809b6d322677349d14c66b1d93271","permalink":"https://hhudelta.github.io/author/haiyan-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haiyan-sun/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Shandong Normal University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few shot Learning and Graph Neural Network. I think machines can take a lot of clues from 2D images and make inferences based on them, just like Sherlock Holmes.\n","tags":null,"title":"Haiyan Sun","type":"authors"},{"authors":["Rui Qin"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Nanjing Tech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include 3D Object Detection and Transformer. You can also follow me on CSDN website with ID 偷走心灵的告白 or Github website with ID QinRui666.\n","date":1723593600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1723593600,"objectID":"6f60bcb8f7961dc317a6a6e4821e33d2","permalink":"https://hhudelta.github.io/author/rui-qin/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/author/rui-qin/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Nanjing Tech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include 3D Object Detection and Transformer. You can also follow me on CSDN website with ID 偷走心灵的告白 or Github website with ID QinRui666.\n","tags":null,"title":"Rui Qin","type":"authors"},{"authors":["Yuting Zhou"],"categories":null,"content":"I received my B.E. degree in computer science and technology from NingboTech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. You can also follow me on CSDN website with ID zyt131415 or Github website with ID zyt0211.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2cbc95840e7b3ff5af192d5451c241d2","permalink":"https://hhudelta.github.io/author/yuting-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuting-zhou/","section":"authors","summary":"I received my B.E. degree in computer science and technology from NingboTech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. You can also follow me on CSDN website with ID zyt131415 or Github website with ID zyt0211.\n","tags":null,"title":"Yuting Zhou","type":"authors"},{"authors":["Tianyu Ni"],"categories":null,"content":"I received my B.E. degree in computer science and technology from NingboTech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"230dbc5cae3760ea0ca2d4504dce0b74","permalink":"https://hhudelta.github.io/author/tianyu-ni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianyu-ni/","section":"authors","summary":"I received my B.E. degree in computer science and technology from NingboTech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu.\n","tags":null,"title":"Tianyu Ni","type":"authors"},{"authors":["Xinfu Liu"],"categories":null,"content":"I received my M.E. degree in computer application technology from Yunnan University in 2022. I am currently working toward Ph.D as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include image segmentation, few-shot learning and incremental learning.\n","date":1723593600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1723593600,"objectID":"42e8adca1a95e9e148efaa23e536b35d","permalink":"https://hhudelta.github.io/author/xinfu-liu/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/author/xinfu-liu/","section":"authors","summary":"I received my M.E. degree in computer application technology from Yunnan University in 2022. I am currently working toward Ph.D as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include image segmentation, few-shot learning and incremental learning.\n","tags":null,"title":"Xinfu Liu","type":"authors"},{"authors":["Yirui Wu"],"categories":null,"content":"I am currently a Young Professor at Hohai University, working as member of Hydrology Big Data Group and leader of Delta Lab. I have been honored as Dayu Scholar of Hohai University. Before coming to Hohai, I obtained my Ph.D degree from Nanjing University in 2016. I received my B.S. Degree from Nanjing University in 2011 as well. During my Ph. D study, I was with the IMAGE Lab under the supervision of Prof. Tong Lu and worked closely with Prof. Shivakumara Palaiahnakote. I visited HongKong University of Science and Technology twice in 2012 and 2014, supervised by Prof. Chiew-Lan Tai and Dr. Oscar Kin-Chung Au. I’ve published more than 40 journal/conference papers on Computer Vision, Pattern Recognition, Multimedia, and Intelligent Water Conservancy, including IEEE Transactions on Image Processing, IEEE Transactions on Multimedia, IEEE Transactions on Industrial Informatics, ACM MM, ICME, BIBM, ICPR, ICDAR, ICIP, and others.\nI am currently serving as Associate Editor for IET Image Processing(SCI, CCF-C), the first Foreign Associate Editor for Malaysian Journal of Computer Science(SCI), and the founding Associate Editor for Artificial Intelligence and Applications. I won two International Conference Best Paper Awards. I was also elected as Vice Chairman of CCF YOCSEF Nanjing, CCF-MM Executive Committee Member and Vice secretary for Multimedia Commission of Jiangsu Information Technology Federation. I severed as guest editors for Frontiers in Plant Science (IF:6.627, JCR Q1), Journal of Sensor and Actuator Network (JCR Q1, ESCI), and PC members for ICML2022(CCF-A, Top Conf in Artificial Intelligence), ACM MM2022/2021(CCF-A, Top Conf in Multimedia), ICPR2022(CCF-C), ICME2022(CCF-B), HPCC2021(CCF-C), CIKM2021(CCF-B), ACPR2021 and others.\nYou can view my full information in my CV. You can also follow me on Zhihu website with ID 河海大学巫义锐 or Scholat website with ID 巫义锐.\n","date":1733788800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1733788800,"objectID":"3ad653018393b0089837827547e6926f","permalink":"https://hhudelta.github.io/author/yirui-wu/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/author/yirui-wu/","section":"authors","summary":"I am currently a Young Professor at Hohai University, working as member of Hydrology Big Data Group and leader of Delta Lab. I have been honored as Dayu Scholar of Hohai University. Before coming to Hohai, I obtained my Ph.D degree from Nanjing University in 2016. I received my B.S. Degree from Nanjing University in 2011 as well. During my Ph. D study, I was with the IMAGE Lab under the supervision of Prof. Tong Lu and worked closely with Prof. Shivakumara Palaiahnakote. I visited HongKong University of Science and Technology twice in 2012 and 2014, supervised by Prof. Chiew-Lan Tai and Dr. Oscar Kin-Chung Au. I’ve published more than 40 journal/conference papers on Computer Vision, Pattern Recognition, Multimedia, and Intelligent Water Conservancy, including IEEE Transactions on Image Processing, IEEE Transactions on Multimedia, IEEE Transactions on Industrial Informatics, ACM MM, ICME, BIBM, ICPR, ICDAR, ICIP, and others.\n","tags":null,"title":"Yirui Wu","type":"authors"},{"authors":null,"categories":null,"content":"I am currently a lecturer at Hohai University, working as a member of Delta Lab. Before coming to Hohai, I obtained my Ph.D degree from Nanjing University in 2024. I received my M.S. degree from Hohai University in 2018, and I received my B.S. degree from Xidian University in 2014. My publications appear in IEEE Transactions on Image Processing, Pattern Recognition, and IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. My current research interests include pattern recognition, machine learning, and image processing.\n","date":1733788800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1733788800,"objectID":"917db75948c8a40ff81d64c9def7c967","permalink":"https://hhudelta.github.io/author/lixin-yuan/","publishdate":"2024-12-18T16:43:55.0093Z","relpermalink":"/author/lixin-yuan/","section":"authors","summary":"I am currently a lecturer at Hohai University, working as a member of Delta Lab. Before coming to Hohai, I obtained my Ph.D degree from Nanjing University in 2024. I received my M.S. degree from Hohai University in 2018, and I received my B.S. degree from Xidian University in 2014. My publications appear in IEEE Transactions on Image Processing, Pattern Recognition, and IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. My current research interests include pattern recognition, machine learning, and image processing.\n","tags":null,"title":"Lixin Yuan","type":"authors"},{"authors":null,"categories":null,"content":"春：数字图像处理 智能本科生\n软件学报模板下载\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"bf74911ecc461138676a4bba67bf08b3","permalink":"https://hhudelta.github.io/teaching/classaiunder/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/teaching/classaiunder/","section":"teaching","summary":"智能本科生","tags":null,"title":"春：数字图像处理","type":"book"},{"authors":null,"categories":null,"content":" 时间安排 11月13日：小作业、大作业确定题目；大作业演讲次序确定 11月18日：开始讲演（每位讲演完的同学，请将修改后的PPT发送至我的邮箱） 12月12日 凌晨 0 点：小作业截止时间 小作业 作业简介 从以下题目中自选一题，根据相关资料与自己的理解，形成篇幅在 10 页左右的综述论文。\n题目选择 小样本学习的研究及其应用\n论文：A CLOSER LOOK AT FEW-SHOT CLASSIFICATION\nGAN 模型的研究及其应用\n论文：Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\nLSTM 模型的研究及其应用\n论文：Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\n作业要求 论文的格式需遵循软件学报模板 论文内容详实，章节中至少应包括： 模型简介 数学推导 相关工作及应用举例 自己对该类模型的见解 论文不得有明显抄袭痕迹。 可额外添加 算法实现及性能分析 部分，该部分将加分。 软件学报模板下载\n大作业 作业简介 从最近两年的 CVPR, ICCV, ECCV, ICLR 论文中自选一题，根据相关资料与自己的理解，形成一次 8 分钟左右的主题报告。\n作业要求 报告形式需遵循 会议口头宣讲流程。 注意掌握时间，超时将被打断。 宣讲内容详实，有自己对该课题的见解，无明显抄袭痕迹。 宣讲完成后，会有 提问环节，提问回答质量计入分数。 联系方式 如对作业提交情况有疑问，请通过邮件 wuyirui@hhu.edu.cn 联系我。\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"80b31b14204c379f329a071e934885e3","permalink":"https://hhudelta.github.io/teaching/classcspost/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/teaching/classcspost/","section":"teaching","summary":"计算机硕士","tags":null,"title":"秋：数字图像处理","type":"book"},{"authors":null,"categories":null,"content":"秋：数字图像处理 计算机本科生\n软件学报模板下载\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"356fd11400a89a285a031c17021ffe02","permalink":"https://hhudelta.github.io/teaching/classcsunder/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/teaching/classcsunder/","section":"teaching","summary":"计算机本科生","tags":null,"title":"秋：数字图像处理","type":"book"},{"authors":null,"categories":null,"content":"秋：水文大数据分析 水文博士生\n","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599609600,"objectID":"b9cd983fc0cbfa90b191581b011c685f","permalink":"https://hhudelta.github.io/teaching/classhydrologyphd/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/teaching/classhydrologyphd/","section":"teaching","summary":"水文博士生","tags":null,"title":"秋：水文大数据分析","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"e72573f1962a907c3c65913112c3e3d1","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt1/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt1/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"ab3d30bae65cc0f60b0631d4c60b390d","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt1/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt1/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"214b65b1755df0854c7bc11609aac2f8","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt1/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt1/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"dd8fc1617d6d36e42cba13546286e26f","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt2/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt2/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"8a4d03b8816ff28e4dd7d89ac340084e","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt2/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt2/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"0fd5a0c1fc213f1bb954aa2b7e41e9a8","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt2/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt2/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"c1edd4ea9ff698f945c4cdf070d36a02","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt3/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt3/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"a21f46b2d16497ddc7e8f4732dcb91fa","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt3/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt3/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"c71a8ba03d9bcaae97af43c241b210ad","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt3/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt3/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"f4fd23dd6e81d61ffba3336e1c90be51","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt4/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt4/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"7e5ca946163cc40509faa4ea55380e70","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt4/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt4/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"94f644f2186128ef604a6c2b7871b742","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt4/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt4/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"9c2cb8efc5e4d0d0b2fbca0780caaefd","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt5/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt5/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第五章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"8d27fe5fe78e095c52b63f120a773c17","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt5/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt5/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第五章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"a20e48d5733d5730591b3fd00ca07f75","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt5/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt5/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第五章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"6d8f1f517ddabca542e5dbad226e7311","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt6/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt6/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第六章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"741063cf75af6a94f6685124fe1a74d2","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt6/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt6/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第六章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"96ee827d427ecb9fad6f88582b9242f2","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt6/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt6/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第六章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"c18800295c89036623de5043f64c8e7d","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt7/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt7/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第七章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"4f965a8be65a85da892fcc36638559db","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt7/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt7/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第七章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"eb5eaca7abf3d2ea844ab8bbde0696e9","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt7/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt7/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第七章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"b8fc18cd6545b2c73dce229b985cd0c5","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt8/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt8/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第八章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"f099a64aeb4f531e6a04d1c75d515c5a","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt8/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt8/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第八章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"72233040da8a4d3af07663ef721712e5","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt8/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt8/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第八章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"0609c9def0a07b4797fa55a8a12a36a5","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt9/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt9/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第九章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"2b0bc3c2b0fba25e1802123b8dc2998b","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt9/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt9/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第九章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"fa08616ef4e423dce172e15c77d2b89a","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt9/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt9/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第九章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"9af0f7343234f4f2117fe222ec1d4b42","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt10/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt10/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"306b196232382c22287b5cefa1ed3daf","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt10/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt10/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"7df2c229937d2a81c489ea3b66f62eb3","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt10/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt10/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"bde094402d5a92b74e05d90739888398","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt11/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt11/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"cfbb05d3e445672d53166cb83da38005","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt11/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt11/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"1cfc188750d3700ed6a9f954f425bbb9","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt11/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt11/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"d228c83afa120bbec5a4869b8c744748","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt12/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt12/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"caa52b010a828113f02e39b9b2233931","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt12/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt12/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"b21c8e32aec33237d671e91f8de06aab","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt12/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt12/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"c6a6eff45479711484caea52b271f02c","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt13/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt13/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"fe3feb57b1a12020c8043e1255ecb5e4","permalink":"https://hhudelta.github.io/teaching/classcspost/chapt13/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/chapt13/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"33938a256ee9d6e5e6c9c8d30a1495a1","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt13/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt13/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"a33cddaa1e14aa18af09f25e972b5abe","permalink":"https://hhudelta.github.io/teaching/classcspost/svm/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcspost/svm/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"SVM","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"3cbdb3c3eb13a4b2e574a2229f4b9086","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt14/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt14/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"8e163ab5cc7aef0a792b187300875c16","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt14/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt14/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"e05c51682b1c43b5802b37ca1a8009ff","permalink":"https://hhudelta.github.io/teaching/classaiunder/chapt15/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/chapt15/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十五章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"6bc610e2f73a00fcca10a6ac8bb93635","permalink":"https://hhudelta.github.io/teaching/classcsunder/chapt15/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/chapt15/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十五章","type":"book"},{"authors":null,"categories":null,"content":"第一次小作业布置及如何使用河海查重系统\nDownload PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"2706093915d749b616c56ed9ad2dbc21","permalink":"https://hhudelta.github.io/teaching/classaiunder/homework/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/homework/","section":"teaching","summary":"第一次小作业布置及如何使用河海查重系统\nDownload PPT\n","tags":null,"title":"小作业","type":"book"},{"authors":null,"categories":null,"content":"第一次小作业布置及如何使用河海查重系统\nDownload PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"bc57775ea44ecd6e9750bba512d204af","permalink":"https://hhudelta.github.io/teaching/classcsunder/homework/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/homework/","section":"teaching","summary":"第一次小作业布置及如何使用河海查重系统\nDownload PPT\n","tags":null,"title":"小作业","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"9fef8c38be8d202eca48fe45b9495d93","permalink":"https://hhudelta.github.io/teaching/classaiunder/paper/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classaiunder/paper/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"如何写好一篇科技论文","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"dbc4f056b0aeef95bb5ebbb35d9f6b82","permalink":"https://hhudelta.github.io/teaching/classcsunder/paper/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/teaching/classcsunder/paper/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"如何写好一篇科技论文","type":"book"},{"authors":null,"categories":null,"content":"AAAI 2025 (CCF-A, Top Conference in Artificial Intelligence)\nPaper Title: Deconfound Semantic Shift and Incompleteness in Incremental Few-shot Semantic Segmentation\nPaper Code:https://github.com/LegendSherlock/Deconfound-IFSS Abstract: Incremental Few-Shot Semantic Segmentation (IFSS) extends the segmentation capability of trained models, enabling them to segment images of new classes with few samples. However, during the incremental learning process, semantics may shift between background and object classes or vice versa. Additionally, when new classes differ significantly from pre-learned old classes, samples from new classes often lack representative feature attributes. In this paper, we propose a causal framework to discuss the causes of semantic shifts and incompleteness in IFSS and eliminate the revealed causal effects from two aspects. First, we introduce a Causal Intervention Module (CIM) to resist semantic shifts. CIM gradually and adaptively updates the prototypes of old classes, intervening to remove confounding factors. Secondly, we propose a Prototype Refinement Module (PRM) to complete missing semantics. In the PRM, knowledge obtained from the scene learning scheme helps integrate the features of new and old class prototypes. Experiments on the PASCAL-VOC 2012 and ADE20k benchmarks demonstrate the superior performance of our method. ","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"8bcca0be37a1453dcbeb0a02da88635d","permalink":"https://hhudelta.github.io/post/24-12-10/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/post/24-12-10/","section":"post","summary":"AAAI 2025 (CCF-A, Top Conference in Artificial Intelligence)\nPaper Title: Deconfound Semantic Shift and Incompleteness in Incremental Few-shot Semantic Segmentation\n","tags":null,"title":"Congratulations to Hao Li and Yuhang Xia on Their AAAI 2025 (CCF-A) Paper Acceptance.","type":"post"},{"authors":["Yirui Wu","Yuhang Xia","Hao Li","Lixin Yuan","Junyang Chen","Jun Liu","Tong Lu","Shaohua Wan"],"categories":null,"content":"","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"0693bce18e6b7c0be334e386ab737a4c","permalink":"https://hhudelta.github.io/publication/aaai2025/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/publication/aaai2025/","section":"publication","summary":"Incremental few-shot semantic segmentation (IFSS) expands segmentation capacity of the trained model to segment new class images with few samples. However, semantic meanings may shift from background to object class or vice versa dur ing incremental learning. Moreover, new-class samples of ten lack representative attribute features when the new class greatly differs from the pre-learned old class. In this paper, we propose a causal framework to discuss the cause of semantic shift and incompleteness in IFSS, and we deconfound the revealed causal effects from two aspects. First, we propose a Causal Intervention Module (CIM) to resist semantic shift. CIM progressively and adaptively updates prototypes of old class, and removes the confounder in an intervention manner. Second, a Prototype Refinement Module (PRM) is proposed to complete the missing semantics. In PRM, knowledge gained from the episode learning scheme assists in fusing fea tures of new-class and old-class prototypes. Experiments on both PASCAL-VOC 2012 and ADE20k benchmarks demon strate the outstanding performance of our method.","tags":[],"title":"Deconfound Semantic Shift and Incompleteness in Incremental Few-shot Semantic Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"Computational Intelligence is a journal in the field of artificial intelligence that publishes original research on a wide range of experimental and theoretical topics in AI and computer science. The journal has a broad scope, covering areas such as machine learning, knowledge mining, network intelligence, artificial intelligence languages, and the philosophical impacts of AI. It is read by both researchers in the academic AI community and professionals in the industry.\nWe welcome submissions from teachers and students!!!\n","date":1733356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733356800,"objectID":"11681d9cb89b1e7694482351eb1c8da5","permalink":"https://hhudelta.github.io/post/24-12-05/","publishdate":"2024-12-05T00:00:00Z","relpermalink":"/post/24-12-05/","section":"post","summary":"Computational Intelligence is a journal in the field of artificial intelligence that publishes original research on a wide range of experimental and theoretical topics in AI and computer science. The journal has a broad scope, covering areas such as machine learning, knowledge mining, network intelligence, artificial intelligence languages, and the philosophical impacts of AI. It is read by both researchers in the academic AI community and professionals in the industry.\n","tags":null,"title":"Professor Yirui Wu Invited to Serve as Associate Editor for Computational Intelligence (CCF-C)","type":"post"},{"authors":null,"categories":null,"content":"Professor Yirui Wu from our lab was awarded the title of IEEE Senior Member\n","date":1733356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733356800,"objectID":"e9c6995fcf777bf4b9fa62a561db286c","permalink":"https://hhudelta.github.io/post/24-12-05-1/","publishdate":"2024-12-05T00:00:00Z","relpermalink":"/post/24-12-05-1/","section":"post","summary":"Professor Yirui Wu from our lab was awarded the title of IEEE Senior Member\n","tags":null,"title":"Professor Yirui Wu was awarded the title of IEEE Senior Member","type":"post"},{"authors":null,"categories":null,"content":"The special issue “Advances in Few-Shot Learning with Multimodal Large Models,” co-organized by Professor Wu Yirui from our lab and Professor Wan Shaohua from the University of Electronic Science and Technology of China, has been launched on Applied Sciences. The submission deadline is March 20, 2025. For more details, please visit https://www.mdpi.com/journal/applsci/special_issues/3N827RPJSC. We warmly invite all faculty and students to submit their papers.\n","date":1733011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733011200,"objectID":"74266124a199a84608d611d0af72d3d7","permalink":"https://hhudelta.github.io/post/24-12-01/","publishdate":"2024-12-01T00:00:00Z","relpermalink":"/post/24-12-01/","section":"post","summary":"The special issue “Advances in Few-Shot Learning with Multimodal Large Models,” co-organized by Professor Wu Yirui from our lab and Professor Wan Shaohua from the University of Electronic Science and Technology of China, has been launched on Applied Sciences. The submission deadline is March 20, 2025. For more details, please visit https://www.mdpi.com/journal/applsci/special_issues/3N827RPJSC. We warmly invite all faculty and students to submit their papers.\n","tags":null,"title":"Call for Papers: Special Issue on Few-Shot Learning (Applied Sciences-Basel, CAS Q3)","type":"post"},{"authors":null,"categories":null,"content":" ","date":1732665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732665600,"objectID":"e7666821138f6d11c7f76e14e1403edc","permalink":"https://hhudelta.github.io/post/24-11-27/","publishdate":"2024-11-27T00:00:00Z","relpermalink":"/post/24-11-27/","section":"post","summary":"","tags":null,"title":"Congratulations to student Guangchen Shi on winning the Outstanding Academic Master's Degree in Jiangsu Province and the Outstanding Master's Thesis of Jiangsu Computer Society","type":"post"},{"authors":null,"categories":null,"content":"News link：JITAS 2024 | Symposium on Advanced Multimedia Technologies Successfully Held\nDuring the 3rd Jiangsu Information Technology Application Conference 2024, the “Advanced Multimedia Technologies” symposium, organized by the Jiangsu Information Technology Application Society and hosted by its Multimedia Technology Committee, was held in Room 230 of the Computer Science Building at Nanjing University. The symposium lasted three and a half hours and attracted numerous experts and scholars from universities and enterprises, fostering in-depth discussions on the core technologies and applications of the multimedia industry in the era of intelligence.\nThe event featured four thematic reports, covering a wide range of cutting-edge topics in the multimedia field.\nProfessors Yirui Wu and Lixin Yuan from the lab contributed to the organization of this symposium.\n","date":1732147200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732147200,"objectID":"2163b8aed05f8fa03ff21e6d990e3932","permalink":"https://hhudelta.github.io/post/24-11-21/","publishdate":"2024-11-21T00:00:00Z","relpermalink":"/post/24-11-21/","section":"post","summary":"News link：JITAS 2024 | Symposium on Advanced Multimedia Technologies Successfully Held\n","tags":null,"title":"The Lab Co-organized the Symposium on Advanced Multimedia Technologies Successfully Held at Nanjing University","type":"post"},{"authors":null,"categories":null,"content":"Qiran Kong’s paper “CDT-CAD: Context-Aware Deformable Transformers for End-to-End Chest Abnormality Detection on X-Ray Images,” published in TCBB, has been recognized as an ESI Highly Cited Paper.\n","date":1731974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731974400,"objectID":"776f1ac1f804b49b707dd034cec90007","permalink":"https://hhudelta.github.io/post/24-11-19/","publishdate":"2024-11-19T00:00:00Z","relpermalink":"/post/24-11-19/","section":"post","summary":"Qiran Kong’s paper “CDT-CAD: Context-Aware Deformable Transformers for End-to-End Chest Abnormality Detection on X-Ray Images,” published in TCBB, has been recognized as an ESI Highly Cited Paper.\n","tags":null,"title":"Congratulations to Qiran Kong on their work published in TCBB being recognized as a Highly Cited Paper","type":"post"},{"authors":null,"categories":null,"content":"The Jiangsu Province Young Science and Technology Talent Support Project (Youth Support Project) is an important talent program aiming to cultivate and select outstanding young science and technology talents.\nThis project mainly targets young scientific researchers within Jiangsu Province, especially those who are under 35 years old and have remarkable achievements and development potential in fields such as natural sciences and engineering technologies. Through measures like providing financial support, building scientific research platforms, organizing academic exchanges and talent cultivation, the Youth Support Project aims to help young science and technology talents grow rapidly and become leading figures in the field of science and technology. Since its implementation, the Youth Support Project has cultivated a number of young science and technology talents with influence both at home and abroad, and has contributed significantly to the economic and social development of Jiangsu Province and even the whole country. ","date":1730937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730937600,"objectID":"cd1f96e12382595f8efc145eac299670","permalink":"https://hhudelta.github.io/post/24-11-7/","publishdate":"2024-11-07T00:00:00Z","relpermalink":"/post/24-11-7/","section":"post","summary":"The Jiangsu Province Young Science and Technology Talent Support Project (Youth Support Project) is an important talent program aiming to cultivate and select outstanding young science and technology talents.\n","tags":null,"title":"Professor Yirui Wu Awarded the 2024 Jiangsu Province Young Science and Technology Talent Support Program","type":"post"},{"authors":null,"categories":null,"content":"At the “3rd Jiangsu Information Technology Application Conference” held at Nanjing University on November 7, 2024, Professor Wu Yirui was awarded the Youth Science and Technology Award of Jiangsu Computer Society in recognition of his innovative achievements in the field of few-shot learning.\nThis conference highly values the scientific research and application of information technology. It gathers experts from universities, scientific research institutions and enterprises both inside and outside the province, and is committed to building a multi-level cooperation platform in the field of information technology, injecting new vitality into technological innovation and achievement transformation. Professor Yirui Wu’s achievements focus on theoretical expansion, platform design, practical application and technology promotion in few-shot learning. He actively leads platform construction, promotes the efficient application of technological achievements in industries such as water conservancy, and promotes industry integration through high-level academic forums, driving new breakthroughs in this field. ","date":1730937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730937600,"objectID":"22187d1f613adffc77126d2b4be3e0a0","permalink":"https://hhudelta.github.io/post/24-11-7-2/","publishdate":"2024-11-07T00:00:00Z","relpermalink":"/post/24-11-7-2/","section":"post","summary":"At the “3rd Jiangsu Information Technology Application Conference” held at Nanjing University on November 7, 2024, Professor Wu Yirui was awarded the Youth Science and Technology Award of Jiangsu Computer Society in recognition of his innovative achievements in the field of few-shot learning.\n","tags":null,"title":"Professor Yirui Wu Awarded the 2024 Youth Science and Technology Award by the Jiangsu Information Technology Application Society","type":"post"},{"authors":null,"categories":null,"content":"The Digital Twin Watershed refers to the digital mapping and intelligent simulation of all elements within a physical watershed and the entire process of water governance and management activities. Centering on the construction of digital twin watersheds, the focus lies on how intelligent simulation technology, through big data analysis and artificial intelligence algorithms, enables real-time monitoring and prediction of multidimensional data such as hydrology, water quality, and the environment within the watershed. Specific case studies highlight the remarkable achievements of intelligent technology in areas like flood forecasting and ecological protection. In light of the characteristics of current pre-trained large models, the future development trends of digital twin and intelligent simulation technologies in watershed management are also explored.\n","date":1726876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726876800,"objectID":"592d9020e1ae362c62868472f85db8c8","permalink":"https://hhudelta.github.io/post/24-09-21/","publishdate":"2024-09-21T00:00:00Z","relpermalink":"/post/24-09-21/","section":"post","summary":"The Digital Twin Watershed refers to the digital mapping and intelligent simulation of all elements within a physical watershed and the entire process of water governance and management activities. Centering on the construction of digital twin watersheds, the focus lies on how intelligent simulation technology, through big data analysis and artificial intelligence algorithms, enables real-time monitoring and prediction of multidimensional data such as hydrology, water quality, and the environment within the watershed. Specific case studies highlight the remarkable achievements of intelligent technology in areas like flood forecasting and ecological protection. In light of the characteristics of current pre-trained large models, the future development trends of digital twin and intelligent simulation technologies in watershed management are also explored.\n","tags":null,"title":"Professor Yirui Wu Invited to Deliver a Keynote on Digital Twin at the 12th China Water Informatization Technology Forum","type":"post"},{"authors":null,"categories":null,"content":"The CCF YOCSEF Nanjing Talk titled “Decoding the Science and Art of Gaming in Black Myth: Wukong”, organized by our lab’s Professor Yirui Wu, was successfully held on the Journal of Image and Graphics live streaming platform.\nNews link: CCF YOCSEF Nanjing Talk Summary | Decoding the Science and Art of Gaming in Black Myth: Wukong – Insights into the Jiangsu Gaming Industry’s Path to Combine Local Advantages with Technological Breakthroughs\nThe event was jointly organized and planned by Professor Yirui Wu and Professor Tianxing Wu from Southeast University. Esteemed speakers included Professor Mingqiang Wei from Nanjing University of Aeronautics and Astronautics, Professor Beibei Wang from Nanjing University, Professor Jie Guo from Nanjing University, Professor Baoping Yan from Nanjing University of the Arts, Professor Haichun Xie from Zhongshan Virtual Reality Research Institute, and Professor Liang Xiao from Nanjing University of Science and Technology. Together, we decoded the technology, art, and industry behind Black Myth: Wukong on the Journal of Image and Graphics live streaming platform.\n","date":1725753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725753600,"objectID":"af7ada5878fd593acb86693a450e79c1","permalink":"https://hhudelta.github.io/post/24-09-08/","publishdate":"2024-09-08T00:00:00Z","relpermalink":"/post/24-09-08/","section":"post","summary":"The CCF YOCSEF Nanjing Talk titled “Decoding the Science and Art of Gaming in Black Myth: Wukong”, organized by our lab’s Professor Yirui Wu, was successfully held on the Journal of Image and Graphics live streaming platform.\n","tags":null,"title":"Lab Successfully Co-hosted the Event 'Decoding the Science and Art of Gaming in Black Myth: Wukong' on the Journal of Image and Graphics Live Streaming Platform","type":"post"},{"authors":null,"categories":null,"content":" The 27th National Conference on Computer-Aided Design and Computer Graphics (CCF CAD/CG 2024), organized by the China Computer Federation (CCF) and co-hosted by the CCF Computer-Aided Design and Graphics Committee, East China Jiaotong University, Zhejiang University Nanchang Research Institute (Jiangxi Qiushi Advanced Research Institute), National Virtual Reality Innovation Center, Nanchang University of Aeronautics, and China Mobile Virtual Reality Innovation Center, will be held in Nanchang, Jiangxi, from August 15 to August 18, 2024. This conference will be held concurrently with the 16th National Conference on Geometric Design and Computing (GDC 2024). The conference aims to support the modernization of the industrial chain in Jiangxi under the “1269 Action Plan” and build Nanchang into a high-tech hub for information technology industries. The conference will bring together experts, scholars, technical innovators, and manufacturing pioneers from multiple fields to discuss the innovative applications and development of VR technology in the manufacturing industry.\nAs a flagship event for the computer-aided design and computer graphics community in China, this conference seeks to accelerate the integration of new-generation information technology with the manufacturing industry, support the digital transformation of manufacturing enterprises, and promote the innovation and popularization of VR technology. The conference aims to introduce intelligent technologies and systems to improve the efficiency and quality of manufacturing and establish Nanchang as a hub for the information technology industry.\nThe paper co-authored by Dr. Xinfu Liu and Rui Qin from our lab was accepted for an oral presentation at CCF CAD/CG 2024 and has been recommended to the Journal of Shanghai Jiao Tong University.\n","date":172368e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":172368e4,"objectID":"5b6704997b20e44c66aca296f2603d61","permalink":"https://hhudelta.github.io/post/24-08-15/","publishdate":"2024-08-15T00:00:00Z","relpermalink":"/post/24-08-15/","section":"post","summary":"","tags":null,"title":"Dr. Xinfu Liu Attended the CCF CAD/CG2024 Conference in Nanchang and Delivered an Oral Presentation","type":"post"},{"authors":null,"categories":null,"content":"Hyperspectral Image Classification Method Based on Global Space-spectral Attention Mechanism\nabstract：In hyperspectral remote sensing images, the interactions between pixels within a defined spatial range lead to the mixing of adjacent pixels. Additionally, the high similarity between adjacent spectra results in information redundancy, which hinders the extraction of global spatial and spectral correlations. To address the issues of adjacent pixel mixing and spectral redundancy, this paper proposes a hyperspectral image classification method based on a global spatial-spectral attention mechanism.\nFirst, the global spatial attention module in the proposed method uses multi-scale dilated convolutions to obtain a larger receptive field, capturing global spatial correlations and extracting unmixed pixel information. Then, the global spectral attention module designs a spectral domain partitioning algorithm, which uses the product of local density and information entropy as a threshold to divide the spectrum into scattered subsets, eliminating redundant information. This approach fully utilizes the global contextual information of the entire spectral band and extracts the correlation of global spectral information.\nFinally, these two modules are combined to obtain the global correlations of both space and spectrum. Experimental results show that the proposed method achieves overall accuracies of 97.28%, 94.73%, and 95.76% on three WHU-Hi hyperspectral datasets, outperforming the comparison methods.\n","date":1723593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723593600,"objectID":"a7f516b11ea8a624aa3dc94cff33623f","permalink":"https://hhudelta.github.io/post/24-08-14/","publishdate":"2024-08-14T00:00:00Z","relpermalink":"/post/24-08-14/","section":"post","summary":"Hyperspectral Image Classification Method Based on Global Space-spectral Attention Mechanism\n","tags":null,"title":"Congratulations to Rui Qin on His Paper's Acceptance by the Journal of Shanghai Jiao Tong University (English Edition).","type":"post"},{"authors":["Rui Qin","Benze Wu","Xinfu Liu","Yirui Wu"],"categories":null,"content":"","date":1723593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723593600,"objectID":"02f88c2523478d9755b2cb16ce006886","permalink":"https://hhudelta.github.io/publication/sju2024/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/publication/sju2024/","section":"publication","summary":"In hyperspectral remote sensing imagery, pixel interactions within defined spatial extents result in the mixing of adjacent pixels. Additionally, the high similarity of adjacent spectra leads to information redundancy, which hinders the extraction of global spatial and spectral correlations. In order to solve the problems of mixed adjacent pixels and redundant adjacent spectra, this work offers a hyperspectral image classification approach that uses a global space-spectral attention mechanism. First, the proposed method’s global spatial attention module uses multi-scale dilated convolution to produce a bigger receptive field to be capable of capturing global spatial correlation and obtain unmixed pixel information. Then, the global spectral attention module designs a spectral domain partition algorithm, using the combination of regional density as well as information entropy as the threshold to divide spectrum into dispersed subsets and eliminate redundant information. The global context information for entire spectral band is fully exploited, and correlation of the global spectral information is extracted. Finally, the two modules combine to provide a global correlation of space and spectrum. Experiments demonstrate that the suggested method obtains overall accuracies of 97.28%, 94.73%, and 95.76% on the three WHU-Hi hyperspectral datasets, surpassing comparison methods.","tags":[],"title":"Hyperspectral Image Classification Method Based on Global Space-Spectral Attention Mechanism","type":"publication"},{"authors":null,"categories":null,"content":"Edge Computing and Few-shot Learning Featured Intelligent Framework in Digital Twin empowered Mobile Networks\nJournal：IEEE Transactions on Network and Service Management(中科院2区) Digital twins (DT) and mobile networks have evolved forms of intelligence in Internet of Things (IoT). In this work, we consider a Digital Twin Mobile Network (DTMN) scenario with few multimedia samples. Facing challenges of knowledge extraction with few samples, stable interaction with dynamic changes of multimedia data, time and privacy saving in low-resource mobile network, we propose an edge computing and few-shot learning featured intelligent framework. Considering time-sensitive property of transmission and privacy risks of directly uploads in mobile network, we deploy edge computing to locally run networks for analysis, thus saving time to offload computing request and enhancing privacy by encrypting original data. Inspired by remarkable relationship representation of graphs, we build Graph Neural Network (GNN) in cloud to map physical mobile systems to virtual entities with DT, thus performing semantic inferences in cloud with few samples uploaded by edges. Occasionally, node features in GNN could converge to similar, non-discriminative embeddings, causing catastrophic unstable phenomena. An iterative reweight and drop structure (IRDS) is thus constructed in cloud, which nonetheless contributes stability with respect to edge uncertainty. As part of IRDS, a drop Edge\u0026amp;Node scheme is proposed to randomly remove certain nodes and edges, which not only enhances distinguished capability of graph neighbor patterns, but also offers data encryption with random strategy. We show one implementation case of image classification in social network, where experiments on public datasets show that our framework is effective with user-friendly advantages and significant intelligence.\n","date":1723248e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723248e3,"objectID":"53e4d73a89ee10356ec3440aaaa5c130","permalink":"https://hhudelta.github.io/post/24-08-10/","publishdate":"2024-08-10T00:00:00Z","relpermalink":"/post/24-08-10/","section":"post","summary":"Edge Computing and Few-shot Learning Featured Intelligent Framework in Digital Twin empowered Mobile Networks\n","tags":null,"title":"Congratulations to Hao Cao for his paper being accepted by TNSM","type":"post"},{"authors":null,"categories":null,"content":"The China Yancheng Innovation and Entrepreneurship Competition, hosted by the Yancheng Municipal Government, aims to promote regional innovation development, attract global talent, and facilitate the transformation of scientific and technological achievements. The competition focuses on multiple high-tech fields and enhances the city’s image and economic development through multi-stage selection and diversified support, while also fostering and incubating innovative talent.\nDr. Xinfu Liu, along with students Xiu Jin and Ao Geng from our lab, presented three industry-academia-research projects at the finals. ","date":1721952e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721952e3,"objectID":"8015e5837a6a38543bc73212ca1638bc","permalink":"https://hhudelta.github.io/post/24-07-26/","publishdate":"2024-07-26T00:00:00Z","relpermalink":"/post/24-07-26/","section":"post","summary":"The China Yancheng Innovation and Entrepreneurship Competition, hosted by the Yancheng Municipal Government, aims to promote regional innovation development, attract global talent, and facilitate the transformation of scientific and technological achievements. The competition focuses on multiple high-tech fields and enhances the city’s image and economic development through multi-stage selection and diversified support, while also fostering and incubating innovative talent.\n","tags":null,"title":"Several students from our team participated in the China Yancheng Innovation and Entrepreneurship Competition.","type":"post"},{"authors":null,"categories":null,"content":"Thank you to the International Symposium on IoT and Smart Cities (ISITSC 2024) for the invitation. We warmly welcome everyone to join the event in person at: Nanjing Jinfan Wanyuan Hotel, 47-1 Pailou Alley, Gulou District, Nanjing, Jiangsu Province (near Hanzhongmen Metro Station, Jiangsu Provincial Hospital of Traditional Chinese Medicine).\n","date":1719014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719014400,"objectID":"7000348eba40dc34f819f8439940e001","permalink":"https://hhudelta.github.io/post/24-06-22-isitsc/","publishdate":"2024-06-22T00:00:00Z","relpermalink":"/post/24-06-22-isitsc/","section":"post","summary":"Thank you to the International Symposium on IoT and Smart Cities (ISITSC 2024) for the invitation. We warmly welcome everyone to join the event in person at: Nanjing Jinfan Wanyuan Hotel, 47-1 Pailou Alley, Gulou District, Nanjing, Jiangsu Province (near Hanzhongmen Metro Station, Jiangsu Provincial Hospital of Traditional Chinese Medicine).\n","tags":null,"title":"Professor Yirui Wu from our lab was invited to deliver a keynote speech on Few-shot Visual Learning at the International Symposium on IoT and Smart Cities","type":"post"},{"authors":null,"categories":null,"content":"Paper published in TECS “Edge-AI-Driven Framework with Efficient Mobile Network Design for Facial Expression Recognition” becomes ESI highly cited paper. This is our forth paper for ESI highly cited paper.\n","date":1716336e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716336e3,"objectID":"4cb2fc98363c3bedf3d36783be39b784","permalink":"https://hhudelta.github.io/post/24-05-22-tecs-high-cited/","publishdate":"2024-05-22T00:00:00Z","relpermalink":"/post/24-05-22-tecs-high-cited/","section":"post","summary":"Paper published in TECS “Edge-AI-Driven Framework with Efficient Mobile Network Design for Facial Expression Recognition” becomes ESI highly cited paper. This is our forth paper for ESI highly cited paper.\n","tags":null,"title":"One paper published in TECS becomes ESI highly cited paper","type":"post"},{"authors":[],"categories":null,"content":"","date":1715504400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715504400,"objectID":"937ede74a72f55763cfd7574b08ce1e1","permalink":"https://hhudelta.github.io/event/24-05-11-ccf-forum/","publishdate":"2024-05-11T00:00:00Z","relpermalink":"/event/24-05-11-ccf-forum/","section":"event","summary":"由CCF南京分部主办的，CCF YOCSEF南京协办的CCF 南京分部公益论坛：大模型助力新质生产力，将于2024 年 5 月 12 日（周日）9:00-11:20在河海大学江宁校区行政楼一楼 多功能厅举行.","tags":[],"title":"CCF 南京分部公益论坛：大模型助力新质生产力","type":"event"},{"authors":null,"categories":null,"content":"We have organized a forum about language and vision big model in Hohai University, Jiangning Campus. We have been grateful to invite Professor Guilin Qi from Southeast University and Professor Liming Wang from Nanjing University. Hope to see you in Nanjing. ","date":1715385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715385600,"objectID":"b48f782d43e08f3a4410ae1a9a481901","permalink":"https://hhudelta.github.io/post/24-05-11-ccf-forum/","publishdate":"2024-05-11T00:00:00Z","relpermalink":"/post/24-05-11-ccf-forum/","section":"post","summary":"We have organized a forum about language and vision big model in Hohai University, Jiangning Campus. We have been grateful to invite Professor Guilin Qi from Southeast University and Professor Liming Wang from Nanjing University. Hope to see you in Nanjing. ","tags":null,"title":"CCF Nanjing Chapter Public Forum: Empowering New Productivity with Large Models","type":"post"},{"authors":null,"categories":null,"content":"","date":1714953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714953600,"objectID":"ce03df1a7560ceb3560948d14cc14155","permalink":"https://hhudelta.github.io/post/24-05-06-valse/","publishdate":"2024-05-06T00:00:00Z","relpermalink":"/post/24-05-06-valse/","section":"post","summary":"","tags":null,"title":"Our team has attented VALSE2024 in Chongqing City","type":"post"},{"authors":[],"categories":null,"content":"","date":1714809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714809600,"objectID":"197b81da2e7cb91f8f1e689e45492b1f","permalink":"https://hhudelta.github.io/event/24-05-06/","publishdate":"2024-05-06T00:00:00Z","relpermalink":"/event/24-05-06/","section":"event","summary":"2024年5月4日至7日,实验室组织研究生同学，前往重庆参加VALSE2024学术会议。","tags":[],"title":"实验室访学记录：重庆VALSE行","type":"event"},{"authors":null,"categories":null,"content":"Paper published in TOIT “Digital Twin of Intelligent Small Surface Defect Detection with Cyber-Manufacturing Systems” becomes ESI highly cited paper. This is our third paper for ESI highly cited paper.\n","date":1710460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710460800,"objectID":"8eadcd0af1d83def38b07a8958ce5b22","permalink":"https://hhudelta.github.io/post/24-03-15-toit-high-cited/","publishdate":"2024-03-15T00:00:00Z","relpermalink":"/post/24-03-15-toit-high-cited/","section":"post","summary":"Paper published in TOIT “Digital Twin of Intelligent Small Surface Defect Detection with Cyber-Manufacturing Systems” becomes ESI highly cited paper. This is our third paper for ESI highly cited paper.\n","tags":null,"title":"One paper published in TOIT becomes ESI highly cited paper","type":"post"},{"authors":null,"categories":null,"content":"Paper published in TNSE “Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection” becomes ESI hot paper. This is our first paper for ESI hot paper.\n","date":1710028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710028800,"objectID":"d4a34b62d77f62d8bf306f8868a1d9c6","permalink":"https://hhudelta.github.io/post/24-03-10-tnse-hot-paper/","publishdate":"2024-03-10T00:00:00Z","relpermalink":"/post/24-03-10-tnse-hot-paper/","section":"post","summary":"Paper published in TNSE “Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection” becomes ESI hot paper. This is our first paper for ESI hot paper.\n","tags":null,"title":"One paper published in TNSE becomes ESI hot paper","type":"post"},{"authors":["Yi Rong","Haoran Zhou","Lixin Yuan","Cheng Mei","Jiahao Wang","Tong Lu"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"c7aa4ff8ad120dd231bed431a7d3bed8","permalink":"https://hhudelta.github.io/publication/rong-cra-pcn-2024/","publishdate":"2024-12-18T16:43:54.971648Z","relpermalink":"/publication/rong-cra-pcn-2024/","section":"publication","summary":"Point cloud completion is an indispensable task for recovering complete point clouds due to incompleteness caused by occlusion, limited sensor resolution, etc. The family of coarse-to-fine generation architectures has recently exhibited great success in point cloud completion and gradually became mainstream. In this work, we unveil one of the key ingredients behind these methods: meticulously devised feature extraction operations with explicit cross-resolution aggregation. We present Cross-Resolution Transformer that efficiently performs cross-resolution aggregation with local attention mechanisms. With the help of our recursive designs, the proposed operation can capture more scales of features than common aggregation operations, which is beneficial for capturing fine geometric characteristics. While prior methodologies have ventured into various manifestations of inter-level cross-resolution aggregation, the effectiveness of intra-level one and their combination has not been analyzed. With unified designs, Cross-Resolution Transformer can perform intra- or inter-level cross-resolution aggregation by switching inputs. We integrate two forms of Cross-Resolution Transformers into one up-sampling block for point generation, and following the coarse-to-fine manner, we construct CRA-PCN to incrementally predict complete shapes with stacked up-sampling blocks. Extensive experiments demonstrate that our method outperforms state-of-the-art methods by a large margin on several widely used benchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.","tags":null,"title":"CRA-PCN: Point Cloud Completion with Intra-and Inter-level Cross-Resolution Transformers","type":"publication"},{"authors":["Lixin Yuan","Cheng Mei","Wenhai Wang","Tong Lu"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"609fd7cf368ee241ad8f99aa04c42f19","permalink":"https://hhudelta.github.io/publication/yuan-feature-2024/","publishdate":"2024-12-18T16:43:54.829346Z","relpermalink":"/publication/yuan-feature-2024/","section":"publication","summary":"Feature selection (FS) has recently attracted considerable attention in many fields. Highly-overlapping classes and skewed distributions of data within classes have been found in various classification tasks. Most existing FS methods are all instance-based, which ignores the significant differences in characteristics between the particular outliers and the main body of the class, causing confusion for classifiers. In this paper, we propose a novel supervised FS method, Intrusive Outliers-based Feature Selection (IOFS), to find out what kind of outliers lead to misclassification and exploit the characteristics of such outliers. In order to accurately identify the intrusive outliers (IOs), we provide a density-mean center algorithm to obtain the appropriate representative of a class. A special distance threshold is given to obtain the candidate for IOs. Combining with several metrics, mathematical formulations are provided to evaluate the overlapping degree of the intrusive class pairs. Features with high overlapping degrees are assigned to low rankings in IOFS method. An extension of IOFS based on a small number of extreme IOs, called E-IOFS, is also proposed. Three theoretical proofs are provided for the essential theoretical basis of IOFS. Experiments comparing against various state-of-the-art methods on eleven benchmark datasets show that IOFS is rational and effective, especially on the datasets with higher overlapping classes. And E-IOFS almost always outperforms IOFS.","tags":["classification","density-mean center","Face recognition","Feature extraction","intrusive outlier","Measurement","Mutual information","overlapping class","Solid modeling","Supervised feature selection","Task analysis","Training"],"title":"Feature Selection Based on Intrusive Outliers Rather Than All Instances","type":"publication"},{"authors":null,"categories":null,"content":"Paper published in TNSE “Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection” becomes ESI highly cited paper. This is our second paper for ESI highly cited.\n","date":1703203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703203200,"objectID":"2e639a80abab3bbf1f4827835d49ccf4","permalink":"https://hhudelta.github.io/post/23-12-22-tnse-high-cited/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/post/23-12-22-tnse-high-cited/","section":"post","summary":"Paper published in TNSE “Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection” becomes ESI highly cited paper. This is our second paper for ESI highly cited.\n","tags":null,"title":"One paper published in TNSE becomes ESI highly cited paper","type":"post"},{"authors":[],"categories":null,"content":" ","date":1701165600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701165600,"objectID":"10f8c133531826e77aeba7a666809217","permalink":"https://hhudelta.github.io/event/23-11-08/","publishdate":"2023-11-08T00:00:00Z","relpermalink":"/event/23-11-08/","section":"event","summary":"下周二11月28日河海江宁校区勤学楼4202会议室，有幸请到IAPR Fellow Pal老师，和西北工业大学的田春伟老师，欢迎有兴趣的老师与同学前往参加。报告以线上线下方式同步举行。 #腾讯会议：434-142-344","tags":[],"title":"IAPR Fellow Pal老师 西北工业大学 田春伟老师 报告会","type":"event"},{"authors":["Yirui Wu","Qiran Kong","Cheng Qian","Michele Nappi","Shaohua Wan"],"categories":null,"content":" ","date":1701129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701129600,"objectID":"deb5f4fb9d0ecbf5e9f952f659d5372d","permalink":"https://hhudelta.github.io/publication/bigdr2023/","publishdate":"2023-11-28T00:00:00Z","relpermalink":"/publication/bigdr2023/","section":"publication","summary":"Deep learning has achieved great success in text detection, where recent methods adopt inspirations from segmentation to detect scene texts. However, most segmentation based methods have high computation cost in pixel-level classification and post refinements. Moreover, they still faces challenges like arbitrary directions, curved texts, illumination and so on. Aim to improve detection accuracy and computation cost, we propose an end-to-end and single-stage method named as End-PolarT network by generating contour points in polar coordinates for text detection. End-PolarT not only regress locations of contour points instead of pixels to relieve high computation cost, but also fits with intrinsic characteristics of text instances by centers and contours to suppress mislabeling boundary pixels. To cope with polar representation, we further propose polar IoU and centerness as key parts of loss functions to generate effective paradigms for text detection. Compared with the existing methods, End-PolarT achieves superior results by testing on several public datasets, thus keeping balance between efficiency and effectiveness in complicated scenes.","tags":[],"title":"End-PolarT: Polar Representation for End-to-End Scene Text Detection","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1699437600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699437600,"objectID":"4ff77b3879bf318b346d997cc7e75504","permalink":"https://hhudelta.github.io/event/23-11-09/","publishdate":"2023-11-09T00:00:00Z","relpermalink":"/event/23-11-09/","section":"event","summary":"2023年11月8日至9日,实验室组织了分散在南京与金坛的研究生同学，前往苏州相城区进行秋游。 整体活动安排为一日自由行，一日参观CCF总部与相城区规划馆，体验商用自动驾驶汽车。","tags":[],"title":"实验室秋游记录：苏州相城行","type":"event"},{"authors":["Hao Li","Yirui Wu","Hexuan Hu","Hu Lu","Qian Huang","Shaohua Wan"],"categories":null,"content":" ","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"8fd5b3653529157560a419eac17f15b5","permalink":"https://hhudelta.github.io/publication/methods2023/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/publication/methods2023/","section":"publication","summary":"Deep learning has brought a significant progress in medical image analysis. However, their lack of interpretability might bring high risk for wrong diagnosis with limited clinical knowledge embedding. In other words, we believe it's crucial for humans to interpret how deep learning work for medical analysis, thus appropriately adding knowledge constraints to correct the bias of wrong results. With such purpose, we propose Representation Group-Disentangling Network (RGD-Net) to explain the process of feature extraction and decision making inside deep learning framework, where we completely disentangle feature space of input X-ray images into independent feature groups, and each group would contribute to diagnose of a specific disease. Specifically, we first state problem definition for interpretable prediction with auto-encoder structure. Then, group-disentangled representations are extracted from input X-ray images with the proposed Group-Disentangle Module, which constructs semantic latent space by enforcing semantic consistency of attributes. Afterwards, adversarial constricts on mapping from features to diseases are proposed to prevent model collapse during training. Finally, a novel design of local tuning medical application is proposed based on RGB-Net, which is capable to aid clinicians for reasonable diagnosis. By conducting quantity of experiments on public datasets, RGD-Net have been superior to comparative studies by leveraging potential factors contributing to different diseases. We believe our work could bring interpretability in digging inherent patterns of deep learning on medical image analysis.","tags":[],"title":"Interpretable Thoracic Pathologic Prediction via Learning Group-Disentangled Representation","type":"publication"},{"authors":["Yirui Wu","Qiran Kong","Lai Yong","Fabio Narducci","Shaohua Wan"],"categories":null,"content":"","date":169344e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":169344e4,"objectID":"6ca1716d01b3815848162536ed374e0b","permalink":"https://hhudelta.github.io/publication/prl2023-2/","publishdate":"2023-08-31T00:00:00Z","relpermalink":"/publication/prl2023-2/","section":"publication","summary":"","tags":[],"title":"CDText: Scene Text Detector Based on Context-Aware Deformable Transformer","type":"publication"},{"authors":["Yirui Wu","Lilai Zhang","Zonghua Gu","Hu Lu","Shaohua Wan"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"2c33cba216c077cf83b0d624a511785d","permalink":"https://hhudelta.github.io/publication/tecs2023/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/tecs2023/","section":"publication","summary":"Facial Expression Recognition (FER) in the wild poses significant challenges due to realistic occlusions, illumination, scale, and head pose variations of the facial images. In this article, we propose an Edge-AI-driven framework for FER. On the algorithms aspect, we propose two attention modules, Arbitrary-oriented Spatial Pooling (ASP) and Scalable Frequency Pooling (SFP), for effective feature extraction to improve classification accuracy. On the systems aspect, we propose an edge-cloud joint inference architecture for FER to achieve low-latency inference, consisting of a lightweight backbone network running on the edge device, and two optional attention modules partially offloaded to the cloud. Performance evaluation demonstrates that our approach achieves a good balance between classification accuracy and inference latency.","tags":[],"title":"Edge-AI-Driven Framework with Efficient Mobile Network Design for Facial Expression Recognition","type":"publication"},{"authors":["Yirui Wu","Qiran Kong","Lilai Zhang","Aniello Castiglione","Michele Nappi","Shaohua Wan"],"categories":null,"content":"","date":1679011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679011200,"objectID":"dd5af90b199a56646d2f0304cbe3bc4d","permalink":"https://hhudelta.github.io/publication/tcbb2023/","publishdate":"2023-03-17T00:00:00Z","relpermalink":"/publication/tcbb2023/","section":"publication","summary":"","tags":[],"title":"CDT-CAD: Context-Aware Deformable Transformers for End-to-End Chest Abnormality Detection on X-Ray Images","type":"publication"},{"authors":["Qian Cheng","Yirui Wu","Aniello Castiglione","Fabio Narducci","Shaohua Wan"],"categories":null,"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"661031a40d039cde79f87788f31f079b","permalink":"https://hhudelta.github.io/publication/jsps2023/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/publication/jsps2023/","section":"publication","summary":"Flood prediction is a challenging task due to the extreme runoff values, short duration, and complex generation mechanisms. This paper introduces DA-Net, a dual attention embedding network that incorporates convolution self-attention (CSA) and Temporal-related Feature Attention (TFA) to improve flood forecasting accuracy. CSA captures local context, while TFA enhances global feature modeling. The proposed method outperforms existing deep learning models on the Changhua and Tunxi watershed datasets.","tags":[],"title":"DA-Net: Dual Attention Network for Flood Forecasting","type":"publication"},{"authors":["Yirui Wu","Lilai Zhang","Hao Li","Yunfei Zhang","Shaohua Wan"],"categories":null,"content":"","date":1675123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675123200,"objectID":"c2344a32a88b28efe82f9bffdab55356","permalink":"https://hhudelta.github.io/publication/tallip2023/","publishdate":"2023-01-31T00:00:00Z","relpermalink":"/publication/tallip2023/","section":"publication","summary":"How to properly involve text characteristics like multi-scale, arbitrary direction, length aspect ratio, into detection network design has become a hot topic in computer vision. Feature Pyramid Network (FPN) is a typical method to achieve robust text detection, where its low-level and high-level feature map retains spatial structure and global semantic information, respectively. However, its strict hierarchical structure fails to fuse low-level and high-level information to improve the distinguish ability of feature map. To address this problem, we propose a novel feature fusion pyramid network for end-to-end scene text detection by fusing multi-modal information. By dividing pyramid structure into high-level and low-level layers, channel and spatial attention modules are adopted to enhance high-level and low-level feature representation by encoding channel and spatial-wise context information, respectively. In order to reduce information loss by layer transmission, a special residual network is designed to achieve short-cut between high-level and low-level features, so as to realize multi-modal feature fusion. Experiments show the precision and recall of the proposed method on ICDAR2015, ICDAR2017-MLT, and MSRA-TD500 datasets reach 88.7%/82.1%, 77.0%/60.3%, and 85.3%/74.8%, respectively.","tags":[],"title":"Feature Fusion Pyramid Network for End-to-End Scene Text Detection","type":"publication"},{"authors":["Yirui Wu","Benze Wu","Yunfei Zhang","Shaohua Wan"],"categories":null,"content":"","date":1673568e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673568e3,"objectID":"4116d2bfabaf5cc3b035cdbc7b9f3988","permalink":"https://hhudelta.github.io/publication/sc2023/","publishdate":"2023-01-13T00:00:00Z","relpermalink":"/publication/sc2023/","section":"publication","summary":"Deep learning has shown remarkable performance in quantity of vision tasks. However, its large network generally requires quantity of samples to support sufficient parameters learning during training process. Such high request greatly reduces efficiency when applying on a small dataset with few samples. To alleviate this problem, we propose a novel data enhancement method for few-shot learning via a cutout approach and feature enhancement. After enhancement, the generated network not only produces distinguish feature map without collecting more samples, but also achieves advantage of feature representation with high efficiency for computing. Specifically, cutout approach is simple yet highly effective for image regulation, which enhances input image matrix by adding a fixed mask to improve robustness and overall performance of network. Afterward, we perform feature enhancement by proposing a feature promotion module, which uses characteristics of dilated convolution and sequential processing to improve feature representation ability, thus improving efficiency of the whole network. We conduct comparative experiments on both miniImageNet and CUB datasets, where the proposed method is superior to comparative methods in both 1-shot and 5-shot cases.","tags":[],"title":"A Novel Method of Data and Feature Enhancement for Few-Shot Image Classification","type":"publication"},{"authors":["Yirui Wu","Hao Li","Xi Feng","Andrea Casanova","Andrea F. Abate","Shaohua Wan"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"d899e0aca94257949fc46a81cce5b7e7","permalink":"https://hhudelta.github.io/publication/prl2023-1/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/prl2023-1/","section":"publication","summary":"Deep learning methods have shown significant performance in medical image analysis tasks. However, they generally act like “black box” without explanations in both feature extraction and decision processes, leading to lack of clinical insights and high risk assessments. To aid deep learning in envisioning diseases with visual clues, we propose a novel Group-Disentangled Representation Learning framework (GDRL). The key contribution is that GDRL completely disentangles latent space into disease concepts with abundant and non-overlapping feature related explanations, thus enhancing interpretability in feature extraction and decision processes. Furthermore, we introduce an implicit group-swap structure by emphasizing the linking relationship between semantical concepts of disease and low-level visual features, other than explicit explanations on general objects and their attributes. We demonstrate our framework on predicting four categories of diseases from chest X-ray images. The AUROC of GDRL on ChestX-ray14 for thoracic pathologic prediction are 0.8630, 0.8980, 0.9269 and 0.8653 respectively, and we showcase the potential of our framework in enhancing interpretability of the factors contributing to different diseases.","tags":[],"title":"GDRL: An Interpretable Framework for Thoracic Pathologic Prediction","type":"publication"},{"authors":["Yirui Wu","Hao Cao","Guoqiang Yang","Tong Lu","Shaohua Wan"],"categories":null,"content":"","date":1668643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668643200,"objectID":"d7134b57e0376167bd592ea08b0bd0fe","permalink":"https://hhudelta.github.io/publication/toit2022/","publishdate":"2022-11-17T00:00:00Z","relpermalink":"/publication/toit2022/","section":"publication","summary":"The rapid advancement in cyber-physical systems has led to the evolution of Industry 4.0, with a key concept being the digital twin (DT). However, linking twin simulations with real-world scenarios remains challenging, especially in tasks like small surface defect detection. This article proposes a cyber-manufacturing system with a DT solution for small surface defect detection. The system uses an Edge-Cloud architecture for efficient data collection and processing, coupled with a deep learning-based detection algorithm utilizing multi-modal data. Experiments demonstrate high accuracy and recall in small defect detection.","tags":[],"title":"Digital Twin of Intelligent Small Surface Defect Detection with Cyber-manufacturing Systems","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://hhudelta.github.io/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://hhudelta.github.io/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":["Minglei Yuan","Chunhao Cai","Tong Lu","Yirui Wu","Qian Xu","Shijie Zhou"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"4bf8be5d47134e5de2a36d189acca8d6","permalink":"https://hhudelta.github.io/publication/pr2022/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/pr2022/","section":"publication","summary":"Existing Few-Shot Learning (FSL) methods learn and recognize new classes with the help of prior knowledge. However, they cannot handle this task well in a cross-domain scenario when training and testing sets are from different domains, since the fact that prior knowledge in different domains often varies greatly. To solve this problem, in this paper, we propose a few-shot domain generalization method, which is designed to extract relationship embeddings using Forget-Update Modules named FUM. The relationship embedding considers valuable relational information between samples in a specific task, and the forget-update module takes into account differences between domains and adjusts the distribution of relational embeddings through forgetting and updating mechanisms based on specific tasks. To evaluate the few-shot domain generalization ability of FUM, extensive experiments on eight cross-domain scenarios and six same-domain scenarios are conducted, and the results show that FUM achieves superior performances compared to recent few-shot learning methods. Visualization results also show that the distribution of the relationship embeddings extracted by FUM has stronger few-shot domain generalization ability than the feature embeddings used in the existing FSL methods.","tags":[],"title":"A Novel Forget-Update Module for Few-Shot Domain Generalization","type":"publication"},{"authors":["Yirui Wu","Lilai Zhang","Stefano Berretti","Shaohua Wan"],"categories":null,"content":"","date":1658966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658966400,"objectID":"a17248951a5b4d4af05151051a7f6d85","permalink":"https://hhudelta.github.io/publication/tii2022/","publishdate":"2022-07-28T00:00:00Z","relpermalink":"/publication/tii2022/","section":"publication","summary":"There exists a rising concern on security of healthcare data and service. Even small lost, stolen, displaced, hacked, or communicated in personal health data could bring huge damage to patients. Therefore, we propose a novel content-aware deoxyribonucleic acid (DNA) computing system to encrypt medical images, thus guaranteeing privacy and promoting secure healthcare environment. The proposed system consists of sender and receiver to perform tasks of encryption and decryption, respectively, where both contain the same structure design, but perform opposite operations. In either sender or receiver, we design a randomly DNA encoding and a content-aware permutation and diffusion module. Considering introducing random mechanism to increase difficulty of cracking, the former module builds a random encryption rule selector in DNA encoding process by randomly mapping quantity of medical image pixels to outputs. Meanwhile, the latter module constructs a permutation sequence, which not only encodes information of pixel values, but also involves redundant correlation between adjacent pixels located in a patch. Such design brings awareness property of medical image content to greatly increase complexity in cracking by embedding semantical information for encryption. We demonstrate that the proposed system successfully improves cybersecurity of medical images against various attacks in robustness and effectiveness when transmitting data in wireless broadcasting scenarios.","tags":[],"title":"Medical Image Encryption by Content-Aware DNA Computing for Secure Healthcare","type":"publication"},{"authors":["Guangchen Shi","Yirui Wu","Jun Liu","Shaohua Wan","Wehai Wang","Tong Lu"],"categories":null,"content":"","date":1658793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658793600,"objectID":"f1f45dbff130e2840add01dabf85bf48","permalink":"https://hhudelta.github.io/publication/acmmm2022/","publishdate":"2022-07-26T00:00:00Z","relpermalink":"/publication/acmmm2022/","section":"publication","summary":"Incremental few-shot semantic segmentation (IFSS) targets at incrementally expanding model's capacity to segment new class of images supervised by only a few samples. However, features learned on old classes could significantly drift, causing catastrophic forgetting. Moreover, few samples for pixel-level segmentation on new classes lead to notorious overfitting issues in each learning session. In this paper, we explicitly represent class-based knowledge for semantic segmentation as a category embedding and a hyper-class embedding, where the former describes exclusive semantical properties, and the latter expresses hyper-class knowledge as class-shared semantic properties. Aiming to solve IFSS problems, we present EHNet, i.e., Embedding adaptive-update and Hyper-class representation Network from two aspects. First, we propose an embedding adaptive-update strategy to avoid feature drift, which maintains old knowledge by hyper-class representation, and adaptively update category embeddings with a class-attention scheme to involve new classes learned in individual sessions. Second, to resist overfitting issues caused by few training samples, a hyper-class embedding is learned by clustering all category embeddings for initialization and aligned with category embedding of the new class for enhancement, where learned knowledge assists to learn new knowledge, thus alleviating performance dependence on training data scale. Significantly, these two designs provide representation capability for classes with sufficient semantics and limited biases, enabling to perform segmentation tasks requiring high semantic dependence. Experiments on PASCAL-5i and COCO datasets show that EHNet achieves new state-of-the-art performance with remarkable advantages.","tags":[],"title":"Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation","type":"publication"},{"authors":["Yirui Wu","Wen Zhang","Shaohua Wan"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"d98beea2b874a8bb8dd00b07dc4d9fb0","permalink":"https://hhudelta.github.io/publication/prl2022/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/prl2022/","section":"publication","summary":"With the significant power of deep learning architectures, researchers have made much progress on effectiveness and efficiency of text detection in the past few years. However, due to the lack of consideration of unique characteristics of text components, directly applying deep learning models to perform text detection task is prone to result in low accuracy, especially producing false positive detection results. To ease this problem, we propose a lightweight and context-aware deep convolutional neural network (CNN) named as CE-Text, which appropriately encodes multi-level channel attention information to construct discriminative feature map for accurate and efficient text detection. To fit with low computation resource of embedded systems, we further transform CE-Text into a lighter version with a frequency based deep CNN compression method, which expands applicable scenarios of CE-Text into variant embedded systems. Experiments on several popular datasets show that CE-Text not only has achieved accurate text detection results in scene images, but also could run with fast performance in embedded systems.","tags":[],"title":"CE-text: A Context-Aware and Embedded Text Detector in Natural Scene Images","type":"publication"},{"authors":["Yonghong Chen","Hao Li","Han Li","Wenhao Liu","Yirui Wu","Qian Huang","Shaohua Wan"],"categories":null,"content":"","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"53a7b3edad694ab36f41e666a601a270","permalink":"https://hhudelta.github.io/publication/tallip2022/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/publication/tallip2022/","section":"publication","summary":"In recent years, with the rapid development of Internet technology and applications, the scale of Internet data has exploded, which contains a significant amount of valuable knowledge. The best methods for the organization, expression, calculation, and deep analysis of this knowledge have attracted a great deal of attention. The knowledge graph has emerged as a rich and intuitive way to express knowledge. Knowledge reasoning based on knowledge graphs is one of the current research hot spots in knowledge graphs and has played an important role in wireless communication networks, intelligent question answering, and other applications. Knowledge graph-oriented knowledge reasoning aims to deduce new knowledge or identify wrong knowledge from existing knowledge. Different from traditional knowledge reasoning, knowledge reasoning methods oriented to knowledge graphs are more diversified due to the concise, intuitive, flexible, and rich knowledge expression forms in knowledge graphs. Based on the basic concepts of knowledge graphs and knowledge graph reasoning, this paper introduces the latest research progress in knowledge graph-oriented knowledge reasoning methods in recent years. Specifically, according to different reasoning methods, knowledge graph reasoning includes rule-based reasoning, distributed representation-based reasoning, neural network-based reasoning, and mixed reasoning. These methods are summarized in detail, and the future research directions and prospects of knowledge reasoning based on knowledge graphs are discussed and prospected.","tags":[],"title":"An Overview of Knowledge Graph Reasoning: Key Technologies and Applications","type":"publication"},{"authors":["Yirui Wu","Haifeng Guo","Chinmay Chakraborty","Mohammad R. Khosravi","Stefano Berretti","Shaohua Wan"],"categories":null,"content":"","date":1644796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644796800,"objectID":"a7ca5d0ef45806152596d91bc30edc12","permalink":"https://hhudelta.github.io/publication/tnse2022/","publishdate":"2022-02-14T00:00:00Z","relpermalink":"/publication/tnse2022/","section":"publication","summary":"With fast increase in volume of mobile multimedia data, how to apply powerful deep learning methods to process data with real-time response becomes a major issue. Meanwhile, edge computing structure helps improve response time and user experience by bringing flexible computation and storage capabilities. Considering both technologies for successful AI-based applications, we propose an edge-computing driven and end-to-end framework to perform tasks of image enhancement and object detection under low-light conditions. The framework consists of a cloud-based enhancement and an edge-based detection stage. In the first stage, we establish connections between edge devices and cloud servers to input re-scaled illumination parts of low-light images, where enhancement subnetworks are dynamically and parallel coupled to compute enhanced illumination parts based on low-light context. During the edge-based detection stage, edge devices could accurately and rapidly detect objects based on cloud-computed informative feature map. Experimental results show the proposed method significantly improves detection performance in low-light conditions with low latency running on edge devices.","tags":[],"title":"Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection","type":"publication"},{"authors":["Lixin Yuan","Guoqiang Yang","Qian Xu","Tong Lu"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"3963c9bfb2bf847f28784b57437557f6","permalink":"https://hhudelta.github.io/publication/yuan-discriminative-2022/","publishdate":"2024-12-18T16:43:54.880803Z","relpermalink":"/publication/yuan-discriminative-2022/","section":"publication","summary":"With the rapid development of multimedia technologies (e.g. deep learning), Feature Selection (FS) is now playing a critical role in acquiring discriminative features from massive data. Traditional FS methods score feature importance and select the top best features by treating all instances equally; Hence, valuable instances like directional outliers (DOs), which are specific outliers closer to other class centres than to their owns, seldom receive particular attention during feature selection. Based on our observation, DOs derive from “misclassified instances” which lead to misclassification. In this paper, we present a novel supervised feature selection method entitled Feature Selection via Directional Outliers Correcting (FSDOC), for accurate data classification. The proposed FSDOC includes an optimization algorithm to capture DOs, and two correcting algorithms to reasonably capture redundant features by correcting DOs with intraclass deviation minimization and interclass relative distance maximization. We give theoretical guarantees and adequate analysis on all algorithms to show the effectiveness of FSDOC. Extensive experiments on fifteen public datasets, and two case studies of deep features and very-high dimensional Fisher Vector selection, demonstrate the superior performance of FSDOC.","tags":["Deviation","Directional outlier","Feature selection","Redundant features","Supervised method"],"title":"Discriminative feature selection with directional outliers correcting for data classification","type":"publication"},{"authors":["Hao Li","Yirui Wu","Hexuan Hu","Hu Lu","Shaohua Wan"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"c2ca79dea98390d8f166305b2c3c7fbf","permalink":"https://hhudelta.github.io/publication/bibm2022/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/bibm2022/","section":"publication","summary":"Deep learning methods have shown significant performance in medical image analysis tasks. However, they generally act like ”black box” without explanations in both feature extraction and decision processes, leading to lack of clinical insights and high risk assessments. To aid deep learning in envisioning diseases with visual clues, we propose Representation Group-Disentangling Network (RGD-Net), which can completely disentangle feature space of input X-ray images into several independent feature groups, each corresponding to a specific disease. Taking several semantically related and labeled X-ray images as input, RGD-Net firstly extracts completely group-disentangled representations of diseases through Group-Disentangle Module, which applies group-swap and linking operations to construct latent space by enforcing semantic consistency of attributes. To prevent learning degenerate representations defined as shortcut problem, we further introduce adversarial constricts on mapping from features to diseases, thus avoiding model collapse with former free-form disentanglement. Experiments on chestxray-14 and ChestXpert datasets demonstrate that RGD-Net are effective in predicting diseases with remarkable advantages, which leverage potential factors contributing to different diseases, thus enhancing interpretability in working patterns of deep learning methods.","tags":[],"title":"Learning Group-Disentangled Representation for Interpretable Thoracic Pathologic Prediction","type":"publication"},{"authors":["Qiran Kong","Yirui Wu","Chi Yuan","Yongli Wang"],"categories":null,"content":"","date":1639008e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639008e3,"objectID":"b28486e302117cf1f66c9630514dbce2","permalink":"https://hhudelta.github.io/publication/bibm2021/","publishdate":"2021-12-09T00:00:00Z","relpermalink":"/publication/bibm2021/","section":"publication","summary":"Supervised based deep learning methods have achieved great success in medical image analysis domain. Essentially, most of them could be further improved by exploring and embedding context knowledge for accuracy boosting. Moreover, they generally suffer from slow convergency and high computing cost, which prevents their usage in a practical scenario. To tackle these problems, we present CT-CAD, context-aware transformers for end-to-end chest abnormality detection on X-Ray images. The proposed method firstly constructs a context-aware feature extractor, which enlarges receptive fields to encode multi-scale context information via an iterative feature fusion scheme and dilated context encoding blocks. Afterwards, deformable transformer detector are built for category classification and location regression, where their deformable attention block attend to a small set of key sampling points, thus allowing the transformer to focus on feature subspace and accelerate convergence speed. Through comparative experiments on Vinbig Chest and Chest Det10 Datasets, the proposed CT-CAD demonstrates its effectiveness and outperforms the existing methods in mAP and training epoches.","tags":[],"title":"CT-CAD: Context-Aware Transformers for End-to-End Chest Abnormality Detection on X-Rays","type":"publication"},{"authors":["Benze Wu","Yirui Wu","Shaohua Wan"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"6958afde5978e9094d377731e216172e","permalink":"https://hhudelta.github.io/publication/euc2021-2/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/euc2021-2/","section":"publication","summary":"In order to predict the unknown image categories, few-shot image classification has recently become a very hot field. However, many methods need a large number of samples to support in order to achieve enough functions. This makes the whole network de amplification to meet a large number of effective feature extraction, and reduces the efficiency of few-shot classification to a certain extent. To solve these problems, we propose a dilate convolutional network with data enhancement. This network can not only meet the necessary features of image classification without increasing the number of samples, but also has a structure that utilizes a large number of effective features without sacrificing efficiency. The cutout structure can enhance the data by adding a fixed area 0 mask matrix in the process of image input. The structure of FAU uses dilate convolution and uses the characteristics of a sequence to improve the efficiency of the network.","tags":[],"title":"An Image Enhancement Method for Few-shot Classification","type":"publication"},{"authors":["Yirui Wu","Pengfei Han","Zhan Zheng"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"7c3080248dcafb164cfdc73f86e24d5d","permalink":"https://hhudelta.github.io/publication/jrtip2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/jrtip2021/","section":"publication","summary":"Water resources are critical for human survival and require continuous monitoring for protection. Machine learning methods have been successfully applied to the identification of water bodies by analyzing remote sensing images. The proposed method first performs pixel-level classification to detect abnormal changes using visual word patterns, then applies a block division method for parallel processing based on a MapReduce structure. This approach allows for accurate and rapid detection of water body variations with instant feedback. Experiments on self-collected datasets demonstrate superior efficiency compared to comparative methods.","tags":[],"title":"Instant water body variation detection via analysis on remote sensing imagery","type":"publication"},{"authors":["Qiran Kong","Yirui Wu","Shaohua Wan"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"d9252ac59c14bb2cecc252bfc3bb6a68","permalink":"https://hhudelta.github.io/publication/euc2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/euc2021/","section":"publication","summary":"Although deep learning has achieved great success in object detection recently, scene text detection remains a challenging task due to inherent difficulties in locating texts in complex scenes. Many approaches are inspired by segmentation methods to detect arbitrarily shaped scene text. However, most segmentation-based methods are computationally expensive and require significant refinements for accurate results. To address this issue, we propose PolarText, a novel single-stage method that detects text regions by generating contour points in polar coordinates. PolarText reduces computation costs by directly regressing contour points instead of pixels and better aligns with the intrinsic characteristics of text instances using centers and contours, mitigating boundary pixel mislabeling caused by pixel-level labeling. The network introduces Polar IoU loss and polar centerness to adapt effective paradigms from box representation for polar representation. Additionally, we incorporate a bounding box branch to handle text detection, as most text instances are approximately rectangular. Experimental results on CTW 1500 and ICDAR 2015 datasets show that PolarText achieves superior accuracy and efficiency compared to existing methods.","tags":[],"title":"PolarText: Single-stage Scene Text Detection with Polar Representation","type":"publication"},{"authors":["Guangchen Shi","Yirui Wu"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"edf9debea5faef0d3a25eb2d013e8fe8","permalink":"https://hhudelta.github.io/publication/jig2021/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/jig2021/","section":"publication","summary":"Extracting textual information from scene images is crucial for understanding the content of a scene, and text detection forms the foundation for text recognition and comprehension. Scene text detection is currently one of the most challenging tasks, receiving increasing attention from researchers.This paper proposes an efficient arbitrary-shaped text detector: the Non-Local Pixel Aggregation Network (NL-PAN). This method utilizes a feature pyramid enhancement module and a feature fusion module for lightweight feature extraction, ensuring speed advantages. It also introduces non-local operations to enhance the feature extraction capability of the backbone network, improving detection accuracy. Non-local operations are an attention mechanism that captures the inherent relationships between text pixels. Additionally, a feature vector fusion module is designed to integrate features from different scales, enhancing the feature representation of scene text instances with varying scales.The proposed method is compared with other methods on three scene text datasets, showing outstanding performance in both speed and accuracy. On the ICDAR 2015 dataset, this method improves the F-score by 1.5% over the best-performing method, achieving a detection speed of 23.1 FPS. On the CTW1500 dataset, the F-score is improved by 1.8% over the best method, with a detection speed of 71.8 FPS. On the Total-Text dataset, the F-score is improved by 0.8%, and the detection speed reaches 64.3 FPS, far surpassing other methods.The proposed method balances accuracy and real-time performance, achieving leading results in both accuracy and speed.","tags":[],"title":"Arbitrary shape scene text detection based on pixel aggregationandfeatureenhancement","type":"publication"},{"authors":["Tingting Hang","Jun Feng","Yirui Wu","Le Yan","Yunfeng Wang"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"d102c9790b979e9d9ec6b4eae749017e","permalink":"https://hhudelta.github.io/publication/eswa2021/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/eswa2021/","section":"publication","summary":"Joint extraction of entities and overlapping relations has attracted considerable attention in recent research. Existing relation extraction methods rely on a training set that is labeled by the distant supervision method for supervised relation extraction. However, the drawbacks of these methods are that large-scale unlabeled data cannot be used and the quality of labeled data cannot be guaranteed. Moreover, owing to the relatively complex overlapping relations, it is difficult to perform joint entity-relation extraction accurately. In this study, we propose an end-to-end neural network model (BERT-JEORE) for the joint extraction of entities and overlapping relations. First, we use the BERT-based parameter-sharing layer to capture the joint features of entities and overlapping relations. Then, we implement the source-target BERT model to assign entity labels to each token in a sentence, thereby expanding the amount of labeled data and improving their quality. Finally, we design a three-step overlapping relations extraction model and use it to predict the relations between all entity pairs. Experiments conducted on two public datasets show that BERT-JEORE achieves the best current performance and outperforms the baseline models by a significant margin. Further analysis shows that our model can effectively capture different types of overlapping relational triplets in a sentence.","tags":[],"title":"Joint Extraction of Entities and Overlapping Relations Using Source-Target Entity Labeling","type":"publication"},{"authors":["Yirui Wu","Wenxiang Liu","Shaohua Wan"],"categories":null,"content":"","date":1627776e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776e3,"objectID":"0a14eb20d590e11e796065be1ec7d0fd","permalink":"https://hhudelta.github.io/publication/jvcir2021/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/jvcir2021/","section":"publication","summary":"Inspired by instance segmentation algorithms, researchers have proposed a variety of segmentation-based methods for text detection, achieving remarkable results on scene text with arbitrary orientation and large aspect ratios. Following their success, we believe cascade architecture and extracting contextual information in multiple aspects are powerful in boosting performance on the basis of segmentation-based methods, especially in decreasing false positive texts in complex natural scenes. Based on such considerations, we propose a multiple-context-aware and cascade CNN structure, which appropriately encodes multiple categories of context information into a cascade R-CNN framework. Specifically, the proposed method consists of two stages: feature generation and cascade detection. In the first stage, we define the ISTK (Isolated Selective Text Kernel) module to refine the feature map, which sequentially encodes channel-wise and kernel-size attention information by designing multiple branches and different kernel sizes in isolated form. Afterwards, we build long-range spatial dependencies in the feature map via non-local operations. Built on the contextual feature map, the Cascade Mask R-CNN structure progressively refines the accurate boundaries of text instances with a multi-stage framework. We conduct comparative experiments on ICDAR2015 and 2017-MLT datasets, where the proposed method outperforms comparative methods in terms of effectiveness and efficiency.","tags":[],"title":"Multiple Attention Encoded Cascade R-CNN for Scene Text Detection","type":"publication"},{"authors":["Yirui Wu","Wenqin Mao","Jun Feng"],"categories":null,"content":"","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626998400,"objectID":"5298b64aa6afc75bcebf7d2d22aae58e","permalink":"https://hhudelta.github.io/publication/monet2021/","publishdate":"2021-07-23T00:00:00Z","relpermalink":"/publication/monet2021/","section":"publication","summary":"Cloud/edge computing and deep learning greatly improve the performance of semantic understanding systems, where cloud/edge computing provides flexible, pervasive computation and storage capabilities to support variant applications, and deep learning models can comprehend text inputs by consuming computing and storage resources. We propose implementing an intelligent online customer service system powered by both technologies. This method jointly models two subtasks, intent recognition and slot filling, in an end-to-end neural network, enhancing feature representation using attention schemes and context information. We deploy this method in an intelligent dialogue system for electrical customer service, with experiments showing promising results on both public and self-collected datasets.","tags":[],"title":"AI for Online Customer Service: Intent Recognition and Slot Filling Based on Deep Learning Technology","type":"publication"},{"authors":["Yirui Wu","Hongfei Guo","Cheng Qian","Wenpeng Wang"],"categories":null,"content":"","date":1625702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625702400,"objectID":"2d788e7335e3cfd5d5a14bd3ca9e8dd6","permalink":"https://hhudelta.github.io/publication/yr2021/","publishdate":"2021-07-08T00:00:00Z","relpermalink":"/publication/yr2021/","section":"publication","summary":"面对洪水发生频率低且机制复杂的问题，提出了一类深度神经网络模型( ET － LSTM) 。该模型通过构建特征增强模块提升了小样本情况下的洪水预报能力，通过结合时序感知模块的深度神经网络模型，构建洪水因子与径流量间的非线性关系，挖掘洪水因子间的隐含时序关联关系。首先利用一维卷积神经网络构建洪水深度特征表达; 然后，结合瓶颈( BottleNeck) 结构设计，通过特征通道间的信息交换，增强洪水深度特征的表达能力; 最后，构建时序无关和时序相关模块，分别提取深度特征中的时序相关与时序无关部分，进一步提升深度特征的时变表达能力，并在流域数据集上进行对比分析。结果表明: 该方法在模拟精度、相关性系数等指标上优于对比方法，能够更好地拟合真实径流量数据，提升洪水预报的准确性与预见期。","tags":[],"title":"基于特征增强与时序感知的洪水预报模型","type":"publication"},{"authors":["Guangchen Shi","Yirui Wu","Shivakumara Palaiahnakote","Umapada Pal","Tong Lu"],"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"029a60c80575cae8783cf12e1a7ec0d4","permalink":"https://hhudelta.github.io/publication/icme2021/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/icme2021/","section":"publication","summary":"Few-shot segmentation has become a research focus for making predictions on unseen classes. However, most methods rely on pixel-level annotation, requiring significant manual effort, and the diversity in feature representation of same-category objects due to size, appearance, or layout differences poses additional challenges. To address these issues, the proposed active-reference network (ARNet) introduces an active-reference mechanism that supports accurate segmentation with cooccurrent objects in support or query images and relaxes the need for precise pixel-level labeling by allowing weak boundary labeling. Additionally, a category-modulation module (CMM) is applied to fuse features from multiple support images, selectively forgetting irrelevant information and enhancing key features. Experiments on the PASCAL-5i dataset show ARNet achieves a mean IOU score of 56.5% for 1-shot and 59.8% for 5-shot segmentation, outperforming the current state-of-the-art methods by 0.5% and 1.3%, respectively.","tags":[],"title":"ARNet: Active-Reference Network for Few-Shot Image Semantic Segmentation","type":"publication"},{"authors":["Jun Feng","Zhongyi Wang","Yirui Wu","Yuqi Xi"],"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"52ed2159d4f7d12e6b2e98ef0626ce5b","permalink":"https://hhudelta.github.io/publication/ijcnn2021/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/ijcnn2021/","section":"publication","summary":"Intelligent flood forecasting systems provide an effective means to predict flood disasters. Accurate flood flow value prediction is a challenging task as it is influenced by both spatial and temporal relationships among flood factors. Popular deep learning architectures like Long Short-Term Memory (LSTM) networks lack the ability to model the spatial correlations of hydrological data, which hinders achieving satisfactory prediction results. Additionally, not all temporal information is equally valuable for flood forecasting. This paper proposes a novel Spatial and Temporal Aware Graph Convolutional Network (ST-GCN) for flood prediction, which extracts spatial-temporal information from raw flood data. Furthermore, a temporal attention mechanism is introduced to weight the importance of different time steps, leveraging global temporal information to improve flood prediction accuracy. Experimental results on two self-collected datasets demonstrate that ST-GCN significantly enhances prediction performance compared to existing methods.","tags":[],"title":"Spatial and Temporal Aware Graph Convolutional Network for Flood Forecasting","type":"publication"},{"authors":["Yirui Wu","Yuntao Ma","Shaohua Wan"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"77e78615780f7fbdc0c960fb0065de86","permalink":"https://hhudelta.github.io/publication/spic2021/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/spic2021/","section":"publication","summary":"The goal of Visual Question Answering (VQA) is to answer questions about images. For the same picture, there are often completely different types of questions. Therefore, the main difficulty of the VQA task lies in how to properly reason relationships among multiple visual objects according to types of input questions. To solve this difficulty, this paper proposes a deep neural network to perform multi-modal relation reasoning in multi-scales, which successfully constructs a regional attention scheme to focus on informative and question-related regions for better answering. Specifically, we first design a regional attention scheme to select regions of interest based on informative evaluation computed by a question-guided soft attention module. Afterwards, features computed by the regional attention scheme are fused in scaled combinations, generating more distinctive features with scalable information. Due to the designs of regional attention and multi-scale property, the proposed method is capable of describing scaled relationships from multi-modal inputs to offer accurate question-guided answers. By conducting experiments on the VQA v1 and VQA v2 datasets, we show that the proposed method has superior efficiency compared to most existing methods.","tags":[],"title":"Multi-Scale Relation Reasoning for Multi-Modal Visual Question Answering","type":"publication"},{"authors":["Yuntao Ma","Tong Lu","Yirui Wu"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"395858670249b8dff9af3c4155fb6ce1","permalink":"https://hhudelta.github.io/publication/icpr2020-1/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/icpr2020-1/","section":"publication","summary":"One of the main challenges in visual question answering (VQA) is properly reasoning relations among visual regions involved in the question. In this paper, we propose a novel neural network for question-guided relational reasoning at multiple scales in VQA, where each image region is enhanced through regional attention. Specifically, we introduce a regional attention module consisting of both soft and hard attention mechanisms to select informative regions of the image based on question-guided evaluations. Different combinations of informative regions are then concatenated with question embeddings across scales to capture relational information. The relational reasoning module extracts question-based relationships among regions, with the multi-scale mechanism enhancing the model's sensitivity to numbers and its ability to model diverse relationships. Experimental results demonstrate that our approach achieves state-of-the-art performance on the VQA v2 dataset.","tags":[],"title":"Multi-scale Relational Reasoning with Regional Attention for Visual Question Answering","type":"publication"},{"authors":["Pengyu Yu","Yirui Wu","Benze Wu"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"a95affc5313fa8458f10a150ce512e69","permalink":"https://hhudelta.github.io/publication/hpcc2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/hpcc2020/","section":"publication","summary":"In order to ensure code quality, it's necessary to construct portraits for developers, which could analyze their behavior to provide personalized programming suggestions. However, most of the existing developer portrait algorithms only use global features and ignore local features extracted from log texts, which leads to the lack of comprehensive personality analysis. To solve this problem, the proposed method proposes a novel developer portrait model, which could describe developers' programming styles more accurately with both global and local information extracted from texts. The proposed model firstly collects the log data produced in the process of continuous integration development. Afterwards, the proposed method proposes the personality portrait model based on BERT-Capsule network, which successfully combines global semantic features and local emotional features. The experimental results show that the proposed BERT-Capsule model can effectively extract the contextual information and the local emotional information of the text, thus improving classification performance of the developer portrait model.","tags":[],"title":"A Novel Developer Portrait Model based on Bert-Capsule Network","type":"publication"},{"authors":["Pengyu Yu","Yirui Wu","Shun Zhao"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"419c25ba7c2ad5061466e34b292e79f9","permalink":"https://hhudelta.github.io/publication/smartcity2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/smartcity2020/","section":"publication","summary":"With the rapid development of information technology, the scale of software products is getting larger. In order to ensure code quality, it's necessary to collect log information in the continuous integration development process, analyze the behavior of developers, build developer user profile models, and provide personalized suggestions to developers based on profile information. All these processes can be defined as goals of user portrait model. Essentially, user portrait model is designed to model users' coding behaviours based on a large amount of data. However, current methods often suffer from the problem of imbalanced data, which is the core challenge for a successful portrait model. In fact, most parts of log information refer to regular programmers, while only few samples correspond to programmers who are supposed to be improved by suggestions from the portrait model. To solve this problem, we propose to adopt SMOTE Algorithm to deal with the imbalanced log data, which is the core innovation of the proposed method. Experiments show the proposed SMOTE Algorithm based model could accurately classify programmers' personality types and offer suggestions.","tags":[],"title":"A Novel SMOTE Algorithm based Portrait Model for Programmers","type":"publication"},{"authors":["Xiaofang Li","Yirui Wu","Wen Zhang","Ruichao Wang","Feng Hou"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"c975954ff13d490d0ae9ab92f54be04d","permalink":"https://hhudelta.github.io/publication/jrtip2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/jrtip2020/","section":"publication","summary":"Super-resolution is generally defined as a process to obtain high-resolution images from low-resolution inputs, which has attracted considerable attention from the image-processing community. This paper aims to analyze, compare, and contrast technical problems, methods, and the performance of super-resolution research, especially real-time super-resolution methods based on deep learning structures. Specifically, we first summarize fundamental problems, categorize algorithms, and analyze possible application scenarios. Increasing attention has been drawn to utilizing convolutional neural networks (CNN) or generative adversarial networks (GAN) to predict high-frequency details lost in low-resolution images. We provide a general overview of background technologies and pay special attention to super-resolution methods based on deep learning architectures for real-time processing, which not only produce desirable reconstruction results but also expand the possible applications of super-resolution to systems like cell phones, drones, and embedded systems. Benchmark datasets are enumerated, and the performance of the most representative approaches is compared to provide a fair view of current methods. Finally, we conclude the paper and suggest ways to improve the use of deep learning methods in real-time image super-resolution.","tags":[],"title":"Deep learning methods in real-time image super-resolution: a survey","type":"publication"},{"authors":["Haifeng Guo","Tong Lu","Yirui Wu"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"d3e0dbf8a1a4b50675fc5b2a51bcdaa7","permalink":"https://hhudelta.github.io/publication/icpr2020/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/icpr2020/","section":"publication","summary":"Object detection based on convolutional neural networks is a key area in computer vision. The illumination component in images significantly affects detection performance, especially under low-light conditions. Although low-light image enhancement can improve image quality and detection performance, existing methods may negatively affect some samples, making it difficult to improve overall detection accuracy in low-light environments. This paper proposes a novel framework that combines low-light enhancement with object detection, enabling end-to-end training. The framework dynamically selects the appropriate enhancement subnetworks for each sample to improve detector performance. The approach consists of two stages: the enhancement stage, which enhances low-light images based on various enhancement methods and outputs corresponding weights, and the detection stage, where the weights provide information for object classification to generate high-quality region proposals, resulting in more accurate detection. Experimental results demonstrate that the proposed method significantly improves detection performance in low-light environments.","tags":[],"title":"Dynamic Low-Light Image Enhancement for Object Detection via End-to-End Training","type":"publication"},{"authors":["Yirui Wu","Xiaozhong Ji","Wanting Ji","Yan Tian","Helen Zhou"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"1510c4ed1b30ee610ac025a4696d7142","permalink":"https://hhudelta.github.io/publication/ncaa2020/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/ncaa2020/","section":"publication","summary":"With the significant power of deep learning architectures, researchers have made much progress on super-resolution in the past few years. However, due to low representational ability of feature maps extracted from nature scene images, directly applying deep learning architectures for super-resolution could result in poor visual effects. Essentially, unique characteristics like low-frequency information should be emphasized for better shape reconstruction, other than treated equally across different patches and channels. To ease this problem, we propose a lightweight context-aware deep residual network named as CASR network, which appropriately encodes channel and spatial attention information to construct context-aware feature map for single-image super-resolution. We firstly design a task-specified inception block with a novel structure of astrous filters and specially chosen kernel size to extract multi-level information from low-resolution images. Then, a Dual-Attention ResNet module is applied to capture context information by dually connecting spatial and channel attention schemes. With high representational ability of context-aware feature map, CASR can accurately and efficiently generate high-resolution images. Experiments on several popular datasets show the proposed method has achieved better visual improvements and superior efficiencies than most of the existing studies.","tags":[],"title":"CASR: a context-aware residual network for single-image super-resolution","type":"publication"},{"authors":["Yirui Wu","Pengyu Yu"],"categories":null,"content":"","date":159624e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":159624e4,"objectID":"58344cb566400dd1b0613b9687aab0d9","permalink":"https://hhudelta.github.io/publication/cybersci2020/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/cybersci2020/","section":"publication","summary":"Because of the differences in code styles and programming levels among developers, it is prone to code irregularities, poor readability, and security vulnerabilities. Although developers can see their problems in the test report, it is difficult to guarantee that they will not make mistakes on the same problem. In this article, we propose a way to build a programming level for developers. The proposed method explores the Stacking model for building a programming level for developers. First, cluster the problems that occurred during the development process, give weight to the category information, score the developer's programming level, and divide the user groups into different categories. Then use the word vector to extract the features of the code defect, generate a feature matrix, and finally pass the feature matrix to the Stacking classifier to classify the defect information and update the developer's programming level portrait. Experimental results show that it is effective in predicting defect code information. In addition, a comparative study with the state-of-the-art method shows that the method is superior to existing methods in terms of classification rate, recall, precision and F-measure.","tags":[],"title":"User Portrait Technology Based on Stacking Mode","type":"publication"},{"authors":["Yirui Wu","Yukai Ding","Jun Feng"],"categories":null,"content":"","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586908800,"objectID":"c1cde949a0cb518dae0e2e1a91103c72","permalink":"https://hhudelta.github.io/publication/ejwcn2020/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/publication/ejwcn2020/","section":"publication","summary":"With a significant development of big data analysis and cloud-fog-edge computing, human-centered computing (HCC) has been a hot research topic worldwide. Essentially, HCC is a cross-disciplinary research domain, in which the core idea is to build an efficient interaction among persons, cyber space, and real world. Inspired by the improvement of HCC on big data analysis, we intend to involve related core and technologies to help solve one of the most important issues in the real world, i.e., flood prediction. To minimize the negative impacts brought by floods, researchers pay special attention to improve the accuracy of flood forecasting with quantity of technologies including HCC. However, historical flood data is essentially imbalanced. Imbalanced data causes machine learning classifiers to be more biased towards patterns with majority samples, resulting in poor classification of pattern with minority samples. In this paper, we propose a novel Synthetic Minority Over-sampling Technique (SMOTE)-Boost-based sparse Bayesian model to perform flood prediction with both high accuracy and robustness. The proposed model consists of three modules, namely, SMOTE-based data enhancement, AdaBoost training strategy, and sparse Bayes model construction. In SMOTE-based data enhancement, we adopt a SMOTE algorithm to effectively cover diverse data modes and generate more samples for prediction pattern with minority samples, which greatly alleviates the problem of imbalanced data by involving experts’ analysis and users’ intentions. During AdaBoost training strategy, we propose a specifically designed AdaBoost training strategy for sparse Bayesian model, which not only adaptively and incrementally increases prediction ability of Bayesian model, but also prevents its overfitting performance. Essentially, the design of AdaBoost strategy helps keep balance between prediction ability and model complexity, which offers different but effective models over diverse rivers and users. Finally, we construct a sparse Bayesian model based on AdaBoost training strategy, which could offer flood prediction results with high rationality and robustness. We demonstrate the accuracy and effectiveness of the proposed model for flood prediction by conducting experiments on a collected dataset with several comparative methods.","tags":[],"title":"SMOTE-Boost-based sparse Bayesian model for flood prediction","type":"publication"},{"authors":["Yirui Wu","Haohang Wang","Dabao Wei","Jun Feng"],"categories":null,"content":"","date":1584230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584230400,"objectID":"da5e7abb9dff40e39342cae64c8869c5","permalink":"https://hhudelta.github.io/publication/hhu2020/","publishdate":"2020-03-15T00:00:00Z","relpermalink":"/publication/hhu2020/","section":"publication","summary":"By representing hydraulic events via constructing multiple features of the watershed spatio-temporal scene，this study proposed a construction method of watershed scene pattern library via the spatio-temporal multiple features． The original hydrological data was firstly divided into events to remove the spatio-temporal redundancy of scene element data． Based on the analysis of element association relation，the corresponding features of scene elements were constructed via multiple ways． Afterwards，key features of watershed scene were selected by the feature selection algorithm to realize the scene initialization． Finally，the initial scene was regarded as the feature space，where the cluster extraction of scene pattern and scene pattern library construction could be carried out．Experimental results show that the proposed method can not only extract the key spatio-temporal scene data of hydrological events，but also mine scene patterns to form a scene pattern library，thus providing accurate and efficient prediction results for the hydrological event with small dataset.","tags":[],"title":"Construction method of watershed scene pattern library via spatio-temporal multiple features","type":"publication"},{"authors":["Yirui Wu","Dabao Wei","Jun Feng","Xiaolong Xu"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"5a64e49a79fe72d799712d2d672b21f5","permalink":"https://hhudelta.github.io/publication/scn2020/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/scn2020/","section":"publication","summary":"With the development of the fifth-generation networks and artificial intelligence technologies, new threats and challenges have emerged to wireless communication systems, especially in cybersecurity. In this paper, we offer a review on attack detection methods involving deep learning techniques. Specifically, we first summarize fundamental problems of network security and attack detection and introduce several successful related applications using deep learning structures. We categorize deep learning methods and focus on attack detection methods built on different kinds of architectures, such as autoencoders, generative adversarial networks, recurrent neural networks, and convolutional neural networks. We also present some benchmark datasets and compare the performance of different approaches to show the current working state of attack detection methods using deep learning structures. Finally, we summarize the paper and discuss ways to improve the performance of attack detection with deep learning techniques.","tags":[],"title":"Network Attacks Detection Methods Based on Deep Learning Techniques: A Survey","type":"publication"},{"authors":["Xiaoge Song","Yirui Wu","Wenhai Wang","Tong Lu"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"cde2811c694c220fc9822ddf3ac793a8","permalink":"https://hhudelta.github.io/publication/mmm2020-1/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/mmm2020-1/","section":"publication","summary":"Benefit from the development of deep neural networks, scene text detectors have progressed rapidly over the past few years and achieved outstanding performance on several standard benchmarks. However, most existing methods adopt quadrilateral bounding boxes to represent texts, which are usually inadequate to deal with multi-shaped texts such as the curved ones. To keep consist detection performance on both quadrilateral and curved texts, we present a novel representation, i.e., text kernel, for multi-shaped texts. On the basis of text kernel, we propose a simple yet effective scene text detection method, named as TK-Text. The proposed method consists of three steps, namely text-context-aware network, segmentation map generation and text kernel based post-clustering. During text-context-aware network, we construct a segmentation-based network to extract feature map from natural scene images, which are further enhanced with text context information extracted from an attention scheme TKAB. In segmentation map generation, text kernels and rough boundaries of text instances are segmented based on the enhanced feature map. Finally, rough text instances are gradually refined to generate accurate text instances by performing clustering based on text kernel. Experiments on public benchmarks including SCUT-CTW1500, ICDAR 2015 and ICDAR 2017 MLT demonstrate that the proposed method achieves competitive detection performance comparing with the existing methods.","tags":[],"title":"TK-Text: Multi-shaped Scene Text Detection via Instance Segmentation","type":"publication"},{"authors":["Xiaozhong Ji","Yirui Wu","Tong Lu"],"categories":null,"content":"","date":1577145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577145600,"objectID":"cfe5914d59400a327dc7b2cd7190d20c","permalink":"https://hhudelta.github.io/publication/mmm2020/","publishdate":"2019-12-24T00:00:00Z","relpermalink":"/publication/mmm2020/","section":"publication","summary":"Deep learning models have achieved significant success in various vision-based applications. However, directly applying deep architectures for single image super-resolution (SISR) results in poor visual effects, such as blurry patches and loss of details, primarily because low-frequency information is treated ambiguously across different patches and channels. To address this issue, we propose a novel context-aware deep residual network with promotion gates, named G-CASR, for SISR. The G-CASR network consists of a sequence of G-CASR modules designed to transform low-resolution features into high-informative features. Each module incorporates a dual-attention residual block (DRB) that captures rich and varying context information through spatial and channel attention. A promotion gate (PG) is applied in each module to analyze the inherent characteristics of input data, enhancing contributive information while suppressing irrelevant data. Experiments on five public datasets (Set5, Set14, B100, Urban100, and Manga109) show that G-CASR outperforms recent methods like SRCNN, VDSR, lapSRN, and EDSR with an average improvement of 1.112 for PSNR and 0.0255 for SSIM. Additionally, the G-CASR model requires only about 25% of the memory cost compared to EDSR.","tags":[],"title":"Context-Aware Residual Network with Promotion Gates for Single Image Super-Resolution","type":"publication"},{"authors":["Yao Xiao","Minglong Xue","Tong Lu","Yirui Wu","Shivakumara Palaiahnakote"],"categories":null,"content":"","date":1567296e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296e3,"objectID":"d7f93e6d1bc04f06a03ea039eabb9847","permalink":"https://hhudelta.github.io/publication/icdar2019/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/icdar2019/","section":"publication","summary":"The existing deep learning based state-of-theart scene text detection methods treat scene texts a type of general objects, or segment text regions directly. The latter category achieves remarkable detection results on arbitrary orientation and large aspect ratios of scene texts based on instance segmentation algorithms. However, due to the lack of context information with consideration of scene text unique characteristics, directly applying instance segmentation to text detection task is prone to result in low accuracy, especially producing false positive detection results. To ease this problem, we propose a novel text-context-aware scene text detection CNN structure, which appropriately encodes channel and spatial attention information to construct context-aware and discriminative feature map for multi-oriented and multi-language text detection tasks. With high representation ability of text context-aware feature map, the proposed instance segmentation based method can not only robustly detect multi-oriented and multi-language text from natural scene images, but also produce better text detection results by greatly reducing false positives. Experiments on ICDAR2015 and ICDAR2017-MLT datasets show that the proposed method has achieved superior performances in precision, recall and F-measure than most of the existing studies.","tags":[],"title":"A Text-Context-Aware CNN Network for Multi-oriented and Multi-language Scene Text Detection","type":"publication"},{"authors":["Yirui Wu","Yukai Ding","Jun Feng"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"7eb589a326edb0dec82c153f3c2c1ee9","permalink":"https://hhudelta.github.io/publication/cpscom2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/cpscom2019/","section":"publication","summary":"Flood is a common disaster in our daily life. It's of great significance to improve the accuracy of flood forecasting, in order to help get rid of loss in both lives and property. However, there exists a uneven distribution of samples in factors of flood forecasting. Therefore, it's difficult to train a single datadriven model to describe the entire complex process of flood generation. In this paper, we propose a novel SMOTEBoost algorithm to perform flood forecasting with both high accuracy and robustness. Specifically, we firstly adopt a SMOTE algorithm to generate virtual samples, which greatly alleviates the problem of uneven sample distribution. Afterwards, we propose a sparse Bayesian model, which is trained with AdaBoost training strategy by improving its performance in over-fitting. At last, we carry out experiments on flood foretasting in Changhua river, which shows that the proposed method achieves high accuracy in prediction, thus owing practical usage.","tags":[],"title":"Sparse Bayesian Flood Forecasting Model Based on SMOTEBoost","type":"publication"},{"authors":["Yukai Ding","Yuelong Zhu","Yirui Wu","Jun Feng","Zirun Cheng"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"d146c0c72492168c6f543daf2a594845","permalink":"https://hhudelta.github.io/publication/cpscom2019-1/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/cpscom2019-1/","section":"publication","summary":"In order to reduce the loss caused by flood, a large number of researches based on data, algorithms, machine learning and other technical means are used to realize flood forecasting. It will be a kind of flexible research method to realize the flood prediction of small and medium-sized rivers through intelligent models such as neural network. The area of small and medium-sized river basins is relatively small. Precipitation, soil moisture, evaporation and other factors can affect the timely total runoff prediction. However, not all the hydrological features is always valuable for flood forecasting, even at some time, noise of the factors will have larger interference on forecast process. Therefore, dynamic extraction of key feature vectors from various hydrological information plays an important role in flood forecasting. This paper proposed a flood forecasting model (STA-LSTM model) by using long short-term memory model (LSTM) and attention mechanism. We take the Lech river basin in Europe as the experimental basin and the results show that STA-LSTM performs well and has high research value with comparison of support vector machine (SVM), fully connected network (FCN) and original LSTM","tags":[],"title":"Spatio-Temporal Attention LSTM Model for Flood Forecasting","type":"publication"},{"authors":["Yirui Wu","Yuechao He","Palaiahnakote Shivakumara","Ziming Li","Hongxin Guo","Tong Lu"],"categories":null,"content":"","date":1559260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559260800,"objectID":"190f989a14bc356ae4606f091cf69ae6","permalink":"https://hhudelta.github.io/publication/cit2019/","publishdate":"2019-05-31T00:00:00Z","relpermalink":"/publication/cit2019/","section":"publication","summary":"Due to natural disaster and global warning, one can expect unexpected fire, which causes panic among people and extent to death. To reduce the impact of fire, the authors propose a new method for predicting and rating fire in video through deep-learning models in this work such that rescue team can save lives of people. The proposed method explores a hybrid deep convolutional neural network, which involves motion detection and maximally stable extremal region for detecting and rating fire in video. Further, the authors propose to use a channel-wise attention mechanism of the deep neural network for detecting rating of fire level. Experimental results on a large dataset show the proposed method outperforms the existing methods for detecting and rating fire in video.","tags":[],"title":"Channel-wise attention model-based fire and rating level detection in video","type":"publication"},{"authors":["Yirui Wu","Lianglei Wei","Yucong Duan"],"categories":null,"content":"","date":1555891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555891200,"objectID":"e9afaf0bf013cd56464c7ac9a9cf6490","permalink":"https://hhudelta.github.io/publication/ci2019/","publishdate":"2019-04-22T00:00:00Z","relpermalink":"/publication/ci2019/","section":"publication","summary":"With the rapid development of RGB-D cameras and pose estimation techniques, action recognition based on three-dimensional skeleton data has gained significant attention in the artificial intelligence community. In this paper, we incorporate temporal pattern descriptors of joint positions with the currently popular long short-term memory (LSTM)-based learning scheme to obtain accurate and robust action recognition. Considering that actions are essentially formed by small subactions, we first utilize a two-dimensional wavelet transform to extract temporal pattern descriptors in the frequency domain for each subaction. Afterward, we design a novel LSTM structure to extract deep features, which model a long-term spatiotemporal correlation between body parts. Since temporal pattern descriptors and LSTM deep features can be regarded as multimodal representations for actions, we fuse them with an autoencoder network to achieve a more effective feature descriptor for action recognition. Experimental results on three challenging data sets with several comparative methods demonstrate the effectiveness of the proposed method for three-dimensional action recognition.","tags":[],"title":"Deep spatiotemporal LSTM network with temporal pattern feature for 3D human action recognition","type":"publication"},{"authors":["Yisheng Yue","Palaiahnakote Shivakumara","Yirui Wu","Liping Zhu","Tong Lu","Umapada Pal"],"categories":null,"content":"","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"9d3a4b67775a4b0a672d61850a19a90f","permalink":"https://hhudelta.github.io/publication/mmm2019/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/publication/mmm2019/","section":"publication","summary":"Due to the introduction of deep learning for text detection and recognition in natural scenes, and the increase in detecting fake images in crime applications, automatically generating fake character images has now received greater attentions. This paper presents a new system named Fake Character GAN (FCGAN). It has the ability to generate fake and artificial scene characters that have similar shapes and colors with the existing ones. The proposed method first extracts shapes and colors of character images. Then, it constructs the FCGAN, which consists of a series of convolution, residual and transposed convolution blocks. The extracted features are then fed to the FCGAN to generate fake characters and verify the quality of the generated characters simultaneously. The proposed system chooses characters from the benchmark ICDAR 2015 dataset for training, and further validated by conducting text detection and recognition experiments on input and generated fake images to show its effectiveness.","tags":[],"title":"An Automatic System for Generating Artificial Fake Character Images","type":"publication"},{"authors":["Yirui Wu","Weigang Xu","Qinghan Yu","Jun Feng","Tong Lu"],"categories":null,"content":"","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"3bffa3422650abd8b0ddee955dd60c17","permalink":"https://hhudelta.github.io/publication/mmm2019-1/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/publication/mmm2019-1/","section":"publication","summary":"To minimize the negative impacts brought by floods, researchers pay special attention to the problem of flood prediction. In this paper, we propose a hierarchical Bayesian network based incremental model to predict floods for small rivers. The proposed model not only appropriately embeds hydrology expert knowledge with Bayesian network for high rationality and robustness, but also designs an incremental learning scheme to improve the self-improving and adaptive ability of the proposed model. Following the idea of a famous hydrology model, i.e., XAJ model, we firstly present the construction of hierarchical Bayesian network as local and global network construction. After that, we propose an incremental learning scheme, which selects proper incremental data to improve the completeness of prior knowledge and updates parameters of Bayesian network to prevent training from scratch. We demonstrate the accuracy and effectiveness of the proposed model by conducting experiments on a collected dataset with one comparative method.","tags":[],"title":"Hierarchical Bayesian Network Based Incremental Model for Flood Prediction","type":"publication"},{"authors":["Yirui Wu","Zhouyu Meng","Palaiahnakote Shivakumara","Tong Lu"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"732dbe3e3a4340fe99bd9fb21c697709","permalink":"https://hhudelta.github.io/publication/mjcs2018/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/publication/mjcs2018/","section":"publication","summary":"Deep neural networks (DNN) have shown significant performance in several domains including computer vision and machine learning. Convolutional Neural Networks (CNN), known as a particular type of DNN, have shown their promising potentials in discovering vision-based patterns from quantity of labeled images. Many CNN-based algorithms are thus proposed to solve the problem of object detection and object recognition. However, CNN-based systems are hard to deploy on embedded systems due to their computationally and storage intensive. In this paper, we propose a method to compress convolutional neural network to decrease its computation and storage cost by exploiting inherent redundancy property of parameters in different kinds of layers of CNN architecture. During the compression, we firstly construct parameter matrices from different kinds of layers and convert parameter matrices to frequency domain through discrete cosine transform (DCT). Due to the smooth property of parameters when processing images, the resulting frequency matrices are dominated by low-frequency components. We thus prune high-frequency part to emphasize the dominating part of frequency matrix and make the frequency matrix sparse. Then, the sparse frequency matrices are sampled with distributed random Gaussian matrix under the guiding of compress sensing. Finally, we retrain the network with the sampling matrices to fine-tune the remaining parameters. We evaluate the proposed method on several typical convolutional neural network and show it outperforms one latest compression approach.","tags":[],"title":"Compressive Sensing Based Convolutional Neural Network for Object Detection","type":"publication"},{"authors":["Yirui Wu","Yisheng Yue","Xiao Tan","Wei Wang","Tong Lu"],"categories":null,"content":"","date":1538352e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352e3,"objectID":"cdf525cb4a65e64a6bd23cb00da5e5a0","permalink":"https://hhudelta.github.io/publication/icip2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/icip2018/","section":"publication","summary":"Classifying human chromosomes from input cell images, i.e., karyotyping, requires domain expertise and quantity of manual effort to perform. In this paper, we propose an end-to-end chromosome karyotyping method, which can automatically detect, segment and classify chromosomes from cell images. During detection, we explore Extremal Regions (ER) to obtain chromosome candidates in input images. During segmentation, we segment overlapping chromosome candidates by approximating chromosome shapes with eclipses. In classification, we first propose Multiple Distribution Generative Advertising Network (MD-GAN) to effectively cover diverse data modes and generate more labeled samples for data augmentation. Then, we finetune pre-trained convolutional neural network (CNN) to classify chromosomes with samples generated by MD-GAN. We demonstrate the accuracy of the proposed end-to-end method in detecting, segmenting and classifying by experiments on a self-collected dataset. Experiments also prove data augmentation with MD-GAN could improve classification performance of CNN.","tags":[],"title":"End-To-End Chromosome Karyotyping with Data Augmentation Using GAN","type":"publication"},{"authors":["Zhaoyang Liu","Yirui Wu","Yukai Ding","Jun Feng","Tong Lu"],"categories":null,"content":"","date":1537488e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537488e3,"objectID":"fbb3bc2319a953139bb1b3ed9905fe54","permalink":"https://hhudelta.github.io/publication/pcm2018/","publishdate":"2018-09-21T00:00:00Z","relpermalink":"/publication/pcm2018/","section":"publication","summary":"To minimize damages brought by floods, researchers pay special attentions to solve the problem of flood prediction. Multiple factors, including rainfall, soil category, the structure of riverway and so on, affect the prediction of sequential flow rate values, but factors are not always informative for flood prediction. Extracting discriminative and informative features thus plays a key role in predicting flow rates. In this paper, we propose a context and temporal aware attention model for flood prediction based on a quantity of collected flood factors. We build our model on top of Long Short-Term Memory (LSTM) networks, which selectively focuses on informative factors and pays different levels of attentions to the outputs of different cells. The proposed CT-LSTM network assigns time-varying weights to input factors at all the cells of LSTM network, and allocates temporal-dependent weights to the outputs of each LSTM cell for boosting prediction performance. Experimental results on a benchmark flood dataset with several comparative methods demonstrate the effectiveness of the proposed CT-LSTM network for flood prediction.","tags":[],"title":"Context and Temporal Aware Attention Model for Flood Prediction","type":"publication"},{"authors":["Yirui Wu","Zhaoyang Liu","Weigang Xu","Jun Feng","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"764ac07cde336d6869a2ee8b007693ec","permalink":"https://hhudelta.github.io/publication/icpr2018-1/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/icpr2018-1/","section":"publication","summary":"To minimize the negative impacts brought by floods, researchers from pattern recognition community utilize artificial intelligence based methods to solve the problem of flood prediction. Inspired by the significant power of Long Short-Term Memory (LSTM) networks in modeling the dynamics and dependencies of sequential data, we intend to utilize LSTM networks to predict sequential flow rate values based on a set of collected flood factors. Since not all factors are informative for flood prediction and the irrelevant factors often bring a lot of noise, we need to pay more attention to the informative ones. However, original LSTM doesn't have strong attention capability. Hence we propose an context-aware attention LSTM (CA-LSTM) network for flood prediction, which is capable to selectively focus on informative factors. During training, the local context-aware attention model is constructed by learning probability distributions between flow rate and hidden output of each LSTM cell. During testing, the learned local attention model assign weights to adjust relations between input factors and predictions at all steps of LSTM network. We conduct experiments on a flood dataset with several comparative methods to demonstrate high accuracy of the proposed method and the effectiveness of the proposed context-aware attention model.","tags":[],"title":"Context-Aware Attention LSTM Network for Flood Prediction","type":"publication"},{"authors":["Yirui Wu","Zhikai Li","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"35c690d4538e4497c4d088af246c12ad","permalink":"https://hhudelta.github.io/publication/icpr2018/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/icpr2018/","section":"publication","summary":"Simultaneous Localization and Mapping (SLAM) is difficult to deploy in the embedded systems due to its high computation cost and stable input requirements. Building on excellent algorithms of recent years, we present Em-SLAM, a monocular SLAM method which is fast and robust in the embedded system. We present Em-SLAM in three stages comprising initial pose estimation, iterative pose optimization and correspondences, and mapping with nearest frame queue. During the first stage, we perform stable initial pose estimation based on the matched ORB features extracted around the selected key points. Regarding initial pose and corresponding key points as input, the second stage of Em-SLAM iteratively optimizes these inputs values by tracking key points in the new frames. At the last stage, we firstly determine keyframes with the help of the proposed nearest frame queue and then design a greedy search algorithm to find matched ORB features between keyframes, which are adopted for compact and robust map reconstruction. Due to the special designs for the embedded systems, Em-SLAM demonstrates a high accurate and fast performance on the embedded system for all SLAM tasks: tracking, mapping and loop closing. We evaluate Em-SLAM on he most popular datasets by comparing with one latest SLAM method.","tags":[],"title":"Em-SLAM: a Fast and Robust Monocular SLAM Method for Embedded Systems","type":"publication"},{"authors":["Yirui Wu","Weigang Xu","Jun Feng","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"f01e4b65defe22116a7e27ccaeaa3972","permalink":"https://hhudelta.github.io/publication/icpr2018-2/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/icpr2018-2/","section":"publication","summary":"To minimize the negative impacts brought by floods, researchers from pattern recognition community pay special attention to the problem of flood prediction by involving technologies of machine learning. In this paper, we propose to construct hierarchical Bayesian network to predict floods for small rivers, which appropriately embed hydrology expert knowledge for high rationality and robustness. We present the construction of the hierarchical Bayesian network in two stages comprising local and global network construction. During the local network construction, we firstly divide the river watershed into small local regions. Following the idea of a famous hydrology model - the Xinanjiang model, we establish the entities and connections of the local Bayesian network to represent the variables and physical processes of the Xinanjiang model, respectively. During the global network construction, intermediate variables for local regions, computed by the local Bayesian network, are coupled to offer an estimation for time-varying values of flow rate by proper inferences of the global network. At last, we propose to improve the output of Bayesian network by utilizing former flow rate values. We demonstrate the accuracy and robustness of the proposed method by conducting experiments on a collected dataset with several comparative methods.","tags":[],"title":"Local and Global Bayesian Network based Model for Flood Prediction","type":"publication"},{"authors":["Sauradip Nag","Palaiahnakote Shivakumara","Yirui Wu","Umapada Pal","Tong Lu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"95887dd24cb44ba2c915ca884968ff72","permalink":"https://hhudelta.github.io/publication/icfhr2018/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/icfhr2018/","section":"publication","summary":"Identifying crime for forensic investigating teams when crimes involve people of different nationals is challenging. This paper proposes a new method for ethnicity (nationality) identification based on Cloud of Line Distribution (COLD) features of handwriting components. The proposed method, at first, uses tangent angle of the contour pixels in each row and the mean of intensity values of each row for segmenting text lines. For segmented text lines, we use tangent angle and direction of base lines to remove rule lines in the image. We use polygonal approximation for finding dominant points for contours of edge components. Then the proposed method connects the nearest dominant points of every dominant point, which results in line segments of dominant point pairs. For each line segment, the proposed method estimates angle and length, which gives a point in polar domain. For all the line segments, the proposed method generates dense points in polar domain, which results in COLD distribution. As character component shapes change, according to nationals, the shape of the distribution changes. This observation is extracted based on distance from pixels of distribution to Principal Axis of the distribution. Then the features are subjected to an SVM classifier for identifying nationals. Experiments are conducted on a complex dataset, which show the proposed method is effective and outperforms the existing method.","tags":[],"title":"New COLD Feature Based Handwriting Analysis for Ethnicity/Nationality Identification","type":"publication"},{"authors":["Lianglei Wei","Yirui Wu","Wenhai Wang","Tong Lu"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"3067d7dd8a4c25351b6ff887fa2b5933","permalink":"https://hhudelta.github.io/publication/mmm2018/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/mmm2018/","section":"publication","summary":"Understanding the meanings of human actions from 3D skeleton data embedded videos is a new challenge in content-oriented video analysis. In this paper, we propose to incorporate temporal patterns of joint positions with currently popular Long Short-Term Memory (LSTM) based learning to improve both accuracy and robustness. Regarding 3D actions are formed by sub-actions, we first propose Wavelet Temporal Pattern (WTP) to extract representations of temporal patterns for each sub-action by wavelet transform. Then, we define a novel Relation-aware LSTM (R-LSTM) structure to extract features by modeling the long-term spatio-temporal correlation between body parts. Regarding WTP and R-LSTM features as heterogeneous representations for human actions, we next fuse WTP and R-LSTM features by an AutoEncoder network to define a more effective action descriptor for classification. The experimental results on a large scale challenging dataset NTU-RGB+D and several other datasets consisting of UT-Kinect and Florence 3D actions for 3D human action analysis demonstrate the effectiveness of the proposed method.","tags":[],"title":"A Novel 3D Human Action Recognition Framework for Video Content Analysis","type":"publication"},{"authors":["Wenhai Wang","Yirui Wu","Palaiahnakote Shivakumara","Tong Lu"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"34fa680d86dc016bb0432b3fc655999a","permalink":"https://hhudelta.github.io/publication/mmm2018-1/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/mmm2018-1/","section":"publication","summary":"Text detection in natural and video scene images is still considered to be challenging due to unpredictable nature of scene texts. This paper presents a new method based on Cloud of Line Distribution (COLD) and Random Forest Classifier for text detection in both natural and video images. The proposed method extracts unique shapes of text components by studying the relationship between dominant points such as straight or cursive over contours of text components, which is called COLD in polar domain. We consider edge components as text candidates if the edge components in Canny and Sobel of an input image share the COLD property. For each text candidate, we further study its COLD distribution at component level to extract statistical features and angle oriented features. Next, these features are fed to a random forest classifier to eliminate false text candidates, which results representatives. We then perform grouping using representatives to form text lines based on the distances between edge components in the edge image. The statistical and angle orientated features are finally extracted at word level for eliminating false positives, which results in text detection. The proposed method is tested on standard database, namely, SVT, ICDAR 2015 scene, ICDAR2013 scene and video databases, to show its effectiveness and usefulness compared with the existing methods.","tags":[],"title":"Cloud of Line Distribution and Random Forest Based Text Detection from Natural/Video Scene Images","type":"publication"},{"authors":["Yirui Wu","Wenhai Wang","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"0f50cd26e42d1a1fc8f2288b70c1ea2b","permalink":"https://hhudelta.github.io/publication/icdar2017/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/publication/icdar2017/","section":"publication","summary":"Text detection in video/scene images has gained a significant attention in the field of image processing and document analysis due to the inherent challenges caused by variations in contrast, orientation, background, text type, font type, non-uniform illumination and so on. In this paper, we propose a novel text detection method to explore symmetry property and appearance features of text for improved accuracy and robustness. First, the proposed method explores Extremal Regions (ER) for detecting text candidates in images. Then we propose a novel feature named as Multi-domain Strokes Symmetry Histogram (MSSH) for each text candidate, which describes the inherent symmetry property of stroke pixel pairs in gray, gradient and frequency domains. Furthermore, deep convolutional features are extracted to describe the appearance for each text candidate. We further fuse them by Auto-Encoder network to define a more discriminative text descriptor for classification. Finally, the proposed method constructs text lines based on the classification results. We demonstrate the effectiveness and robustness detection results of our proposed method by testing on four different benchmark databases.","tags":[],"title":"A Robust Symmetry-Based Method for Scene/Video Text Detection through Neural Network","type":"publication"},{"authors":["Yirui Wu","Zhouyu Meng","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1506816e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506816e3,"objectID":"4bd1fa6a6616c9ad1cabaad29ac473b2","permalink":"https://hhudelta.github.io/publication/acpr2017/","publishdate":"2017-10-01T00:00:00Z","relpermalink":"/publication/acpr2017/","section":"publication","summary":"Object detection is one of the fundamental challenges in pattern recognition community. Recently, convolutional neural networks (CNN) are increasingly exploited in object detection, showing their promising potentials of generatively discovering patterns from quantity of labeled images. Among CNN-based systems, we focus on one state-of-the-art architecture designed for fast object detection, named as YOLO. However, YOLO, as well as CNN-based systems are hard to deploy on embedded systems due to their computationally and storage intensive. In this paper, we propose to compress YOLO network by compressive sensing, which exploits in-herent redundancy property of parameters in layers of CNN architecture, leading to decrease the computation and storage cost. We firstly convert parameter matrix to frequency domain through discrete cosine transform (DCT). Due to the smooth property of parameters when processing images, the resulting frequency matrix are dominated by low-frequency components. Next, we prune high-frequency part to make the frequency matrix sparse. After pruning, we sample the frequency matrix with distributed random Gaussian matrix. Finally, we retrain the network to finetune the remaining parameters. We evaluate the proposed compress method on VOC 2012 dataset and show it outperforms one latest compression approach.","tags":[],"title":"Compressing YOLO Network by Compressive Sensing","type":"publication"},{"authors":["Hengduo Li","Jun Liu","Guyue Zhang","Yuan Gao","Yirui Wu"],"categories":null,"content":"","date":1504224e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224e3,"objectID":"523b39e36411e572fa44babb31179f51","permalink":"https://hhudelta.github.io/publication/icip2017/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/icip2017/","section":"publication","summary":"With the development of depth cameras such as Kinect and Intel Realsense, RGB-D based human detection receives continuous research attention due to its usage in a variety of applications. In this paper, we propose a new Multi-Glimpse LSTM (MG-LSTM) network, in which multi-scale contextual information is sequentially integrated to promote the human detection performance. Furthermore, we propose a feature fusion strategy based on our MG-LSTM network to better incorporate the RGB and depth information. To the best of our knowledge, this is the first attempt to utilize LSTM structure for RGB-D based human detection. Our method achieves superior performance on two publicly available datasets.","tags":[],"title":"Multi-glimpse LSTM with color-depth feature fusion for human detection","type":"publication"},{"authors":["Lixin Yuan","ShiJin Li","YaPing Jiang"],"categories":null,"content":"","date":1504224e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224e3,"objectID":"36c750cbcd5cfabbb29838c1c819b27f","permalink":"https://hhudelta.github.io/publication/yuan-remote-2017/","publishdate":"2024-12-18T16:43:54.920255Z","relpermalink":"/publication/yuan-remote-2017/","section":"publication","summary":"Due to the high intraclass variability and the low interclass disparity in high-resolution remote sensing (RS) image scenes, high-resolution RS scene classification is a challenging task. The performance of scene classification not only relies on discriminative feature representation but also needs appropriate classification strategies. In this paper, a scene preclassification strategy based on unsupervised learning is proposed, which divides scenes into two groups. The division method called dynamic-sphere division method (DSDM) is based on a dynamic-sphere division and an inside-sphere membership assessment. For the group with lower membership, after introducing the spatial location and scale of the scale invariant feature transformation (SIFT) descriptor to form a transaction, frequent itemset mining and an improved feature selection criterion are implemented to reduce the redundancy from the aspects of feature quantity and feature dimension, and a more discriminative structural feature histogram FMS-hist is finally obtained. Both the radius of the dynamic-sphere and the final optimal feature dimension are automatically selected according to the inflection point of the corresponding curves. Experimental results based on two representative data sets show that the proposed DSDM can select the suitable group, the proposed FMS-hist is superior to the bag-of-SIFT-based models. The holistic procedure can further enhance the scene classification accuracy.","tags":["Feature extraction","Dynamic-sphere division method with membership (DSDM)","Earth","feature mining (FM)","feature selection (FS)","Histograms","Image color analysis","Itemsets","preclassification","Remote sensing","scene classification","Visualization"],"title":"Remote Sensing Scene Classification Using a Preclassification Strategy and an Improved Structural Feature","type":"publication"},{"authors":["Yirui Wu","Tong Lu","Zehuan Yuan","Hao Wang."],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"0f68fafc00711a4e47195e056b6a1598","permalink":"https://hhudelta.github.io/publication/tmm2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/tmm2017/","section":"publication","summary":"Sculpture design is challenging due to its inherent difficulty in characterizing artworks quantitatively; thus, few works have been done to assist sculpture design in the past decades in the multimedia community. We have cooperated with several sculptors on analyzing styles of different artists consisting of Giacometti, Augeuste Rodin, Henry Moore, and Marino Marini from which we find pose editing plays an important role in sculpture design. Motivated by this, we present a novel platform that allows sculptors to edit virtual three-dimensional (3-D) sculptures by a free way. The proposed platform consists of three modules, namely, sculpture initialization, sculptor-sculpture mapping, and interactive pose editing. In sculpture initialization, a virtual 3-D sculpture is first incrementally reconstructed from multiview images. Then, we define Laplace operator and its corresponding spectrum to describe the geometry information of the reconstructed sculpture. During sculptor–sculpture mapping, we apply spectral analysis on the low-frequency parts of the spectrum to search for candidate editing points on the surface of the sculpture. Next, body actions of the sculptor are captured by Kinect and further mapped onto editing points as a predefined configuration set. Finally, during interactive pose editing, a real-time Kinect-driven sculpture pose editing scheme is presented, which not only preserves geometry features of the sculpture but also allows instant changes of sculpture poses. We demonstrate that our platform successfully assists sculptors on real-time pose editing by comparing its performance with those of the existing sculpture assisting methods.","tags":[],"title":"FreeScup: A Novel Platform for Assisting Sculpture Pose Design","type":"publication"},{"authors":["Yiyang Zhou","Wenhai Wang","Wenjie Guan","Yirui Wu","Heng Lai","Tong Lu","Min Cai"],"categories":null,"content":"","date":1483142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483142400,"objectID":"064e8ea026f5512723157a4d18c6edb0","permalink":"https://hhudelta.github.io/publication/mmm2017/","publishdate":"2016-12-31T00:00:00Z","relpermalink":"/publication/mmm2017/","section":"publication","summary":"In this paper, we present a novel framework to drive automatic robotic grasp by matching camera captured RGB-D data with 3D meshes, on which prior knowledge for grasp is pre-defined for each object type. The proposed framework consists of two modules, namely, pre-defining grasping knowledge for each type of object shape on 3D meshes, and automatic robotic grasping by matching RGB-D data with pre-defined 3D meshes. In the first module, we scan 3D meshes for typical object shapes and pre-define grasping regions for each 3D shape surface, which will be considered as the prior knowledge for guiding automatic robotic grasp. In the second module, for each RGB-D image captured by a depth camera, we recognize 2D shape of the object in it by an SVM classifier, and then segment it from background using depth data. Next, we propose a new algorithm to match the segmented RGB-D shape with predefined 3D meshes to guide robotic self-location and grasp by an automatic way. Our experimental results show that the proposed framework is particularly useful to guide camera based robotic grasp.","tags":[],"title":"Visual Robotic Object Grasping Through Combining RGB-D Data and 3D Meshes","type":"publication"},{"authors":["Yirui Wu","Palaiahnakote Shivakumara","Tong Lu","Chew Lim Tan","Michael Blumenstein","G. Hemantha Kumar"],"categories":null,"content":" ","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"d5576294330dc86bf5fe3a494b39e66a","permalink":"https://hhudelta.github.io/publication/tip2016/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/tip2016/","section":"publication","summary":"Text recognition in video/natural scene images has gained significant attention in the field of image processing in many computer vision applications, which is much more challenging than recognition in plain background images. In this paper, we aim to restore complete character contours in video/scene images from gray values, in contrast to the conventional techniques that consider edge images/binary information as inputs for text detection and recognition. We explore and utilize the strengths of zero crossing points given by the Laplacian to identify stroke candidate pixels (SPC). For each SPC pair, we propose new symmetry features based on gradient magnitude and Fourier phase angles to identify probable stroke candidate pairs (PSCP). The same symmetry properties are proposed at the PSCP level to choose seed stroke candidate pairs (SSCP). Finally, an iterative algorithm is proposed for SSCP to restore complete character contours. Experimental results on benchmark databases, namely, the ICDAR family of video and natural scenes, Street View Data, and MSRA data sets, show that the proposed technique outperforms the existing techniques in terms of both quality measures and recognition rate. We also show that character contour restoration is effective for text detection in video and natural scene images.","tags":[],"title":"Contour Restoration of Text Components for Recognition in Video/Scene Images","type":"publication"},{"authors":["Wenhai Wang","Yirui Wu","Shivakumara Palaiahnakote","Tong Lu","Jun Liu"],"categories":null,"content":"","date":147528e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":147528e4,"objectID":"0064a6eb713e33b1d2f08070be96e530","permalink":"https://hhudelta.github.io/publication/pcm2017/","publishdate":"2016-10-01T00:00:00Z","relpermalink":"/publication/pcm2017/","section":"publication","summary":"Detecting arbitrary oriented text in scene and license plate images is challenging due to multiple adverse factors caused by images of diversified applications. This paper proposes a novel idea of extracting Cloud of Line Distribution (COLD) for the text candidates given by Extremal regions (ER). The features extracted by COLD are fed to Random forest to label character components. The character components are grouped according to probability distribution of nearest neighbor components. This results in text line. The proposed method is demonstrated on standard database of natural scene images, namely ICDAR 2015, video images, namely ICDAR 2015 and license plate databases. Experimental results and comparative study show that the proposed method outperforms the existing methods in terms of invariant to rotations, scripts and applications.","tags":[],"title":"Cloud of Line Distribution for Arbitrary Text Detection in Scene/Video/License Plate Images","type":"publication"},{"authors":["Zehuan Yuan","Tong Lu","Yirui Wu"],"categories":null,"content":"","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"beeb18bd7dc2b2d492e3ff3f09b1cd09","permalink":"https://hhudelta.github.io/publication/ijcai2017/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/publication/ijcai2017/","section":"publication","summary":"We address the problem of object co-segmentation in images. Object co-segmentation aims to segment common objects in images and has promising applications in AI agents. We solve it by proposing a co-occurrence map, which measures how likely an image region belongs to an object and also appears in other images. The co-occurrence map of an image is calculated by combining two parts: objectness scores of image regions and similarity evidences from object proposals across images. We introduce a deep-dense conditional random field framework to infer co-occurrence maps. Both similarity metric and objectness measure are learned end-to-end in a single deep network. We evaluate our method on two benchmarks and achieve competitive performance.","tags":[],"title":"Deep-dense Conditional Random Fields for Object Co-segmentation","type":"publication"},{"authors":["Shijin Li","Hui Yu","Lixin Yuan"],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"efa3a947b82eae01c2b0c1e0d7344f4f","permalink":"https://hhudelta.github.io/publication/li-novel-2016/","publishdate":"2024-12-18T16:43:55.0093Z","relpermalink":"/publication/li-novel-2016/","section":"publication","summary":"With the development of remote sensing (RS) techniques, the amount of RS images increases dramatically. It is a challenge to utilize those RS big data efficiently. Content-based Image Retrieval (CBIR) is a typical approximate similarity search problem, which needs to establish an effective index structure to reduce the time of retrieval. By analyzing the limitations of commonly-used indexing mechanisms in the current CBIR system, we propose a novel scheme that dynamically combines vantage point tree (vp-tree) indexes to CBIR by using spacing-correlation strategies to determine the vantage points. Borrowing ideas from feature selection, we have also put forward a new measure to adaptively online select proper vp-tree indexing in different feature spaces, the distance-contrast-based indexing validity index (DCIVI). And we then employ vp-tree index structure in each feature space, which can properly describe the content of the RS image by the chosen features. Experimental results on various typical land covers retrieval validate that the proposed method is effective and not only is the response speed increased by 70 100 times, but also the retrieval quality (in terms of precision and recall) is improved.","tags":["Feature extraction","Image color analysis","Remote sensing","content-based image retrieval","feature selection","high-dimensional indexing structure","Image retrieval","Indexing","remote sensing","Time factors","vp-tree"],"title":"A Novel Approach to Remote Sensing Image Retrieval with Multi-feature VP-Tree Indexing and Online Feature Selection","type":"publication"},{"authors":["Yirui Wu","Xianli Zhou","Tong Lu","Guo Mei","Sun Linbi"],"categories":null,"content":"","date":1448928e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928e3,"objectID":"a8b793e7a05333d523526ef5affb99c7","permalink":"https://hhudelta.github.io/publication/icpr2016/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/icpr2016/","section":"publication","summary":"Imitation cartoon drawing is an important skill for cartoonists, requiring quantity of efforts on practising and guidance. In this paper, we propose EvaToon, an imitated drawing evaluate system, which automatically assigns judging scores and marks improper drawing regions. With our system, cartoonists can practise and get guidance by themselves. We have cooperated with several experts on developing such an evaluation system. Based on their guide, we present EvaToon in two stages comprising cartoon drawings analyzing and similarity evaluating. During analyzing, we first locate contour pixels with high curvature as interest points and then extract multi-scale features around interest points to hierarchically describe shape. During evaluating, we first match interest points between original and imitated drawing based on distance of features. After matching, we construct a regression tree to map high dimensional difference of matching features to scores and marks based on quantity of manually evaluated training examples. Finally, our system matches an input imitated drawing with the original one and predicts its scores automatically. We demonstrate the accuracy of our EvaToon system in matching and predicting and prove the capability of describing shape of our proposed features by experiments on a collected dataset of imitated drawings.","tags":[],"title":"EvaToon: A novel graph matching system for evaluating cartoon drawings","type":"publication"},{"authors":["Yirui Wu","Oscar Kin-Chung Au","Chiew-Lan Tai","Tong Lu"],"categories":null,"content":"","date":1427155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1427155200,"objectID":"882c4902f6846b507abb1e136de5d829","permalink":"https://hhudelta.github.io/publication/cagd2015/","publishdate":"2015-03-24T00:00:00Z","relpermalink":"/publication/cagd2015/","section":"publication","summary":"Most existing handle-based mesh deformation methods require costly re-computation for every handle set updating, namely, adding or removing of handles on the mesh surface. In this paper, we propose a reduced deformation model that is independent of handle configuration, allowing users to dynamically update the handle set without noticeable waiting time. We represent the deformation space of a mesh as propagation fields defined by only the mesh geometry, independent of the handle set. We define the propagation fields as selected eigenvectors of the Laplacian operator and adopt the transformations of isolines sampled from the fields as the deformation descriptors. In this way, the deformation descriptors are pre-computed before handle specification. During interactive manipulation, constraints generated from the handles are incorporated into the deformation system in real time. Our method therefore supports incremental mesh editing where the user can freely define different handle sets to edit different parts of the shape without waiting for long re-computation. Our reduced model is scalable since the updating time per iteration is independent of the mesh size and the number of handles. We demonstrate the effectiveness of the proposed deformation method and compare its performance with related reduced deformation models.","tags":[],"title":"HIRM: A Handle-Independent Reduced Model for Incremental Mesh Editing","type":"publication"},{"authors":["Yirui Wu","Tong Lu","Zehuan Yuan","Hao Wang."],"categories":null,"content":"","date":1412121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412121600,"objectID":"36673f9c10e09df223888458f5365f12","permalink":"https://hhudelta.github.io/publication/icme2015/","publishdate":"2014-10-01T00:00:00Z","relpermalink":"/publication/icme2015/","section":"publication","summary":"Sculpture design is challenging due to its inherent difficulty in characterizing an artwork quantitatively, and few works have been done to assist sculpture design. We present a novel platform to help sculptors in two stages, comprising automatic sculpture reconstruction and free spectral-based sculpture pose editing. During sculpture reconstruction, we co-segment a sculpture from real scene images of different views through a two-label MRF framework, aiming at performing sculpture reconstruction efficiently. During sculpture pose editing, we automatically extract candidate editing points on the sculpture by searching in the spectrums of Laplacian operator. After manually mapping body joints of a sculptor to particular editing points, we further construct a global Laplacian-based linear system by adopting the spectrums of Laplacian operator and using Kinect captured body motions for real time pose editing. The constructed system thus allows the sculptor to freely edit different kinds of sculpture artworks through Kinect. Experimental results demonstrate that our platform successfully assists sculptors in real-time pose editing.","tags":[],"title":"FreeScup: A novel platform for assisting sculpture pose design","type":"publication"},{"authors":["LiMin Wang","Yirui Wu","Tong Lu","Kang Chen"],"categories":null,"content":"","date":131976e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":131976e4,"objectID":"2d857c3af8385fe472d6e195cbc6f9f0","permalink":"https://hhudelta.github.io/publication/acmmm2011/","publishdate":"2011-10-28T00:00:00Z","relpermalink":"/publication/acmmm2011/","section":"publication","summary":"In this paper, we present a novel approach for multiclass object detection by combining local appearances and contextual constraints. We first construct a multiclass Hough forest of local patches, which can well deal with multiclass object deformations and local appearance variations, due to randomization and discrimination of the forest. Then, in the object hypothesis space, a new multiclass context model is proposed to capture relative location constraints, disambiguating appearance inputs in multiclass object detection. Finally, multiclass objects are detected with a greedy search algorithm efficiently. Experimental evaluations on two image data sets show that the combination of local appearances and context achieves state-of-the-art performance in multiclass object detection.","tags":[],"title":"Multiclass object detection by combining local appearances and context","type":"publication"}]
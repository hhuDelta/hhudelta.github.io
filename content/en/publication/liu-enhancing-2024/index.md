---
title: Enhancing Large Language Models with Multimodality and Knowledge Graphs for
  Hallucination-free Open-set Object Recognition
authors:
- Xinfu Liu
- Yirui Wu
- Yuting Zhou
- Junyang Chen
- Huan Wang
- Ye Liu
- Shaohua Wan
date: '2024-01-01'
publishDate: '2024-12-24T07:13:10.091115Z'
publication_types:
- paper-conference
publication: '*VLDB Workshops*'
abstract: Open-set object recognition plays a significant role in todayâ€™s production
  and daily life, such as in surface defect detection, biometric identification, and
  autonomous driving recognition. However, due to the diversity of unknown categories
  and the complexity of scenarios, existing methods often perform poorly. Therefore,
  open-set object recognition remains an important and popular research topic. Recently,
  collaborative utilization of multiple pre-trained Large Language Models (LLMs) has
  emerged rapidly, which becomes a new research hotspot in addressing open-set object
  recognition tasks. Among this, a core challenge lies in amplifying the strengths
  of individual LLMs while mitigating their weaknesses. In this paper, we propose
  a novel joint framework tailored for open-set object recognition tasks, aiming to
  more efficiently harness the capabilities of diverse LLMs and Knowledge Graphs (KGs).
  Initially, for the text data generated by textual LLMs, we use Wikipedia to correct
  and complete it. Then, we designed a text-image multi-modal fusion method to further
  correct and complete the text information by utilizing the implicit semantic information
  in the image. Additionally, we propose some novel designs to alleviate the hallucination
  issue of LLMs and reduce their instability. Extensive experiments demonstrate that
  our approach outperforms all the comparison methods.
url_pdf: https://vldb.org/workshops/2024/proceedings/LLM+KG/LLM+KG-11.pdf
---

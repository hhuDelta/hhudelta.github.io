[{"authors":["Hongyi Lu"],"categories":null,"content":"我是河海大学2023级电子信息工程本科毕业生，硕士报考方向是电子信息计算机方向，对计算机视觉和图像分割方向很感兴趣，因此加入了巫义锐老师的课题组。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"8d69f871b347160e9a6ba8fdf442c995","permalink":"https://hhudelta.github.io/zh/author/%E5%8D%A2%E6%B3%93%E5%B1%B9/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E5%8D%A2%E6%B3%93%E5%B1%B9/","section":"authors","summary":"我是河海大学2023级电子信息工程本科毕业生，硕士报考方向是电子信息计算机方向，对计算机视觉和图像分割方向很感兴趣，因此加入了巫义锐老师的课题组。\n","tags":null,"title":"卢泓屹","type":"authors"},{"authors":["Guang Yang"],"categories":null,"content":"我于2024年获得江苏科技大学的工学学士学位。目前，我在河海大学计算机视觉实验室（HHU-CV Lab）攻读工学硕士学位，导师是巫义锐教授。我的研究兴趣包括大语言模型（LLM）和小样本学习（Few-Shot Learning）。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"193cb2e91ba01e00601634748311c026","permalink":"https://hhudelta.github.io/zh/author/%E6%9D%A8%E5%85%89/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E6%9D%A8%E5%85%89/","section":"authors","summary":"我于2024年获得江苏科技大学的工学学士学位。目前，我在河海大学计算机视觉实验室（HHU-CV Lab）攻读工学硕士学位，导师是巫义锐教授。我的研究兴趣包括大语言模型（LLM）和小样本学习（Few-Shot Learning）。\n","tags":null,"title":"杨光","type":"authors"},{"authors":["Baoxi Jia"],"categories":null,"content":"我于2024年获得曲阜师范大学人工智能学士学位。我目前在巫义锐教授的指导下，作为Delta Lab的一员，正在攻读硕士学位。我目前的研究兴趣包括小样本学习和解耦表征学习。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"0480a8f6c1877e7d4a8da99090e57f5a","permalink":"https://hhudelta.github.io/zh/author/%E8%B4%BE%E5%AE%9D%E7%8E%BA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E8%B4%BE%E5%AE%9D%E7%8E%BA/","section":"authors","summary":"我于2024年获得曲阜师范大学人工智能学士学位。我目前在巫义锐教授的指导下，作为Delta Lab的一员，正在攻读硕士学位。我目前的研究兴趣包括小样本学习和解耦表征学习。\n","tags":null,"title":"贾宝玺","type":"authors"},{"authors":["Hao Wang"],"categories":null,"content":"我于 2024 年获得河海大学计算机科学与技术学士学位，目前是 Delta Lab的成员，在巫义锐教授的指导下攻读硕士学位。我目前的研究兴趣是语义分割。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"81312ddab37221f82f5b8600183cf2e7","permalink":"https://hhudelta.github.io/zh/author/%E6%B1%AA%E8%B1%AA/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E6%B1%AA%E8%B1%AA/","section":"authors","summary":"我于 2024 年获得河海大学计算机科学与技术学士学位，目前是 Delta Lab的成员，在巫义锐教授的指导下攻读硕士学位。我目前的研究兴趣是语义分割。\n","tags":null,"title":"汪豪","type":"authors"},{"authors":["Ziye Wang"],"categories":null,"content":"我于 2024 年获得河海大学计算机科学与技术学士学位，目前是 Delta Lab的成员，在巫义锐教授的指导下攻读硕士学位。我目前的研究兴趣是迁移学习。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"7e1e1c851319c8ddab53c174133dccfe","permalink":"https://hhudelta.github.io/zh/author/%E7%8E%8B%E7%B4%AB%E6%99%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E7%8E%8B%E7%B4%AB%E6%99%94/","section":"authors","summary":"我于 2024 年获得河海大学计算机科学与技术学士学位，目前是 Delta Lab的成员，在巫义锐教授的指导下攻读硕士学位。我目前的研究兴趣是迁移学习。\n","tags":null,"title":"王紫晔","type":"authors"},{"authors":["Fuchen Ma"],"categories":null,"content":"我于 2024 年获得河海大学计算机科学与技术学士学位，目前是 Delta Lab的成员，在巫义锐教授的指导下攻读硕士学位。我目前的研究兴趣是遗忘学习。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"dc8b8561264cbfca141c0966aff8b0ce","permalink":"https://hhudelta.github.io/zh/author/%E9%A9%AC%E7%A6%8F%E8%BE%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E9%A9%AC%E7%A6%8F%E8%BE%B0/","section":"authors","summary":"我于 2024 年获得河海大学计算机科学与技术学士学位，目前是 Delta Lab的成员，在巫义锐教授的指导下攻读硕士学位。我目前的研究兴趣是遗忘学习。\n","tags":null,"title":"马福辰","type":"authors"},{"authors":["Yuhang Xia"],"categories":null,"content":"I received Bachelor of Management degree in Engineering Management from Nanjing Audit University in 2019. I am currently studying for my Master’s degree and i am a member of the Delta Lab under Professor Yirui Wu.\n","date":1735689600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1735689600,"objectID":"946465037dee3da1b9db6783121c63e7","permalink":"https://hhudelta.github.io/zh/author/%E5%A4%8F%E5%AE%87%E8%88%AA/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/zh/author/%E5%A4%8F%E5%AE%87%E8%88%AA/","section":"authors","summary":"I received Bachelor of Management degree in Engineering Management from Nanjing Audit University in 2019. I am currently studying for my Master’s degree and i am a member of the Delta Lab under Professor Yirui Wu.\n","tags":null,"title":"夏宇航","type":"authors"},{"authors":["Ao Geng"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Anhui University of Science and Technology in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few-shot Learning and object detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"eb9e895946f1c2290c8907b9675566d5","permalink":"https://hhudelta.github.io/zh/author/%E8%80%BF%E5%A5%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E8%80%BF%E5%A5%A5/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Anhui University of Science and Technology in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few-shot Learning and object detection.\n","tags":null,"title":"耿奥","type":"authors"},{"authors":["Cheng Zhen"],"categories":null,"content":"I received my B.E. degree in Software Engineering from Jiangsu University of Science and Technology in 2023. I am currently studying for my Master’s degree and am a member of the Delta Lab under Professor Yirui Wu. I often post my projects on Github. If you are interested, you can communicate with me through email or post an issue.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"d361335ccb1bfb38be467b6f4a90bd30","permalink":"https://hhudelta.github.io/zh/author/%E7%94%84%E8%AF%9A/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E7%94%84%E8%AF%9A/","section":"authors","summary":"I received my B.E. degree in Software Engineering from Jiangsu University of Science and Technology in 2023. I am currently studying for my Master’s degree and am a member of the Delta Lab under Professor Yirui Wu. I often post my projects on Github. If you are interested, you can communicate with me through email or post an issue.\n","tags":null,"title":"甄诚","type":"authors"},{"authors":["Jianzhou Wang"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Nanjing Forestry University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Transformer and Incremental Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"6a713745217eb980fa430474cb55490e","permalink":"https://hhudelta.github.io/zh/author/%E6%B1%AA%E5%BB%BA%E6%B4%B2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E6%B1%AA%E5%BB%BA%E6%B4%B2/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Nanjing Forestry University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Transformer and Incremental Learning.\n","tags":null,"title":"汪建洲","type":"authors"},{"authors":["Shijia Qiao"],"categories":null,"content":"I received my B.E. degree in Computer Science and Technology from North China University of Water Resources and Electric Power in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests include Forgetting learning and causal inference.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"33f2eba52c39e990cb01cff17ade1e25","permalink":"https://hhudelta.github.io/zh/author/%E4%B9%94%E4%B8%96%E5%98%89/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E4%B9%94%E4%B8%96%E5%98%89/","section":"authors","summary":"I received my B.E. degree in Computer Science and Technology from North China University of Water Resources and Electric Power in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidance of Professor Yirui Wu. My current research interests include Forgetting learning and causal inference.\n","tags":null,"title":"乔世嘉","type":"authors"},{"authors":["Xiu Jin"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Nanjing Tech University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"e3c7b59182007f56c34ea2f9d3d4fcaa","permalink":"https://hhudelta.github.io/zh/author/%E9%9D%B3%E4%BF%AE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E9%9D%B3%E4%BF%AE/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Nanjing Tech University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu.\n","tags":null,"title":"靳修","type":"authors"},{"authors":["Haiyan Sun"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Shandong Normal University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few shot Learning and Graph Neural Network. I think machines can take a lot of clues from 2D images and make inferences based on them, just like Sherlock Holmes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"837809b6d322677349d14c66b1d93271","permalink":"https://hhudelta.github.io/zh/author/%E5%AD%99%E6%B5%B7%E7%87%95/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E5%AD%99%E6%B5%B7%E7%87%95/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Shandong Normal University in 2023. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include Few shot Learning and Graph Neural Network. I think machines can take a lot of clues from 2D images and make inferences based on them, just like Sherlock Holmes.\n","tags":null,"title":"孙海燕","type":"authors"},{"authors":["Rui Qin"],"categories":null,"content":"I received my B.E. degree in computer science and technology from Nanjing Tech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include 3D Object Detection and Transformer. You can also follow me on CSDN website with ID 偷走心灵的告白 or Github website with ID QinRui666.\n","date":1723593600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1723593600,"objectID":"6f60bcb8f7961dc317a6a6e4821e33d2","permalink":"https://hhudelta.github.io/zh/author/%E7%A7%A6%E9%94%90/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/zh/author/%E7%A7%A6%E9%94%90/","section":"authors","summary":"I received my B.E. degree in computer science and technology from Nanjing Tech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include 3D Object Detection and Transformer. You can also follow me on CSDN website with ID 偷走心灵的告白 or Github website with ID QinRui666.\n","tags":null,"title":"秦锐","type":"authors"},{"authors":["Yuting Zhou"],"categories":null,"content":"I received my B.E. degree in computer science and technology from NingboTech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. You can also follow me on CSDN website with ID zyt131415 or Github website with ID zyt0211.\n","date":1704067200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1704067200,"objectID":"2cbc95840e7b3ff5af192d5451c241d2","permalink":"https://hhudelta.github.io/zh/author/%E5%91%A8%E7%8E%89%E5%A9%B7/","publishdate":"2024-12-24T07:13:10.091115Z","relpermalink":"/zh/author/%E5%91%A8%E7%8E%89%E5%A9%B7/","section":"authors","summary":"I received my B.E. degree in computer science and technology from NingboTech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu. You can also follow me on CSDN website with ID zyt131415 or Github website with ID zyt0211.\n","tags":null,"title":"周玉婷","type":"authors"},{"authors":["Tianyu Ni"],"categories":null,"content":"I received my B.E. degree in computer science and technology from NingboTech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"230dbc5cae3760ea0ca2d4504dce0b74","permalink":"https://hhudelta.github.io/zh/author/%E5%80%AA%E5%A4%A9%E5%AE%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E5%80%AA%E5%A4%A9%E5%AE%87/","section":"authors","summary":"I received my B.E. degree in computer science and technology from NingboTech University in 2021. I am currently working toward M.E. degree as a member of Delta Lab, under the guidence of Professor Yirui Wu.\n","tags":null,"title":"倪天宇","type":"authors"},{"authors":["Xinfu Liu"],"categories":null,"content":"I received my M.E. degree in computer application technology from Yunnan University in 2022. I am currently working toward Ph.D as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include image segmentation, few-shot learning and incremental learning.\n","date":1723593600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1723593600,"objectID":"42e8adca1a95e9e148efaa23e536b35d","permalink":"https://hhudelta.github.io/zh/author/%E5%88%98%E6%96%B0%E5%AF%8C/","publishdate":"2024-12-24T07:13:10.091115Z","relpermalink":"/zh/author/%E5%88%98%E6%96%B0%E5%AF%8C/","section":"authors","summary":"I received my M.E. degree in computer application technology from Yunnan University in 2022. I am currently working toward Ph.D as a member of Delta Lab, under the guidence of Professor Yirui Wu. My current research interests include image segmentation, few-shot learning and incremental learning.\n","tags":null,"title":"刘新富","type":"authors"},{"authors":["Yirui Wu"],"categories":null,"content":"巫义锐，男，工学博士，河海大学计算机与信息学院青年教授，我是水利大数据研究所成员，河海大学Delta Lab负责人，获河海大学“大禹学者”称号。研究方向为计算机视觉、多媒体计算与智慧水利。 我于2011年在南京大学计算机科学与技术系获得理学学士学位，同年被保送至南大计算机系攻读博士学位，并于2016年获得工学博士学位，我的指导老师是路通教授， 并和Shivakumara Palaiahnakote教授 保持了紧密合作关系。我曾于2012年和2014年两次访问香港科学与技术大学，并接受了戴秋兰教授和 欧建忠博士的指导。主持了国家自然科学基金、江苏省自然科学基金、国家重点研发计划子专题、国家重点实验室开放课题等多个研究项目。我于权威国际刊物《IEEE Transactions on Image Processing》、《IEEE Transactions on Multimedia》、 《IEEE Transactions on Network Science and Engineering》及高水平国际学术会议ACM Multimedia、 ICME、ICPR、BIBM、ICIP等发表学术论文四十余篇。\n我现担任《IET Image Processing》(CCF-C, SCI)的副主编， 《Malaysian Journal of Computer Science》(SCI)的首位外籍副主编， 《Artificial Intelligence and Applications》的创刊副主编， CCF-YOCSEF南京副主席，CCF-MM执行委员，江苏省信息技术应用学会多媒体专委会副秘书长。我担任过 Journal of Sensor and Actuator Network (JCR Q1)和IEEE COMSOC MMTC Communications-Frontiers的客座主编， ICML2022(CCF-A, 人工智能顶会), ACM MM2022/2021(CCF-A, 多媒体顶会), ICPR2022(CCF-C), ICME2022(CCF-B), HPCC2021(CCF-C), CIKM2021(CCF-B), ACPR2021 等会议程序主席。\n您可以在知乎上关注我的账号 河海大学巫义锐 或者在学者网上关注我的账号 巫义锐.\n","date":1735689600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1735689600,"objectID":"3ad653018393b0089837827547e6926f","permalink":"https://hhudelta.github.io/zh/author/%E5%B7%AB%E4%B9%89%E9%94%90/","publishdate":"2024-12-24T07:13:10.099096Z","relpermalink":"/zh/author/%E5%B7%AB%E4%B9%89%E9%94%90/","section":"authors","summary":"巫义锐，男，工学博士，河海大学计算机与信息学院青年教授，我是水利大数据研究所成员，河海大学Delta Lab负责人，获河海大学“大禹学者”称号。研究方向为计算机视觉、多媒体计算与智慧水利。 我于2011年在南京大学计算机科学与技术系获得理学学士学位，同年被保送至南大计算机系攻读博士学位，并于2016年获得工学博士学位，我的指导老师是路通教授， 并和Shivakumara Palaiahnakote教授 保持了紧密合作关系。我曾于2012年和2014年两次访问香港科学与技术大学，并接受了戴秋兰教授和 欧建忠博士的指导。主持了国家自然科学基金、江苏省自然科学基金、国家重点研发计划子专题、国家重点实验室开放课题等多个研究项目。我于权威国际刊物《IEEE Transactions on Image Processing》、《IEEE Transactions on Multimedia》、 《IEEE Transactions on Network Science and Engineering》及高水平国际学术会议ACM Multimedia、 ICME、ICPR、BIBM、ICIP等发表学术论文四十余篇。\n我现担任《IET Image Processing》(CCF-C, SCI)的副主编， 《Malaysian Journal of Computer Science》(SCI)的首位外籍副主编， 《Artificial Intelligence and Applications》的创刊副主编， CCF-YOCSEF南京副主席，CCF-MM执行委员，江苏省信息技术应用学会多媒体专委会副秘书长。我担任过 Journal of Sensor and Actuator Network (JCR Q1)和IEEE COMSOC MMTC Communications-Frontiers的客座主编， ICML2022(CCF-A, 人工智能顶会), ACM MM2022/2021(CCF-A, 多媒体顶会), ICPR2022(CCF-C), ICME2022(CCF-B), HPCC2021(CCF-C), CIKM2021(CCF-B), ACPR2021 等会议程序主席。\n","tags":null,"title":"巫义锐","type":"authors"},{"authors":null,"categories":null,"content":"袁俐新，女，工学博士，河海大学计算机与软件学院讲师。我于2014年在西安电子科技大学电子科学与技术学院获得工学学士学位，于2018年获得河海大学计算机与信息工程学院硕士学位，并于2024年获得南京大学计算机科学与技术系博士学位。我曾在《IEEE Transactions on Image Processing》、《Pattern Recognition》和《IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing》等权威国际刊物上发表论文。我目前的研究兴趣包括模式识别、机器学习和图像处理。\n","date":1735689600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1735689600,"objectID":"917db75948c8a40ff81d64c9def7c967","permalink":"https://hhudelta.github.io/zh/author/%E8%A2%81%E4%BF%90%E6%96%B0/","publishdate":"2024-12-18T16:43:55.0093Z","relpermalink":"/zh/author/%E8%A2%81%E4%BF%90%E6%96%B0/","section":"authors","summary":"袁俐新，女，工学博士，河海大学计算机与软件学院讲师。我于2014年在西安电子科技大学电子科学与技术学院获得工学学士学位，于2018年获得河海大学计算机与信息工程学院硕士学位，并于2024年获得南京大学计算机科学与技术系博士学位。我曾在《IEEE Transactions on Image Processing》、《Pattern Recognition》和《IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing》等权威国际刊物上发表论文。我目前的研究兴趣包括模式识别、机器学习和图像处理。\n","tags":null,"title":"袁俐新","type":"authors"},{"authors":null,"categories":null,"content":"春：数字图像处理 智能本科生\n软件学报模板下载\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1536451200,"objectID":"bf74911ecc461138676a4bba67bf08b3","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/","section":"teaching","summary":"智能本科生","tags":null,"title":"春：数字图像处理","type":"book"},{"authors":null,"categories":null,"content":" 时间安排 11月13日：小作业、大作业确定题目；大作业演讲次序确定 11月18日：开始讲演（每位讲演完的同学，请将修改后的PPT发送至我的邮箱） 12月12日 凌晨 0 点：小作业截止时间 小作业 作业简介 从以下题目中自选一题，根据相关资料与自己的理解，形成篇幅在 10 页左右的综述论文。\n题目选择 小样本学习的研究及其应用\n论文：A CLOSER LOOK AT FEW-SHOT CLASSIFICATION\nGAN 模型的研究及其应用\n论文：Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\nLSTM 模型的研究及其应用\n论文：Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\n作业要求 论文的格式需遵循软件学报模板 论文内容详实，章节中至少应包括： 模型简介 数学推导 相关工作及应用举例 自己对该类模型的见解 论文不得有明显抄袭痕迹。 可额外添加 算法实现及性能分析 部分，该部分将加分。 软件学报模板下载\n大作业 作业简介 从最近两年的 CVPR, ICCV, ECCV, ICLR 论文中自选一题，根据相关资料与自己的理解，形成一次 8 分钟左右的主题报告。\n作业要求 报告形式需遵循 会议口头宣讲流程。 注意掌握时间，超时将被打断。 宣讲内容详实，有自己对该课题的见解，无明显抄袭痕迹。 宣讲完成后，会有 提问环节，提问回答质量计入分数。 联系方式 如对作业提交情况有疑问，请通过邮件 wuyirui@hhu.edu.cn 联系我。\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1536451200,"objectID":"80b31b14204c379f329a071e934885e3","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/zh/teaching/classcspost/","section":"teaching","summary":"计算机硕士","tags":null,"title":"秋：数字图像处理","type":"book"},{"authors":null,"categories":null,"content":"秋：数字图像处理 计算机本科生\n软件学报模板下载\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1536451200,"objectID":"356fd11400a89a285a031c17021ffe02","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/","section":"teaching","summary":"计算机本科生","tags":null,"title":"秋：数字图像处理","type":"book"},{"authors":null,"categories":null,"content":"秋：水文大数据分析 水文博士生\n","date":1599609600,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1599609600,"objectID":"b9cd983fc0cbfa90b191581b011c685f","permalink":"https://hhudelta.github.io/zh/teaching/classhydrologyphd/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/zh/teaching/classhydrologyphd/","section":"teaching","summary":"水文博士生","tags":null,"title":"秋：水文大数据分析","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"e72573f1962a907c3c65913112c3e3d1","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt1/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt1/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"ab3d30bae65cc0f60b0631d4c60b390d","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt1/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt1/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"214b65b1755df0854c7bc11609aac2f8","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt1/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt1/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"dd8fc1617d6d36e42cba13546286e26f","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt2/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt2/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"8a4d03b8816ff28e4dd7d89ac340084e","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt2/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt2/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"0fd5a0c1fc213f1bb954aa2b7e41e9a8","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt2/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt2/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"c1edd4ea9ff698f945c4cdf070d36a02","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt3/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt3/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"a21f46b2d16497ddc7e8f4732dcb91fa","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt3/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt3/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"c71a8ba03d9bcaae97af43c241b210ad","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt3/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt3/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"f4fd23dd6e81d61ffba3336e1c90be51","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt4/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt4/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"7e5ca946163cc40509faa4ea55380e70","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt4/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt4/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"94f644f2186128ef604a6c2b7871b742","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt4/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt4/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"9c2cb8efc5e4d0d0b2fbca0780caaefd","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt5/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt5/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第五章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"8d27fe5fe78e095c52b63f120a773c17","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt5/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt5/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第五章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"a20e48d5733d5730591b3fd00ca07f75","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt5/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt5/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第五章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"6d8f1f517ddabca542e5dbad226e7311","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt6/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt6/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第六章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"741063cf75af6a94f6685124fe1a74d2","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt6/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt6/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第六章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"96ee827d427ecb9fad6f88582b9242f2","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt6/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt6/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第六章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"c18800295c89036623de5043f64c8e7d","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt7/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt7/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第七章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"4f965a8be65a85da892fcc36638559db","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt7/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt7/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第七章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"eb5eaca7abf3d2ea844ab8bbde0696e9","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt7/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt7/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第七章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"b8fc18cd6545b2c73dce229b985cd0c5","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt8/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt8/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第八章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"f099a64aeb4f531e6a04d1c75d515c5a","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt8/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt8/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第八章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"72233040da8a4d3af07663ef721712e5","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt8/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt8/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第八章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"0609c9def0a07b4797fa55a8a12a36a5","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt9/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt9/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第九章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"2b0bc3c2b0fba25e1802123b8dc2998b","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt9/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt9/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第九章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"fa08616ef4e423dce172e15c77d2b89a","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt9/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt9/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第九章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"9af0f7343234f4f2117fe222ec1d4b42","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt10/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt10/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"306b196232382c22287b5cefa1ed3daf","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt10/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt10/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"7df2c229937d2a81c489ea3b66f62eb3","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt10/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt10/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"bde094402d5a92b74e05d90739888398","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt11/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt11/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"cfbb05d3e445672d53166cb83da38005","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt11/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt11/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"1cfc188750d3700ed6a9f954f425bbb9","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt11/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt11/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十一章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"d228c83afa120bbec5a4869b8c744748","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt12/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt12/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"caa52b010a828113f02e39b9b2233931","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt12/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt12/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"b21c8e32aec33237d671e91f8de06aab","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt12/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt12/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十二章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"c6a6eff45479711484caea52b271f02c","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt13/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt13/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"fe3feb57b1a12020c8043e1255ecb5e4","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/chapt13/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/chapt13/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"33938a256ee9d6e5e6c9c8d30a1495a1","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt13/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt13/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十三章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"a33cddaa1e14aa18af09f25e972b5abe","permalink":"https://hhudelta.github.io/zh/teaching/classcspost/svm/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcspost/svm/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"SVM","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"3cbdb3c3eb13a4b2e574a2229f4b9086","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt14/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt14/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"8e163ab5cc7aef0a792b187300875c16","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt14/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt14/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十四章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"e05c51682b1c43b5802b37ca1a8009ff","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/chapt15/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/chapt15/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十五章","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"6bc610e2f73a00fcca10a6ac8bb93635","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/chapt15/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/chapt15/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"第十五章","type":"book"},{"authors":null,"categories":null,"content":"第一次小作业布置及如何使用河海查重系统\nDownload PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"2706093915d749b616c56ed9ad2dbc21","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/homework/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/homework/","section":"teaching","summary":"第一次小作业布置及如何使用河海查重系统\nDownload PPT\n","tags":null,"title":"小作业","type":"book"},{"authors":null,"categories":null,"content":"第一次小作业布置及如何使用河海查重系统\nDownload PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"bc57775ea44ecd6e9750bba512d204af","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/homework/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/homework/","section":"teaching","summary":"第一次小作业布置及如何使用河海查重系统\nDownload PPT\n","tags":null,"title":"小作业","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"9fef8c38be8d202eca48fe45b9495d93","permalink":"https://hhudelta.github.io/zh/teaching/classaiunder/paper/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classaiunder/paper/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"如何写好一篇科技论文","type":"book"},{"authors":null,"categories":null,"content":"Download PPT\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557014400,"objectID":"dbc4f056b0aeef95bb5ebbb35d9f6b82","permalink":"https://hhudelta.github.io/zh/teaching/classcsunder/paper/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/zh/teaching/classcsunder/paper/","section":"teaching","summary":"Download PPT\n","tags":null,"title":"如何写好一篇科技论文","type":"book"},{"authors":["巫义锐","夏宇航","Hao Li","袁俐新","Junyang Chen","Jun Liu","Tong Lu","Shaohua Wan"],"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1735689600,"objectID":"0693bce18e6b7c0be334e386ab737a4c","permalink":"https://hhudelta.github.io/zh/publication/aaai2025/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/zh/publication/aaai2025/","section":"publication","summary":"Incremental few-shot semantic segmentation (IFSS) expands segmentation capacity of the trained model to segment new class images with few samples. However, semantic meanings may shift from background to object class or vice versa dur ing incremental learning. Moreover, new-class samples of ten lack representative attribute features when the new class greatly differs from the pre-learned old class. In this paper, we propose a causal framework to discuss the cause of semantic shift and incompleteness in IFSS, and we deconfound the revealed causal effects from two aspects. First, we propose a Causal Intervention Module (CIM) to resist semantic shift. CIM progressively and adaptively updates prototypes of old class, and removes the confounder in an intervention manner. Second, a Prototype Refinement Module (PRM) is proposed to complete the missing semantics. In PRM, knowledge gained from the episode learning scheme assists in fusing fea tures of new-class and old-class prototypes. Experiments on both PASCAL-VOC 2012 and ADE20k benchmarks demon strate the outstanding performance of our method.","tags":[],"title":"Deconfound Semantic Shift and Incompleteness in Incremental Few-shot Semantic Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"Deconfound Semantic Shift and Incompleteness in Incremental Few-shot Semantic Segmentation\nAAAI 2025 （CCF-A，人工智能领域顶会）\n增量少样本语义分割（IFSS）扩展了训练模型的分割能力，使其能够用少量样本对新的类别图像进行分割。然而，在增量学习过程中，语义可能从背景转移到对象类别或反之亦然。此外，当新类别与预学习的旧类别差异很大时，新类别的样本通常缺乏代表性的属性特征。在本文中，我们提出一个因果框架来讨论IFSS中语义偏移和不完整性的原因，并从两个方面消除揭示的因果效应。首先，我们提出一个因果干预模块（CIM）来抵抗语义偏移。CIM逐步和自适应地更新旧类别的原型，并以干预的方式去除混杂因素。其次，我们提出一个原型精炼模块（PRM）来完成缺失的语义。在PRM中，从场景学习方案中获得的知识有助于融合新旧类别原型的特征。在PASCAL-VOC 2012和ADE20k基准测试上的实验证明了我们方法卓越的性能。 ","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1733788800,"objectID":"8bcca0be37a1453dcbeb0a02da88635d","permalink":"https://hhudelta.github.io/zh/post/24-12-10/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/zh/post/24-12-10/","section":"post","summary":"Deconfound Semantic Shift and Incompleteness in Incremental Few-shot Semantic Segmentation\n","tags":null,"title":"恭喜实验室李豪、夏宇航同学论文被AAAI2025(CCF-A)录用","type":"post"},{"authors":null,"categories":null,"content":"实验室巫义锐老师获评IEEE Senior Member\n","date":1733356800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1733356800,"objectID":"e9c6995fcf777bf4b9fa62a561db286c","permalink":"https://hhudelta.github.io/zh/post/24-12-05-1/","publishdate":"2024-12-05T00:00:00Z","relpermalink":"/zh/post/24-12-05-1/","section":"post","summary":"实验室巫义锐老师获评IEEE Senior Member\n","tags":null,"title":"实验室巫义锐老师获评IEEE Senior Member","type":"post"},{"authors":null,"categories":null,"content":"计算智能是一本人工智能领域的期刊，它出版关于人工智能和计算机科学广泛实验和理论主题的原创研究。该期刊具有广泛的范围，涵盖了机器学习、知识挖掘、网络智能、人工智能语言和哲学影响等方面。我们既被人工智能学术界的研究人员阅读，也被工业界的专业人士阅读。\n欢迎各位老师与同学投稿！！！\n","date":1733356800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1733356800,"objectID":"11681d9cb89b1e7694482351eb1c8da5","permalink":"https://hhudelta.github.io/zh/post/24-12-05/","publishdate":"2024-12-05T00:00:00Z","relpermalink":"/zh/post/24-12-05/","section":"post","summary":"计算智能是一本人工智能领域的期刊，它出版关于人工智能和计算机科学广泛实验和理论主题的原创研究。该期刊具有广泛的范围，涵盖了机器学习、知识挖掘、网络智能、人工智能语言和哲学影响等方面。我们既被人工智能学术界的研究人员阅读，也被工业界的专业人士阅读。\n","tags":null,"title":"实验室巫义锐老师受邀成为《Computational Intelligence》(CCF-C)的副主编","type":"post"},{"authors":null,"categories":null,"content":"由实验室巫义锐老师和电子科大万少华老师共同组织的applied science特刊\u0026#34;Advances in Few-Shot Learning with Multimodal Large Models\u0026#34;已经上线，地址是https://www.mdpi.com/journal/applsci/special_issues/3N827RPJSC，截止日期是2025年3月20日，欢迎各位老师同学投稿\n","date":1733011200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1733011200,"objectID":"74266124a199a84608d611d0af72d3d7","permalink":"https://hhudelta.github.io/zh/post/24-12-01/","publishdate":"2024-12-01T00:00:00Z","relpermalink":"/zh/post/24-12-01/","section":"post","summary":"由实验室巫义锐老师和电子科大万少华老师共同组织的applied science特刊\"Advances in Few-Shot Learning with Multimodal Large Models\"已经上线，地址是https://www.mdpi.com/journal/applsci/special_issues/3N827RPJSC，截止日期是2025年3月20日，欢迎各位老师同学投稿\n","tags":null,"title":"小样本学习专刊征稿 (Applied Sciences-Basel，中科院3区)","type":"post"},{"authors":null,"categories":null,"content":" ","date":1732665600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1732665600,"objectID":"e7666821138f6d11c7f76e14e1403edc","permalink":"https://hhudelta.github.io/zh/post/24-11-27/","publishdate":"2024-11-27T00:00:00Z","relpermalink":"/zh/post/24-11-27/","section":"post","summary":"","tags":null,"title":"恭喜师广琛同学获评江苏省优秀学术学位硕士与江苏省计算机学会优秀硕士学位论文","type":"post"},{"authors":null,"categories":null,"content":"新闻链接：JITAS 2024 | 多媒体前沿技术专题研讨会顺利召开\n在2024第三届江苏省信息技术应用大会期间，由江苏省信息技术应用学会主办，江苏省信息技术应用学会多媒体技术专委会承办的“多媒体前沿技术”专题研讨会在南京大学计算机楼230会议室召开。会议持续了三个半小时，吸引了众多来自高校和企业的专家学者参与，共同探讨智能化时代多媒体产业的核心技术与应用。会议以四个专题报告为核心，涵盖了多媒体领域的多个前沿话题。\n实验室巫义锐老师，袁俐新老师参与了会议的组织工作。\n","date":1732147200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1732147200,"objectID":"2163b8aed05f8fa03ff21e6d990e3932","permalink":"https://hhudelta.github.io/zh/post/24-11-21/","publishdate":"2024-11-21T00:00:00Z","relpermalink":"/zh/post/24-11-21/","section":"post","summary":"新闻链接：JITAS 2024 | 多媒体前沿技术专题研讨会顺利召开\n","tags":null,"title":"实验室参与组织的多媒体前沿技术专题研讨会在南京大学顺利召开","type":"post"},{"authors":null,"categories":null,"content":"孔其然同学发表在TCBB的论文 “CDT-CAD: Context-Aware Deformable Transformers for End-to-End Chest Abnormality Detection on X-Ray Images” 被评为ESI高被引论文。\n","date":1731974400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1731974400,"objectID":"776f1ac1f804b49b707dd034cec90007","permalink":"https://hhudelta.github.io/zh/post/24-11-19/","publishdate":"2024-11-19T00:00:00Z","relpermalink":"/zh/post/24-11-19/","section":"post","summary":"孔其然同学发表在TCBB的论文 “CDT-CAD: Context-Aware Deformable Transformers for End-to-End Chest Abnormality Detection on X-Ray Images” 被评为ESI高被引论文。\n","tags":null,"title":"恭喜孔其然同学发表在TCBB上的工作成为高被引论文","type":"post"},{"authors":null,"categories":null,"content":"在2024年11月7日于南京大学召开的“第三届江苏省信息技术应用大会”上，巫义锐教授荣获江苏省计算机学会青年科技奖，以表彰其在小样本学习领域的创新性成果。\n此次大会高度重视信息技术科研应用，汇聚省内外高校、科研单位和企业的专家，致力于搭建信息技术领域的多层次合作平台，为技术创新与成果转化注入新活力。巫义锐教授的成果专注于小样本学习的理论拓展、平台设计、落地应用和技术推广，积极主导平台建设、推动技术成果在水利等行业的高效应用，并通过高层学术论坛促进行业融合，推动该领域实现新突破。 ","date":1730937600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1730937600,"objectID":"22187d1f613adffc77126d2b4be3e0a0","permalink":"https://hhudelta.github.io/zh/post/24-11-7-2/","publishdate":"2024-11-07T00:00:00Z","relpermalink":"/zh/post/24-11-7-2/","section":"post","summary":"在2024年11月7日于南京大学召开的“第三届江苏省信息技术应用大会”上，巫义锐教授荣获江苏省计算机学会青年科技奖，以表彰其在小样本学习领域的创新性成果。\n","tags":null,"title":"实验室巫义锐老师获评2024江苏省信息技术应用学会青年科技奖","type":"post"},{"authors":null,"categories":null,"content":"江苏省青年科技人才托举工程（青托工程）是一项旨在培养和选拔优秀青年科技人才的重要人才计划。\n该工程主要面向江苏省内的青年科研人员，特别是年龄在35周岁以下、在自然科学和工程技术等领域具有显著成就和发展潜力的青年。通过提供资金支持、搭建科研平台、组织学术交流和人才培养等措施，青托工程旨在帮助青年科技人才快速成长，成为科技领域的领军人物。自实施以来，青托工程已培养出一批在国内外具有影响力的青年科技人才，为江苏省乃至全国的经济社会发展贡献了重要力量。 ","date":1730937600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1730937600,"objectID":"cd1f96e12382595f8efc145eac299670","permalink":"https://hhudelta.github.io/zh/post/24-11-7/","publishdate":"2024-11-07T00:00:00Z","relpermalink":"/zh/post/24-11-7/","section":"post","summary":"江苏省青年科技人才托举工程（青托工程）是一项旨在培养和选拔优秀青年科技人才的重要人才计划。\n","tags":null,"title":"实验室巫义锐老师获评2024年江苏省青年科技人才托举工程","type":"post"},{"authors":null,"categories":null,"content":"数字孪生流域是对物理流域全要素和水利治理管理活动全过程实现数字化映射和智能化模拟。围绕数字孪生流域建设，聚焦智能模拟技术如何通过大数据分析和人工智能算法，实现对流域水文、水质、环境等多维度数据的实时监测与预测。通过具体案例揭示了智能技术在洪水预警、生态保护等领域的显著成效。结合当下预训练大模型技术特点，展望数字孪生与智能模拟技术在流域管理中的未来发展趋势。\n","date":1726876800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1726876800,"objectID":"592d9020e1ae362c62868472f85db8c8","permalink":"https://hhudelta.github.io/zh/post/24-09-21/","publishdate":"2024-09-21T00:00:00Z","relpermalink":"/zh/post/24-09-21/","section":"post","summary":"数字孪生流域是对物理流域全要素和水利治理管理活动全过程实现数字化映射和智能化模拟。围绕数字孪生流域建设，聚焦智能模拟技术如何通过大数据分析和人工智能算法，实现对流域水文、水质、环境等多维度数据的实时监测与预测。通过具体案例揭示了智能技术在洪水预警、生态保护等领域的显著成效。结合当下预训练大模型技术特点，展望数字孪生与智能模拟技术在流域管理中的未来发展趋势。\n","tags":null,"title":"实验室巫义锐老师受邀在第十二届中国水利信息化技术论坛做数字孪生主题报告","type":"post"},{"authors":null,"categories":null,"content":"由实验室巫义锐老师组织的CCF YOCSEF南京Talk-解码黑神话悟空的游戏科学与艺术活动在中国图象图形学报直播平台顺利召开。\n新闻链接：CCF YOCSEF南京Talk总结 | 解码黑神话悟空的游戏科学与艺术，江苏游戏产业应以技术为突破点，寻求本地优势结合\n由我与东南吴天星老师共同组织策划，由南航魏明强老师，南大王贝贝老师，南大过洁老师，南艺严宝平老师，钟山虚拟现实研究院谢海春老师，南理工肖亮老师共同解码黑神话悟空背后的技术，艺术与产业。相约中国图象图形学报直播平台。\n","date":1725753600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1725753600,"objectID":"af7ada5878fd593acb86693a450e79c1","permalink":"https://hhudelta.github.io/zh/post/24-09-08/","publishdate":"2024-09-08T00:00:00Z","relpermalink":"/zh/post/24-09-08/","section":"post","summary":"由实验室巫义锐老师组织的CCF YOCSEF南京Talk-解码黑神话悟空的游戏科学与艺术活动在中国图象图形学报直播平台顺利召开。\n","tags":null,"title":"实验室参与组织的《解码黑神话悟空的游戏科学与艺术》活动在中国图象图形学报直播平台顺利召开","type":"post"},{"authors":null,"categories":null,"content":" 由中国计算机学会(CCF)主办，CCF计算机辅助设计与图形学专业委员会、华东交通大学、浙江大学南昌研究院（江西求是高等研究院）、国家虚拟现实创新中心、南昌航空大学、中国移动虚拟现实创新中心共同承办的第27届全国计算机辅助设计与图形学学术会议（CCF CAD/CG 2024）将于2024年8月15日至8月18日在江西南昌举行。此次会议将与第十六届全国几何设计与计算学术会议（GDC 2024）同期同地举办。 助力江西推进产业链现代化“1269行动计划”，打造南昌信息技术产业高地，会议旨在汇聚多个领域的专家学者、技术创新者和制造业先驱，探讨VR产业在制造业中的创新应用与发展。作为中国计算机辅助设计与计算机图形学领域的最高水平旗舰会议，为加快新一代信息技术与制造业相互渗透、相互融合，支持制造业企业数字化转型，推动VR产业的创新和普及，致力于引入智能化的技术和系统，以提高制造业的效率和质量，打造南昌信息技术产业高地。\n刘新富博士和秦锐同学合作的论文被CCF CAD/CG2024录用为口头报告论文，并被推荐至上海交通大学学报。\n","date":172368e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":172368e4,"objectID":"5b6704997b20e44c66aca296f2603d61","permalink":"https://hhudelta.github.io/zh/post/24-08-15/","publishdate":"2024-08-15T00:00:00Z","relpermalink":"/zh/post/24-08-15/","section":"post","summary":"","tags":null,"title":"实验室刘新富博士前往南昌参加CCF CAD/CG2024会议，并做口头报告","type":"post"},{"authors":["秦锐","Benze Wu","刘新富","巫义锐"],"categories":null,"content":"","date":1723593600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1723593600,"objectID":"02f88c2523478d9755b2cb16ce006886","permalink":"https://hhudelta.github.io/zh/publication/sju2024/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/zh/publication/sju2024/","section":"publication","summary":"In hyperspectral remote sensing imagery, pixel interactions within defined spatial extents result in the mixing of adjacent pixels. Additionally, the high similarity of adjacent spectra leads to information redundancy, which hinders the extraction of global spatial and spectral correlations. In order to solve the problems of mixed adjacent pixels and redundant adjacent spectra, this work offers a hyperspectral image classification approach that uses a global space-spectral attention mechanism. First, the proposed method’s global spatial attention module uses multi-scale dilated convolution to produce a bigger receptive field to be capable of capturing global spatial correlation and obtain unmixed pixel information. Then, the global spectral attention module designs a spectral domain partition algorithm, using the combination of regional density as well as information entropy as the threshold to divide spectrum into dispersed subsets and eliminate redundant information. The global context information for entire spectral band is fully exploited, and correlation of the global spectral information is extracted. Finally, the two modules combine to provide a global correlation of space and spectrum. Experiments demonstrate that the suggested method obtains overall accuracies of 97.28%, 94.73%, and 95.76% on the three WHU-Hi hyperspectral datasets, surpassing comparison methods.","tags":[],"title":"Hyperspectral Image Classification Method Based on Global Space-Spectral Attention Mechanism","type":"publication"},{"authors":null,"categories":null,"content":"Hyperspectral Image Classification Method Based on Global Space-spectral Attention Mechanism\n摘要：在超光谱遥感图像中，定义的空间范围内像素之间的相互作用导致相邻像素的混合。此外，相邻光谱的高相似性导致信息冗余，这阻碍了全局空间和光谱相关性的提取。为了解决相邻像素混合和相邻光谱冗余的问题，本文提出了一种基于全局空间-光谱注意力机制的 hyperspectral 图像分类方法。首先，所提出方法中的全局空间注意力模块使用多尺度膨胀卷积来获得更大的感受野，以捕获全局空间相关性并获得未混合的像素信息。然后，全局光谱注意力模块设计了一种光谱域划分算法，使用局部密度和信息熵的乘积作为阈值将光谱划分为分散的子集，消除冗余信息。充分利用了整个光谱带的全局上下文信息，并提取了全局光谱信息的相关性。最后，将这两个模块连接起来，获得空间和光谱的全局相关性。实验表明，该方法在三个 WHU-Hi 超光谱数据集上实现了97.28%、94.73%和95.76%的整体准确率，超过了比较方法。\n","date":1723593600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1723593600,"objectID":"a7f516b11ea8a624aa3dc94cff33623f","permalink":"https://hhudelta.github.io/zh/post/24-08-14/","publishdate":"2024-08-14T00:00:00Z","relpermalink":"/zh/post/24-08-14/","section":"post","summary":"Hyperspectral Image Classification Method Based on Global Space-spectral Attention Mechanism\n","tags":null,"title":"恭喜实验室秦锐同学论文被上海交通大学学报(英文版)录用","type":"post"},{"authors":null,"categories":null,"content":"Edge Computing and Few-shot Learning Featured Intelligent Framework in Digital Twin empowered Mobile Networks\n发表期刊：IEEE Transactions on Network and Service Management(中科院2区) 数字孪生（DT）和移动网络在物联网（IoT）中演变出了智能形式。在本工作中，我们考虑了一个数字孪生移动网络（DTMN）场景，其中多媒体样本较少。面对少量样本知识提取、与多媒体数据动态变化的稳定交互、在低资源移动网络中节省时间和隐私的挑战，我们提出了一种具有边缘计算和少样本学习特性的智能框架。考虑到传输的时效性和移动网络中直接上传的隐私风险，我们部署边缘计算在本地运行网络进行分析，从而节省时间卸载计算请求，并通过加密原始数据提高隐私性。受图中的显著关系表示启发，我们在云端构建了图神经网络（GNN），将物理移动系统映射到数字孪生中的虚拟实体，从而在云端使用边缘上传的少量样本进行语义推理。偶尔，GNN中的节点特征可能会收敛到相似的非判别性嵌入，导致灾难性的不稳定现象。因此，在云端构建了一个迭代重新加权和丢弃结构（IRDS），尽管如此，它仍然对边缘不确定性提供了稳定性。作为IRDS的一部分，我们提出了一种丢弃边缘和节点（Edge\u0026amp;Node）方案，随机移除某些节点和边，这不仅增强了图邻接模式的区分能力，还提供了基于随机策略的数据加密。我们展示了一个社交网络中图像分类的实现案例，在公共数据集上的实验表明，我们的框架具有用户友好的优势，并具有显著智能。\n","date":1723248e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1723248e3,"objectID":"53e4d73a89ee10356ec3440aaaa5c130","permalink":"https://hhudelta.github.io/zh/post/24-08-10/","publishdate":"2024-08-10T00:00:00Z","relpermalink":"/zh/post/24-08-10/","section":"post","summary":"Edge Computing and Few-shot Learning Featured Intelligent Framework in Digital Twin empowered Mobile Networks\n","tags":null,"title":"恭喜实验室曹浩同学论文被TNSM录用","type":"post"},{"authors":null,"categories":null,"content":"中国盐城创新创业大赛，由盐城市政府主办，旨在推动区域创新发展，吸引全球创新人才，促进科技成果转化，该大赛聚焦多个高新技术领域，通过多阶段的赛事选拔和多元化支持，提升城市形象，助力经济发展，并致力于培养和孵化创新人才。\n实验室刘新富博士、靳修同学、耿奥同学携实验室3个产学研项目分别在决赛现场进行了路演。 ","date":1721952e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1721952e3,"objectID":"8015e5837a6a38543bc73212ca1638bc","permalink":"https://hhudelta.github.io/zh/post/24-07-26/","publishdate":"2024-07-26T00:00:00Z","relpermalink":"/zh/post/24-07-26/","section":"post","summary":"中国盐城创新创业大赛，由盐城市政府主办，旨在推动区域创新发展，吸引全球创新人才，促进科技成果转化，该大赛聚焦多个高新技术领域，通过多阶段的赛事选拔和多元化支持，提升城市形象，助力经济发展，并致力于培养和孵化创新人才。\n","tags":null,"title":"实验室多位同学参加中国盐城创新创业大赛","type":"post"},{"authors":null,"categories":null,"content":"感谢物联网与智能城市国际研讨会(ISITSC 2024)的邀请。欢迎各位前来线下参会：南京金帆万源酒店–江苏省南京市鼓楼区牌楼巷 47-1 号(近汉中门地铁站,江苏省中医院)\n","date":1719014400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1719014400,"objectID":"7000348eba40dc34f819f8439940e001","permalink":"https://hhudelta.github.io/zh/post/24-06-22-isitsc/","publishdate":"2024-06-22T00:00:00Z","relpermalink":"/zh/post/24-06-22-isitsc/","section":"post","summary":"感谢物联网与智能城市国际研讨会(ISITSC 2024)的邀请。欢迎各位前来线下参会：南京金帆万源酒店–江苏省南京市鼓楼区牌楼巷 47-1 号(近汉中门地铁站,江苏省中医院)\n","tags":null,"title":"实验室巫义锐老师受邀在物联网与智能城市国际研讨会上做小样本视觉学习专题演讲","type":"post"},{"authors":null,"categories":null,"content":"我们发表在TECS的论文 “Edge-AI-Driven Framework with Efficient Mobile Network Design for Facial Expression Recognition” 被评为ESI高被引论文\n","date":1716336e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1716336e3,"objectID":"4cb2fc98363c3bedf3d36783be39b784","permalink":"https://hhudelta.github.io/zh/post/24-05-22-tecs-high-cited/","publishdate":"2024-05-22T00:00:00Z","relpermalink":"/zh/post/24-05-22-tecs-high-cited/","section":"post","summary":"我们发表在TECS的论文 “Edge-AI-Driven Framework with Efficient Mobile Network Design for Facial Expression Recognition” 被评为ESI高被引论文\n","tags":null,"title":"实验室发表在TECS的论文被评为ESI高被引论文","type":"post"},{"authors":[],"categories":null,"content":"","date":1715504400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1715504400,"objectID":"937ede74a72f55763cfd7574b08ce1e1","permalink":"https://hhudelta.github.io/zh/event/24-05-11-ccf-forum/","publishdate":"2024-05-11T00:00:00Z","relpermalink":"/zh/event/24-05-11-ccf-forum/","section":"event","summary":"由CCF南京分部主办的，CCF YOCSEF南京协办的CCF 南京分部公益论坛：大模型助力新质生产力，将于2024 年 5 月 12 日（周日）9:00-11:20在河海大学江宁校区行政楼一楼 多功能厅举行.","tags":[],"title":"CCF 南京分部公益论坛：大模型助力新质生产力","type":"event"},{"authors":null,"categories":null,"content":"由CCF南京分部主办的，CCF YOCSEF南京协办的CCF 南京分部公益论坛：大模型助力新质生产力，将于2024 年 5 月 12 日（周日）9:00-11:20在河海大学江宁校区行政楼一楼 多功能厅举行。活动面向高校大学生，邀请到了国内知名大模型专家，东南大学漆桂林教授、南京大学王利民教授，对自然语言和视觉大模型技术的当下与 未来发展脉络做技术报告，提高学生对新一代人工智能技术-预训练大模型的认知，启迪他们设计技术落地实际应用的场景，为他们未来的职业规划、 职业目标做准备。欢迎各位学术界、工业界小伙伴以及同学们现场参与。 ","date":1715385600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1715385600,"objectID":"b48f782d43e08f3a4410ae1a9a481901","permalink":"https://hhudelta.github.io/zh/post/24-05-11-ccf-forum/","publishdate":"2024-05-11T00:00:00Z","relpermalink":"/zh/post/24-05-11-ccf-forum/","section":"post","summary":"由CCF南京分部主办的，CCF YOCSEF南京协办的CCF 南京分部公益论坛：大模型助力新质生产力，将于2024 年 5 月 12 日（周日）9:00-11:20在河海大学江宁校区行政楼一楼 多功能厅举行。活动面向高校大学生，邀请到了国内知名大模型专家，东南大学漆桂林教授、南京大学王利民教授，对自然语言和视觉大模型技术的当下与 未来发展脉络做技术报告，提高学生对新一代人工智能技术-预训练大模型的认知，启迪他们设计技术落地实际应用的场景，为他们未来的职业规划、 职业目标做准备。欢迎各位学术界、工业界小伙伴以及同学们现场参与。 ","tags":null,"title":"CCF 南京分部公益论坛：大模型助力新质生产力","type":"post"},{"authors":null,"categories":null,"content":"实验室访学记录：重庆VALSE行,2024年5月4日至7日,实验室组织了研究生同学，前往重庆参加VALSE2024学术会议。\n","date":1714953600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1714953600,"objectID":"ce03df1a7560ceb3560948d14cc14155","permalink":"https://hhudelta.github.io/zh/post/24-05-06-valse/","publishdate":"2024-05-06T00:00:00Z","relpermalink":"/zh/post/24-05-06-valse/","section":"post","summary":"实验室访学记录：重庆VALSE行,2024年5月4日至7日,实验室组织了研究生同学，前往重庆参加VALSE2024学术会议。\n","tags":null,"title":"实验室同学前往重庆参加VALSE2024学术会议","type":"post"},{"authors":[],"categories":null,"content":"","date":1714809600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1714809600,"objectID":"197b81da2e7cb91f8f1e689e45492b1f","permalink":"https://hhudelta.github.io/zh/event/24-05-06/","publishdate":"2024-05-06T00:00:00Z","relpermalink":"/zh/event/24-05-06/","section":"event","summary":"2024年5月4日至7日,实验室组织研究生同学，前往重庆参加VALSE2024学术会议。","tags":[],"title":"实验室访学记录：重庆VALSE行","type":"event"},{"authors":null,"categories":null,"content":"我们发表在TOIT的论文 “Digital Twin of Intelligent Small Surface Defect Detection with Cyber-Manufacturing Systems” 被评为ESI高被引论文。\n","date":1710460800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1710460800,"objectID":"8eadcd0af1d83def38b07a8958ce5b22","permalink":"https://hhudelta.github.io/zh/post/24-03-15-toit-high-cited/","publishdate":"2024-03-15T00:00:00Z","relpermalink":"/zh/post/24-03-15-toit-high-cited/","section":"post","summary":"我们发表在TOIT的论文 “Digital Twin of Intelligent Small Surface Defect Detection with Cyber-Manufacturing Systems” 被评为ESI高被引论文。\n","tags":null,"title":"实验室发表在TOIT的论文被评为ESI高被引论文","type":"post"},{"authors":null,"categories":null,"content":"我们发表在TNSE的论文 “Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection” 被评为ESI热点论文。\n","date":1710028800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1710028800,"objectID":"d4a34b62d77f62d8bf306f8868a1d9c6","permalink":"https://hhudelta.github.io/zh/post/24-03-10-tnse-hot-paper/","publishdate":"2024-03-10T00:00:00Z","relpermalink":"/zh/post/24-03-10-tnse-hot-paper/","section":"post","summary":"我们发表在TNSE的论文 “Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection” 被评为ESI热点论文。\n","tags":null,"title":"实验室发表在TNSE的论文被评为ESI热点论文。","type":"post"},{"authors":["Yi Rong","Haoran Zhou","袁俐新","Cheng Mei","Jiahao Wang","Tong Lu"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704067200,"objectID":"c7aa4ff8ad120dd231bed431a7d3bed8","permalink":"https://hhudelta.github.io/zh/publication/rong-cra-pcn-2024/","publishdate":"2024-12-18T16:43:54.971648Z","relpermalink":"/zh/publication/rong-cra-pcn-2024/","section":"publication","summary":"Point cloud completion is an indispensable task for recovering complete point clouds due to incompleteness caused by occlusion, limited sensor resolution, etc. The family of coarse-to-fine generation architectures has recently exhibited great success in point cloud completion and gradually became mainstream. In this work, we unveil one of the key ingredients behind these methods: meticulously devised feature extraction operations with explicit cross-resolution aggregation. We present Cross-Resolution Transformer that efficiently performs cross-resolution aggregation with local attention mechanisms. With the help of our recursive designs, the proposed operation can capture more scales of features than common aggregation operations, which is beneficial for capturing fine geometric characteristics. While prior methodologies have ventured into various manifestations of inter-level cross-resolution aggregation, the effectiveness of intra-level one and their combination has not been analyzed. With unified designs, Cross-Resolution Transformer can perform intra- or inter-level cross-resolution aggregation by switching inputs. We integrate two forms of Cross-Resolution Transformers into one up-sampling block for point generation, and following the coarse-to-fine manner, we construct CRA-PCN to incrementally predict complete shapes with stacked up-sampling blocks. Extensive experiments demonstrate that our method outperforms state-of-the-art methods by a large margin on several widely used benchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.","tags":null,"title":"CRA-PCN: Point Cloud Completion with Intra-and Inter-level Cross-Resolution Transformers","type":"publication"},{"authors":["刘新富","巫义锐","周玉婷","Junyang Chen","Huan Wang","Ye Liu","Shaohua Wan"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704067200,"objectID":"109ab578689a7fea23978dbe7827eea7","permalink":"https://hhudelta.github.io/zh/publication/liu-enhancing-2024/","publishdate":"2024-12-24T07:13:10.091115Z","relpermalink":"/zh/publication/liu-enhancing-2024/","section":"publication","summary":"Open-set object recognition plays a significant role in today’s production and daily life, such as in surface defect detection, biometric identification, and autonomous driving recognition. However, due to the diversity of unknown categories and the complexity of scenarios, existing methods often perform poorly. Therefore, open-set object recognition remains an important and popular research topic. Recently, collaborative utilization of multiple pre-trained Large Language Models (LLMs) has emerged rapidly, which becomes a new research hotspot in addressing open-set object recognition tasks. Among this, a core challenge lies in amplifying the strengths of individual LLMs while mitigating their weaknesses. In this paper, we propose a novel joint framework tailored for open-set object recognition tasks, aiming to more efficiently harness the capabilities of diverse LLMs and Knowledge Graphs (KGs). Initially, for the text data generated by textual LLMs, we use Wikipedia to correct and complete it. Then, we designed a text-image multi-modal fusion method to further correct and complete the text information by utilizing the implicit semantic information in the image. Additionally, we propose some novel designs to alleviate the hallucination issue of LLMs and reduce their instability. Extensive experiments demonstrate that our approach outperforms all the comparison methods.","tags":null,"title":"Enhancing Large Language Models with Multimodality and Knowledge Graphs for Hallucination-free Open-set Object Recognition","type":"publication"},{"authors":["袁俐新","Cheng Mei","Wenhai Wang","Tong Lu"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704067200,"objectID":"609fd7cf368ee241ad8f99aa04c42f19","permalink":"https://hhudelta.github.io/zh/publication/yuan-feature-2024/","publishdate":"2024-12-18T16:43:54.829346Z","relpermalink":"/zh/publication/yuan-feature-2024/","section":"publication","summary":"Feature selection (FS) has recently attracted considerable attention in many fields. Highly-overlapping classes and skewed distributions of data within classes have been found in various classification tasks. Most existing FS methods are all instance-based, which ignores the significant differences in characteristics between the particular outliers and the main body of the class, causing confusion for classifiers. In this paper, we propose a novel supervised FS method, Intrusive Outliers-based Feature Selection (IOFS), to find out what kind of outliers lead to misclassification and exploit the characteristics of such outliers. In order to accurately identify the intrusive outliers (IOs), we provide a density-mean center algorithm to obtain the appropriate representative of a class. A special distance threshold is given to obtain the candidate for IOs. Combining with several metrics, mathematical formulations are provided to evaluate the overlapping degree of the intrusive class pairs. Features with high overlapping degrees are assigned to low rankings in IOFS method. An extension of IOFS based on a small number of extreme IOs, called E-IOFS, is also proposed. Three theoretical proofs are provided for the essential theoretical basis of IOFS. Experiments comparing against various state-of-the-art methods on eleven benchmark datasets show that IOFS is rational and effective, especially on the datasets with higher overlapping classes. And E-IOFS almost always outperforms IOFS.","tags":["classification","density-mean center","Face recognition","Feature extraction","intrusive outlier","Measurement","Mutual information","overlapping class","Solid modeling","Supervised feature selection","Task analysis","Training"],"title":"Feature Selection Based on Intrusive Outliers Rather Than All Instances","type":"publication"},{"authors":["Guangchen Shi","Wei Zhu","巫义锐","Danhuai Zhao","Kang Zheng","Tong Lu"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704067200,"objectID":"c7b414ad6041a473a3f3e1eca3c639d9","permalink":"https://hhudelta.github.io/zh/publication/shi-few-shot-2024/","publishdate":"2024-12-24T07:13:10.078879Z","relpermalink":"/zh/publication/shi-few-shot-2024/","section":"publication","summary":"Few-shot semantic segmentation (FSS) aims to locate pixels of unseen classes with clues from a few labeled samples. Recently, thanks to profound prior knowledge, diffusion models have been expanded to achieve FSS tasks. However, due to probabilistic noising and denoising processes, it is difficult for them to maintain spatial relationships between inputs and outputs, leading to inaccurate segmentation masks. To address this issue, we propose a Diffusion-based Segmentation network (DiffSeg), which decouples probabilistic denoising and segmentation processes. Specifically, DiffSeg leverages attention maps extracted from a pretrained diffusion model as support-query interaction information to guide segmentation, which mitigates the impact of probabilistic processes while benefiting from rich prior knowledge of diffusion models. In the segmentation stage, we present a Perceptual Attention Module (PAM), where two cross-attention mechanisms capture semantic information of support-query interaction and spatial information produced by the pretrained diffusion model. Furthermore, a self-attention mechanism within PAM ensures a balanced dependence for segmentation, thus preventing inconsistencies between the aforementioned semantic and spatial information. Additionally, considering the uncertainty inherent in the generation process of diffusion models, we equip DiffSeg with a Spatial Control Module (SCM), which models spatial structural information of query images to control boundaries of attention maps, thus aligning the spatial location between knowledge representation and query images. Experiments on PASCAL-5i and COCO datasets show that DiffSeg achieves new state-of-the-art performance with remarkable advantages.","tags":["diffusion model","few-shot segmentation","perceptual attention","spatial control"],"title":"Few-shot Semantic Segmentation via Perceptual Attention and Spatial Control","type":"publication"},{"authors":["Junyang Chen","Guoxuan Zou","Pan Zhou","巫义锐","Zhenghan Chen","Houcheng Su","Huan Wang","Zhiguo Gong"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1704067200,"objectID":"85bcb7868bc2fc14f437fcef991b4552","permalink":"https://hhudelta.github.io/zh/publication/chen-sparse-2024/","publishdate":"2024-12-24T07:13:10.099096Z","relpermalink":"/zh/publication/chen-sparse-2024/","section":"publication","summary":"Sequential Recommendation plays a significant role in daily recommendation systems, such as e-commerce platforms like Amazon and Taobao. However, even with the advent of large models, these platforms often face sparse issues in the historical browsing records of individual users due to new users joining or the introduction of new products. As a result, existing sequence recommendation algorithms may not perform well. To address this, sequence-based data augmentation methods have garnered attention. Existing sequence enhancement methods typically rely on augmenting existing data, employing techniques like cropping, masking prediction, random reordering, and random replacement of the original sequence. While these methods have shown improvements, they often overlook the exploration of the deep embedding space of the sequence. To tackle these challenges, we propose a Sparse Enhanced Network (SparseEnNet), which is a robust adversarial generation method. SparseEnNet aims to fully explore the hidden space in sequence recommendation, generating more robust enhanced items. Additionally, we adopt an adversarial generation method, allowing the model to differentiate between data augmentation categories and achieve better prediction performance for the next item in the sequence. Experiments have demonstrated that our method achieves a remarkable 4-14% improvement over existing methods when evaluated on the real-world datasets. https://github.com/junyachen/SparseEnNet","tags":null,"title":"Sparse Enhanced Network: An Adversarial Generation Method for Robust Augmentation in Sequential Recommendation","type":"publication"},{"authors":null,"categories":null,"content":"我们发表在TNSE的论文 “Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection” 被评为ESI高被引论文\n","date":1703203200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1703203200,"objectID":"2e639a80abab3bbf1f4827835d49ccf4","permalink":"https://hhudelta.github.io/zh/post/23-12-22-tnse-high-cited/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/zh/post/23-12-22-tnse-high-cited/","section":"post","summary":"我们发表在TNSE的论文 “Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection” 被评为ESI高被引论文\n","tags":null,"title":"实验室发表在TNSE的论文被评为ESI高被引论文","type":"post"},{"authors":[],"categories":null,"content":" ","date":1701165600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1701165600,"objectID":"10f8c133531826e77aeba7a666809217","permalink":"https://hhudelta.github.io/zh/event/23-11-08/","publishdate":"2023-11-08T00:00:00Z","relpermalink":"/zh/event/23-11-08/","section":"event","summary":"下周二11月28日河海江宁校区勤学楼4202会议室，有幸请到IAPR Fellow Pal老师，和西北工业大学的田春伟老师，欢迎有兴趣的老师与同学前往参加。报告以线上线下方式同步举行。 #腾讯会议：434-142-344","tags":[],"title":"IAPR Fellow Pal老师 西北工业大学 田春伟老师 报告会","type":"event"},{"authors":["巫义锐","Qiran Kong","Cheng Qian","Michele Nappi","Shaohua Wan"],"categories":null,"content":" ","date":1701129600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1701129600,"objectID":"deb5f4fb9d0ecbf5e9f952f659d5372d","permalink":"https://hhudelta.github.io/zh/publication/bigdr2023/","publishdate":"2023-11-28T00:00:00Z","relpermalink":"/zh/publication/bigdr2023/","section":"publication","summary":"Deep learning has achieved great success in text detection, where recent methods adopt inspirations from segmentation to detect scene texts. However, most segmentation based methods have high computation cost in pixel-level classification and post refinements. Moreover, they still faces challenges like arbitrary directions, curved texts, illumination and so on. Aim to improve detection accuracy and computation cost, we propose an end-to-end and single-stage method named as End-PolarT network by generating contour points in polar coordinates for text detection. End-PolarT not only regress locations of contour points instead of pixels to relieve high computation cost, but also fits with intrinsic characteristics of text instances by centers and contours to suppress mislabeling boundary pixels. To cope with polar representation, we further propose polar IoU and centerness as key parts of loss functions to generate effective paradigms for text detection. Compared with the existing methods, End-PolarT achieves superior results by testing on several public datasets, thus keeping balance between efficiency and effectiveness in complicated scenes.","tags":[],"title":"End-PolarT: Polar Representation for End-to-End Scene Text Detection","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1699437600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1699437600,"objectID":"4ff77b3879bf318b346d997cc7e75504","permalink":"https://hhudelta.github.io/zh/event/23-11-09/","publishdate":"2023-11-09T00:00:00Z","relpermalink":"/zh/event/23-11-09/","section":"event","summary":"2023年11月8日至9日,实验室组织了分散在南京与金坛的研究生同学，前往苏州相城区进行秋游。 整体活动安排为一日自由行，一日参观CCF总部与相城区规划馆，体验商用自动驾驶汽车。","tags":[],"title":"实验室秋游记录：苏州相城行","type":"event"},{"authors":["Hao Li","巫义锐","Hexuan Hu","Hu Lu","Qian Huang","Shaohua Wan"],"categories":null,"content":" ","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1696118400,"objectID":"8fd5b3653529157560a419eac17f15b5","permalink":"https://hhudelta.github.io/zh/publication/methods2023/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/zh/publication/methods2023/","section":"publication","summary":"Deep learning has brought a significant progress in medical image analysis. However, their lack of interpretability might bring high risk for wrong diagnosis with limited clinical knowledge embedding. In other words, we believe it's crucial for humans to interpret how deep learning work for medical analysis, thus appropriately adding knowledge constraints to correct the bias of wrong results. With such purpose, we propose Representation Group-Disentangling Network (RGD-Net) to explain the process of feature extraction and decision making inside deep learning framework, where we completely disentangle feature space of input X-ray images into independent feature groups, and each group would contribute to diagnose of a specific disease. Specifically, we first state problem definition for interpretable prediction with auto-encoder structure. Then, group-disentangled representations are extracted from input X-ray images with the proposed Group-Disentangle Module, which constructs semantic latent space by enforcing semantic consistency of attributes. Afterwards, adversarial constricts on mapping from features to diseases are proposed to prevent model collapse during training. Finally, a novel design of local tuning medical application is proposed based on RGB-Net, which is capable to aid clinicians for reasonable diagnosis. By conducting quantity of experiments on public datasets, RGD-Net have been superior to comparative studies by leveraging potential factors contributing to different diseases. We believe our work could bring interpretability in digging inherent patterns of deep learning on medical image analysis.","tags":[],"title":"Interpretable Thoracic Pathologic Prediction via Learning Group-Disentangled Representation","type":"publication"},{"authors":["巫义锐","Qiran Kong","Lai Yong","Fabio Narducci","Shaohua Wan"],"categories":null,"content":"","date":169344e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":169344e4,"objectID":"6ca1716d01b3815848162536ed374e0b","permalink":"https://hhudelta.github.io/zh/publication/prl2023-2/","publishdate":"2023-08-31T00:00:00Z","relpermalink":"/zh/publication/prl2023-2/","section":"publication","summary":"","tags":[],"title":"CDText: Scene Text Detector Based on Context-Aware Deformable Transformer","type":"publication"},{"authors":["巫义锐","Lilai Zhang","Zonghua Gu","Hu Lu","Shaohua Wan"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1682899200,"objectID":"2c33cba216c077cf83b0d624a511785d","permalink":"https://hhudelta.github.io/zh/publication/tecs2023/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/zh/publication/tecs2023/","section":"publication","summary":"Facial Expression Recognition (FER) in the wild poses significant challenges due to realistic occlusions, illumination, scale, and head pose variations of the facial images. In this article, we propose an Edge-AI-driven framework for FER. On the algorithms aspect, we propose two attention modules, Arbitrary-oriented Spatial Pooling (ASP) and Scalable Frequency Pooling (SFP), for effective feature extraction to improve classification accuracy. On the systems aspect, we propose an edge-cloud joint inference architecture for FER to achieve low-latency inference, consisting of a lightweight backbone network running on the edge device, and two optional attention modules partially offloaded to the cloud. Performance evaluation demonstrates that our approach achieves a good balance between classification accuracy and inference latency.","tags":[],"title":"Edge-AI-Driven Framework with Efficient Mobile Network Design for Facial Expression Recognition","type":"publication"},{"authors":["巫义锐","Qiran Kong","Lilai Zhang","Aniello Castiglione","Michele Nappi","Shaohua Wan"],"categories":null,"content":"","date":1679011200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1679011200,"objectID":"dd5af90b199a56646d2f0304cbe3bc4d","permalink":"https://hhudelta.github.io/zh/publication/tcbb2023/","publishdate":"2023-03-17T00:00:00Z","relpermalink":"/zh/publication/tcbb2023/","section":"publication","summary":"","tags":[],"title":"CDT-CAD: Context-Aware Deformable Transformers for End-to-End Chest Abnormality Detection on X-Ray Images","type":"publication"},{"authors":["Qian Cheng","巫义锐","Aniello Castiglione","Fabio Narducci","Shaohua Wan"],"categories":null,"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677628800,"objectID":"661031a40d039cde79f87788f31f079b","permalink":"https://hhudelta.github.io/zh/publication/jsps2023/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/zh/publication/jsps2023/","section":"publication","summary":"Flood prediction is a challenging task due to the extreme runoff values, short duration, and complex generation mechanisms. This paper introduces DA-Net, a dual attention embedding network that incorporates convolution self-attention (CSA) and Temporal-related Feature Attention (TFA) to improve flood forecasting accuracy. CSA captures local context, while TFA enhances global feature modeling. The proposed method outperforms existing deep learning models on the Changhua and Tunxi watershed datasets.","tags":[],"title":"DA-Net: Dual Attention Network for Flood Forecasting","type":"publication"},{"authors":["巫义锐","Lilai Zhang","Hao Li","Yunfei Zhang","Shaohua Wan"],"categories":null,"content":"","date":1675123200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1675123200,"objectID":"c2344a32a88b28efe82f9bffdab55356","permalink":"https://hhudelta.github.io/zh/publication/tallip2023/","publishdate":"2023-01-31T00:00:00Z","relpermalink":"/zh/publication/tallip2023/","section":"publication","summary":"How to properly involve text characteristics like multi-scale, arbitrary direction, length aspect ratio, into detection network design has become a hot topic in computer vision. Feature Pyramid Network (FPN) is a typical method to achieve robust text detection, where its low-level and high-level feature map retains spatial structure and global semantic information, respectively. However, its strict hierarchical structure fails to fuse low-level and high-level information to improve the distinguish ability of feature map. To address this problem, we propose a novel feature fusion pyramid network for end-to-end scene text detection by fusing multi-modal information. By dividing pyramid structure into high-level and low-level layers, channel and spatial attention modules are adopted to enhance high-level and low-level feature representation by encoding channel and spatial-wise context information, respectively. In order to reduce information loss by layer transmission, a special residual network is designed to achieve short-cut between high-level and low-level features, so as to realize multi-modal feature fusion. Experiments show the precision and recall of the proposed method on ICDAR2015, ICDAR2017-MLT, and MSRA-TD500 datasets reach 88.7%/82.1%, 77.0%/60.3%, and 85.3%/74.8%, respectively.","tags":[],"title":"Feature Fusion Pyramid Network for End-to-End Scene Text Detection","type":"publication"},{"authors":["巫义锐","Benze Wu","Yunfei Zhang","Shaohua Wan"],"categories":null,"content":"","date":1673568e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1673568e3,"objectID":"4116d2bfabaf5cc3b035cdbc7b9f3988","permalink":"https://hhudelta.github.io/zh/publication/sc2023/","publishdate":"2023-01-13T00:00:00Z","relpermalink":"/zh/publication/sc2023/","section":"publication","summary":"Deep learning has shown remarkable performance in quantity of vision tasks. However, its large network generally requires quantity of samples to support sufficient parameters learning during training process. Such high request greatly reduces efficiency when applying on a small dataset with few samples. To alleviate this problem, we propose a novel data enhancement method for few-shot learning via a cutout approach and feature enhancement. After enhancement, the generated network not only produces distinguish feature map without collecting more samples, but also achieves advantage of feature representation with high efficiency for computing. Specifically, cutout approach is simple yet highly effective for image regulation, which enhances input image matrix by adding a fixed mask to improve robustness and overall performance of network. Afterward, we perform feature enhancement by proposing a feature promotion module, which uses characteristics of dilated convolution and sequential processing to improve feature representation ability, thus improving efficiency of the whole network. We conduct comparative experiments on both miniImageNet and CUB datasets, where the proposed method is superior to comparative methods in both 1-shot and 5-shot cases.","tags":[],"title":"A Novel Method of Data and Feature Enhancement for Few-Shot Image Classification","type":"publication"},{"authors":["巫义锐","Hao Li","Xi Feng","Andrea Casanova","Andrea F. Abate","Shaohua Wan"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1672531200,"objectID":"d899e0aca94257949fc46a81cce5b7e7","permalink":"https://hhudelta.github.io/zh/publication/prl2023-1/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/zh/publication/prl2023-1/","section":"publication","summary":"Deep learning methods have shown significant performance in medical image analysis tasks. However, they generally act like “black box” without explanations in both feature extraction and decision processes, leading to lack of clinical insights and high risk assessments. To aid deep learning in envisioning diseases with visual clues, we propose a novel Group-Disentangled Representation Learning framework (GDRL). The key contribution is that GDRL completely disentangles latent space into disease concepts with abundant and non-overlapping feature related explanations, thus enhancing interpretability in feature extraction and decision processes. Furthermore, we introduce an implicit group-swap structure by emphasizing the linking relationship between semantical concepts of disease and low-level visual features, other than explicit explanations on general objects and their attributes. We demonstrate our framework on predicting four categories of diseases from chest X-ray images. The AUROC of GDRL on ChestX-ray14 for thoracic pathologic prediction are 0.8630, 0.8980, 0.9269 and 0.8653 respectively, and we showcase the potential of our framework in enhancing interpretability of the factors contributing to different diseases.","tags":[],"title":"GDRL: An Interpretable Framework for Thoracic Pathologic Prediction","type":"publication"},{"authors":["巫义锐","Hao Cao","Guoqiang Yang","Tong Lu","Shaohua Wan"],"categories":null,"content":"","date":1668643200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1668643200,"objectID":"d7134b57e0376167bd592ea08b0bd0fe","permalink":"https://hhudelta.github.io/zh/publication/toit2022/","publishdate":"2022-11-17T00:00:00Z","relpermalink":"/zh/publication/toit2022/","section":"publication","summary":"The rapid advancement in cyber-physical systems has led to the evolution of Industry 4.0, with a key concept being the digital twin (DT). However, linking twin simulations with real-world scenarios remains challenging, especially in tasks like small surface defect detection. This article proposes a cyber-manufacturing system with a DT solution for small surface defect detection. The system uses an Edge-Cloud architecture for efficient data collection and processing, coupled with a deep learning-based detection algorithm utilizing multi-modal data. Experiments demonstrate high accuracy and recall in small defect detection.","tags":[],"title":"Digital Twin of Intelligent Small Surface Defect Detection with Cyber-manufacturing Systems","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://hhudelta.github.io/zh/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/zh/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://hhudelta.github.io/zh/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/zh/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":["Minglei Yuan","Chunhao Cai","Tong Lu","巫义锐","Qian Xu","Shijie Zhou"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1661990400,"objectID":"4bf8be5d47134e5de2a36d189acca8d6","permalink":"https://hhudelta.github.io/zh/publication/pr2022/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/zh/publication/pr2022/","section":"publication","summary":"Existing Few-Shot Learning (FSL) methods learn and recognize new classes with the help of prior knowledge. However, they cannot handle this task well in a cross-domain scenario when training and testing sets are from different domains, since the fact that prior knowledge in different domains often varies greatly. To solve this problem, in this paper, we propose a few-shot domain generalization method, which is designed to extract relationship embeddings using Forget-Update Modules named FUM. The relationship embedding considers valuable relational information between samples in a specific task, and the forget-update module takes into account differences between domains and adjusts the distribution of relational embeddings through forgetting and updating mechanisms based on specific tasks. To evaluate the few-shot domain generalization ability of FUM, extensive experiments on eight cross-domain scenarios and six same-domain scenarios are conducted, and the results show that FUM achieves superior performances compared to recent few-shot learning methods. Visualization results also show that the distribution of the relationship embeddings extracted by FUM has stronger few-shot domain generalization ability than the feature embeddings used in the existing FSL methods.","tags":[],"title":"A Novel Forget-Update Module for Few-Shot Domain Generalization","type":"publication"},{"authors":["巫义锐","Lilai Zhang","Stefano Berretti","Shaohua Wan"],"categories":null,"content":"","date":1658966400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1658966400,"objectID":"a17248951a5b4d4af05151051a7f6d85","permalink":"https://hhudelta.github.io/zh/publication/tii2022/","publishdate":"2022-07-28T00:00:00Z","relpermalink":"/zh/publication/tii2022/","section":"publication","summary":"There exists a rising concern on security of healthcare data and service. Even small lost, stolen, displaced, hacked, or communicated in personal health data could bring huge damage to patients. Therefore, we propose a novel content-aware deoxyribonucleic acid (DNA) computing system to encrypt medical images, thus guaranteeing privacy and promoting secure healthcare environment. The proposed system consists of sender and receiver to perform tasks of encryption and decryption, respectively, where both contain the same structure design, but perform opposite operations. In either sender or receiver, we design a randomly DNA encoding and a content-aware permutation and diffusion module. Considering introducing random mechanism to increase difficulty of cracking, the former module builds a random encryption rule selector in DNA encoding process by randomly mapping quantity of medical image pixels to outputs. Meanwhile, the latter module constructs a permutation sequence, which not only encodes information of pixel values, but also involves redundant correlation between adjacent pixels located in a patch. Such design brings awareness property of medical image content to greatly increase complexity in cracking by embedding semantical information for encryption. We demonstrate that the proposed system successfully improves cybersecurity of medical images against various attacks in robustness and effectiveness when transmitting data in wireless broadcasting scenarios.","tags":[],"title":"Medical Image Encryption by Content-Aware DNA Computing for Secure Healthcare","type":"publication"},{"authors":["Guangchen Shi","巫义锐","Jun Liu","Shaohua Wan","Wehai Wang","Tong Lu"],"categories":null,"content":"","date":1658793600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1658793600,"objectID":"f1f45dbff130e2840add01dabf85bf48","permalink":"https://hhudelta.github.io/zh/publication/acmmm2022/","publishdate":"2022-07-26T00:00:00Z","relpermalink":"/zh/publication/acmmm2022/","section":"publication","summary":"Incremental few-shot semantic segmentation (IFSS) targets at incrementally expanding model's capacity to segment new class of images supervised by only a few samples. However, features learned on old classes could significantly drift, causing catastrophic forgetting. Moreover, few samples for pixel-level segmentation on new classes lead to notorious overfitting issues in each learning session. In this paper, we explicitly represent class-based knowledge for semantic segmentation as a category embedding and a hyper-class embedding, where the former describes exclusive semantical properties, and the latter expresses hyper-class knowledge as class-shared semantic properties. Aiming to solve IFSS problems, we present EHNet, i.e., Embedding adaptive-update and Hyper-class representation Network from two aspects. First, we propose an embedding adaptive-update strategy to avoid feature drift, which maintains old knowledge by hyper-class representation, and adaptively update category embeddings with a class-attention scheme to involve new classes learned in individual sessions. Second, to resist overfitting issues caused by few training samples, a hyper-class embedding is learned by clustering all category embeddings for initialization and aligned with category embedding of the new class for enhancement, where learned knowledge assists to learn new knowledge, thus alleviating performance dependence on training data scale. Significantly, these two designs provide representation capability for classes with sufficient semantics and limited biases, enabling to perform segmentation tasks requiring high semantic dependence. Experiments on PASCAL-5i and COCO datasets show that EHNet achieves new state-of-the-art performance with remarkable advantages.","tags":[],"title":"Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation","type":"publication"},{"authors":["巫义锐","Wen Zhang","Shaohua Wan"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1656633600,"objectID":"d98beea2b874a8bb8dd00b07dc4d9fb0","permalink":"https://hhudelta.github.io/zh/publication/prl2022/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/zh/publication/prl2022/","section":"publication","summary":"With the significant power of deep learning architectures, researchers have made much progress on effectiveness and efficiency of text detection in the past few years. However, due to the lack of consideration of unique characteristics of text components, directly applying deep learning models to perform text detection task is prone to result in low accuracy, especially producing false positive detection results. To ease this problem, we propose a lightweight and context-aware deep convolutional neural network (CNN) named as CE-Text, which appropriately encodes multi-level channel attention information to construct discriminative feature map for accurate and efficient text detection. To fit with low computation resource of embedded systems, we further transform CE-Text into a lighter version with a frequency based deep CNN compression method, which expands applicable scenarios of CE-Text into variant embedded systems. Experiments on several popular datasets show that CE-Text not only has achieved accurate text detection results in scene images, but also could run with fast performance in embedded systems.","tags":[],"title":"CE-text: A Context-Aware and Embedded Text Detector in Natural Scene Images","type":"publication"},{"authors":["Yonghong Chen","Hao Li","Han Li","Wenhao Liu","巫义锐","Qian Huang","Shaohua Wan"],"categories":null,"content":"","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1649980800,"objectID":"53a7b3edad694ab36f41e666a601a270","permalink":"https://hhudelta.github.io/zh/publication/tallip2022/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/zh/publication/tallip2022/","section":"publication","summary":"In recent years, with the rapid development of Internet technology and applications, the scale of Internet data has exploded, which contains a significant amount of valuable knowledge. The best methods for the organization, expression, calculation, and deep analysis of this knowledge have attracted a great deal of attention. The knowledge graph has emerged as a rich and intuitive way to express knowledge. Knowledge reasoning based on knowledge graphs is one of the current research hot spots in knowledge graphs and has played an important role in wireless communication networks, intelligent question answering, and other applications. Knowledge graph-oriented knowledge reasoning aims to deduce new knowledge or identify wrong knowledge from existing knowledge. Different from traditional knowledge reasoning, knowledge reasoning methods oriented to knowledge graphs are more diversified due to the concise, intuitive, flexible, and rich knowledge expression forms in knowledge graphs. Based on the basic concepts of knowledge graphs and knowledge graph reasoning, this paper introduces the latest research progress in knowledge graph-oriented knowledge reasoning methods in recent years. Specifically, according to different reasoning methods, knowledge graph reasoning includes rule-based reasoning, distributed representation-based reasoning, neural network-based reasoning, and mixed reasoning. These methods are summarized in detail, and the future research directions and prospects of knowledge reasoning based on knowledge graphs are discussed and prospected.","tags":[],"title":"An Overview of Knowledge Graph Reasoning: Key Technologies and Applications","type":"publication"},{"authors":["巫义锐","Haifeng Guo","Chinmay Chakraborty","Mohammad R. Khosravi","Stefano Berretti","Shaohua Wan"],"categories":null,"content":"","date":1644796800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1644796800,"objectID":"a7ca5d0ef45806152596d91bc30edc12","permalink":"https://hhudelta.github.io/zh/publication/tnse2022/","publishdate":"2022-02-14T00:00:00Z","relpermalink":"/zh/publication/tnse2022/","section":"publication","summary":"With fast increase in volume of mobile multimedia data, how to apply powerful deep learning methods to process data with real-time response becomes a major issue. Meanwhile, edge computing structure helps improve response time and user experience by bringing flexible computation and storage capabilities. Considering both technologies for successful AI-based applications, we propose an edge-computing driven and end-to-end framework to perform tasks of image enhancement and object detection under low-light conditions. The framework consists of a cloud-based enhancement and an edge-based detection stage. In the first stage, we establish connections between edge devices and cloud servers to input re-scaled illumination parts of low-light images, where enhancement subnetworks are dynamically and parallel coupled to compute enhanced illumination parts based on low-light context. During the edge-based detection stage, edge devices could accurately and rapidly detect objects based on cloud-computed informative feature map. Experimental results show the proposed method significantly improves detection performance in low-light conditions with low latency running on edge devices.","tags":[],"title":"Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection","type":"publication"},{"authors":["袁俐新","Guoqiang Yang","Qian Xu","Tong Lu"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1640995200,"objectID":"3963c9bfb2bf847f28784b57437557f6","permalink":"https://hhudelta.github.io/zh/publication/yuan-discriminative-2022/","publishdate":"2024-12-18T16:43:54.880803Z","relpermalink":"/zh/publication/yuan-discriminative-2022/","section":"publication","summary":"With the rapid development of multimedia technologies (e.g. deep learning), Feature Selection (FS) is now playing a critical role in acquiring discriminative features from massive data. Traditional FS methods score feature importance and select the top best features by treating all instances equally; Hence, valuable instances like directional outliers (DOs), which are specific outliers closer to other class centres than to their owns, seldom receive particular attention during feature selection. Based on our observation, DOs derive from “misclassified instances” which lead to misclassification. In this paper, we present a novel supervised feature selection method entitled Feature Selection via Directional Outliers Correcting (FSDOC), for accurate data classification. The proposed FSDOC includes an optimization algorithm to capture DOs, and two correcting algorithms to reasonably capture redundant features by correcting DOs with intraclass deviation minimization and interclass relative distance maximization. We give theoretical guarantees and adequate analysis on all algorithms to show the effectiveness of FSDOC. Extensive experiments on fifteen public datasets, and two case studies of deep features and very-high dimensional Fisher Vector selection, demonstrate the superior performance of FSDOC.","tags":["Deviation","Directional outlier","Feature selection","Redundant features","Supervised method"],"title":"Discriminative feature selection with directional outliers correcting for data classification","type":"publication"},{"authors":["Hao Li","巫义锐","Hexuan Hu","Hu Lu","Shaohua Wan"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1640995200,"objectID":"c2ca79dea98390d8f166305b2c3c7fbf","permalink":"https://hhudelta.github.io/zh/publication/bibm2022/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/zh/publication/bibm2022/","section":"publication","summary":"Deep learning methods have shown significant performance in medical image analysis tasks. However, they generally act like ”black box” without explanations in both feature extraction and decision processes, leading to lack of clinical insights and high risk assessments. To aid deep learning in envisioning diseases with visual clues, we propose Representation Group-Disentangling Network (RGD-Net), which can completely disentangle feature space of input X-ray images into several independent feature groups, each corresponding to a specific disease. Taking several semantically related and labeled X-ray images as input, RGD-Net firstly extracts completely group-disentangled representations of diseases through Group-Disentangle Module, which applies group-swap and linking operations to construct latent space by enforcing semantic consistency of attributes. To prevent learning degenerate representations defined as shortcut problem, we further introduce adversarial constricts on mapping from features to diseases, thus avoiding model collapse with former free-form disentanglement. Experiments on chestxray-14 and ChestXpert datasets demonstrate that RGD-Net are effective in predicting diseases with remarkable advantages, which leverage potential factors contributing to different diseases, thus enhancing interpretability in working patterns of deep learning methods.","tags":[],"title":"Learning Group-Disentangled Representation for Interpretable Thoracic Pathologic Prediction","type":"publication"},{"authors":["Qiran Kong","巫义锐","Chi Yuan","Yongli Wang"],"categories":null,"content":"","date":1639008e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1639008e3,"objectID":"b28486e302117cf1f66c9630514dbce2","permalink":"https://hhudelta.github.io/zh/publication/bibm2021/","publishdate":"2021-12-09T00:00:00Z","relpermalink":"/zh/publication/bibm2021/","section":"publication","summary":"Supervised based deep learning methods have achieved great success in medical image analysis domain. Essentially, most of them could be further improved by exploring and embedding context knowledge for accuracy boosting. Moreover, they generally suffer from slow convergency and high computing cost, which prevents their usage in a practical scenario. To tackle these problems, we present CT-CAD, context-aware transformers for end-to-end chest abnormality detection on X-Ray images. The proposed method firstly constructs a context-aware feature extractor, which enlarges receptive fields to encode multi-scale context information via an iterative feature fusion scheme and dilated context encoding blocks. Afterwards, deformable transformer detector are built for category classification and location regression, where their deformable attention block attend to a small set of key sampling points, thus allowing the transformer to focus on feature subspace and accelerate convergence speed. Through comparative experiments on Vinbig Chest and Chest Det10 Datasets, the proposed CT-CAD demonstrates its effectiveness and outperforms the existing methods in mAP and training epoches.","tags":[],"title":"CT-CAD: Context-Aware Transformers for End-to-End Chest Abnormality Detection on X-Rays","type":"publication"},{"authors":["Benze Wu","巫义锐","Shaohua Wan"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1633046400,"objectID":"6958afde5978e9094d377731e216172e","permalink":"https://hhudelta.github.io/zh/publication/euc2021-2/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/zh/publication/euc2021-2/","section":"publication","summary":"In order to predict the unknown image categories, few-shot image classification has recently become a very hot field. However, many methods need a large number of samples to support in order to achieve enough functions. This makes the whole network de amplification to meet a large number of effective feature extraction, and reduces the efficiency of few-shot classification to a certain extent. To solve these problems, we propose a dilate convolutional network with data enhancement. This network can not only meet the necessary features of image classification without increasing the number of samples, but also has a structure that utilizes a large number of effective features without sacrificing efficiency. The cutout structure can enhance the data by adding a fixed area 0 mask matrix in the process of image input. The structure of FAU uses dilate convolution and uses the characteristics of a sequence to improve the efficiency of the network.","tags":[],"title":"An Image Enhancement Method for Few-shot Classification","type":"publication"},{"authors":["巫义锐","Pengfei Han","Zhan Zheng"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1633046400,"objectID":"7c3080248dcafb164cfdc73f86e24d5d","permalink":"https://hhudelta.github.io/zh/publication/jrtip2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/zh/publication/jrtip2021/","section":"publication","summary":"Water resources are critical for human survival and require continuous monitoring for protection. Machine learning methods have been successfully applied to the identification of water bodies by analyzing remote sensing images. The proposed method first performs pixel-level classification to detect abnormal changes using visual word patterns, then applies a block division method for parallel processing based on a MapReduce structure. This approach allows for accurate and rapid detection of water body variations with instant feedback. Experiments on self-collected datasets demonstrate superior efficiency compared to comparative methods.","tags":[],"title":"Instant water body variation detection via analysis on remote sensing imagery","type":"publication"},{"authors":["Qiran Kong","巫义锐","Shaohua Wan"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1633046400,"objectID":"d9252ac59c14bb2cecc252bfc3bb6a68","permalink":"https://hhudelta.github.io/zh/publication/euc2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/zh/publication/euc2021/","section":"publication","summary":"Although deep learning has achieved great success in object detection recently, scene text detection remains a challenging task due to inherent difficulties in locating texts in complex scenes. Many approaches are inspired by segmentation methods to detect arbitrarily shaped scene text. However, most segmentation-based methods are computationally expensive and require significant refinements for accurate results. To address this issue, we propose PolarText, a novel single-stage method that detects text regions by generating contour points in polar coordinates. PolarText reduces computation costs by directly regressing contour points instead of pixels and better aligns with the intrinsic characteristics of text instances using centers and contours, mitigating boundary pixel mislabeling caused by pixel-level labeling. The network introduces Polar IoU loss and polar centerness to adapt effective paradigms from box representation for polar representation. Additionally, we incorporate a bounding box branch to handle text detection, as most text instances are approximately rectangular. Experimental results on CTW 1500 and ICDAR 2015 datasets show that PolarText achieves superior accuracy and efficiency compared to existing methods.","tags":[],"title":"PolarText: Single-stage Scene Text Detection with Polar Representation","type":"publication"},{"authors":["Tingting Hang","Jun Feng","巫义锐","Le Yan","Yunfeng Wang"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1630454400,"objectID":"d102c9790b979e9d9ec6b4eae749017e","permalink":"https://hhudelta.github.io/zh/publication/eswa2021/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/zh/publication/eswa2021/","section":"publication","summary":"Joint extraction of entities and overlapping relations has attracted considerable attention in recent research. Existing relation extraction methods rely on a training set that is labeled by the distant supervision method for supervised relation extraction. However, the drawbacks of these methods are that large-scale unlabeled data cannot be used and the quality of labeled data cannot be guaranteed. Moreover, owing to the relatively complex overlapping relations, it is difficult to perform joint entity-relation extraction accurately. In this study, we propose an end-to-end neural network model (BERT-JEORE) for the joint extraction of entities and overlapping relations. First, we use the BERT-based parameter-sharing layer to capture the joint features of entities and overlapping relations. Then, we implement the source-target BERT model to assign entity labels to each token in a sentence, thereby expanding the amount of labeled data and improving their quality. Finally, we design a three-step overlapping relations extraction model and use it to predict the relations between all entity pairs. Experiments conducted on two public datasets show that BERT-JEORE achieves the best current performance and outperforms the baseline models by a significant margin. Further analysis shows that our model can effectively capture different types of overlapping relational triplets in a sentence.","tags":[],"title":"Joint Extraction of Entities and Overlapping Relations Using Source-Target Entity Labeling","type":"publication"},{"authors":["Guangchen Shi","巫义锐"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1630454400,"objectID":"edf9debea5faef0d3a25eb2d013e8fe8","permalink":"https://hhudelta.github.io/zh/publication/jig2021/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/zh/publication/jig2021/","section":"publication","summary":"目的 获取场景图像中的文本信息对理解场景内容具有重要意义，而文本检测是对文本识别、理解的基础。目前，场景文本检测是最具挑战性的任务之一，正受到越来越多的研究关注。方法 本文提出了一种高效的任意形状文本检测器：非局部像素聚合网络，该方法使用特征金字塔增强模块和特征融合模块进行轻量级特征提取，保证了速度优势；同时引入非局部操作以增强骨干网络的提取特征的能力，使其检测的准确性得到提高。非局部操作是一种注意力机制，它能捕捉到文本像素之间的内在关系。此外，本文设计了一种特征向量融合模块，用于融合不同尺度的特征图，使尺度多变的场景文本实例的特征表达得到增强。结果 本方法在3个场景文本数据集上与其他方法进行了比较，在速度和准确度上均表现突出。在ICDAR2015数据集上，本方法比最优方法的F值提高了1.5%，检测速度达到了23.1FPS；在CTW1500数据集上，本方法比最优方法的F值提高了1.8%，检测速度达到了71.8FPS；在Total-Text数据集上，本方法比最优方法的F值提高了0.8%，检测速度达到了64.3FPS，远远超出其它方法。结论 本文所提出的方法兼顾了准确性和实时性，在准确度和速度方面均处于领先水平。","tags":[],"title":"像素聚合和特征增强的任意形状场景文本检测","type":"publication"},{"authors":["巫义锐","Wenxiang Liu","Shaohua Wan"],"categories":null,"content":"","date":1627776e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1627776e3,"objectID":"0a14eb20d590e11e796065be1ec7d0fd","permalink":"https://hhudelta.github.io/zh/publication/jvcir2021/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/zh/publication/jvcir2021/","section":"publication","summary":"Inspired by instance segmentation algorithms, researchers have proposed a variety of segmentation-based methods for text detection, achieving remarkable results on scene text with arbitrary orientation and large aspect ratios. Following their success, we believe cascade architecture and extracting contextual information in multiple aspects are powerful in boosting performance on the basis of segmentation-based methods, especially in decreasing false positive texts in complex natural scenes. Based on such considerations, we propose a multiple-context-aware and cascade CNN structure, which appropriately encodes multiple categories of context information into a cascade R-CNN framework. Specifically, the proposed method consists of two stages: feature generation and cascade detection. In the first stage, we define the ISTK (Isolated Selective Text Kernel) module to refine the feature map, which sequentially encodes channel-wise and kernel-size attention information by designing multiple branches and different kernel sizes in isolated form. Afterwards, we build long-range spatial dependencies in the feature map via non-local operations. Built on the contextual feature map, the Cascade Mask R-CNN structure progressively refines the accurate boundaries of text instances with a multi-stage framework. We conduct comparative experiments on ICDAR2015 and 2017-MLT datasets, where the proposed method outperforms comparative methods in terms of effectiveness and efficiency.","tags":[],"title":"Multiple Attention Encoded Cascade R-CNN for Scene Text Detection","type":"publication"},{"authors":["巫义锐","Wenqin Mao","Jun Feng"],"categories":null,"content":"","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1626998400,"objectID":"5298b64aa6afc75bcebf7d2d22aae58e","permalink":"https://hhudelta.github.io/zh/publication/monet2021/","publishdate":"2021-07-23T00:00:00Z","relpermalink":"/zh/publication/monet2021/","section":"publication","summary":"Cloud/edge computing and deep learning greatly improve the performance of semantic understanding systems, where cloud/edge computing provides flexible, pervasive computation and storage capabilities to support variant applications, and deep learning models can comprehend text inputs by consuming computing and storage resources. We propose implementing an intelligent online customer service system powered by both technologies. This method jointly models two subtasks, intent recognition and slot filling, in an end-to-end neural network, enhancing feature representation using attention schemes and context information. We deploy this method in an intelligent dialogue system for electrical customer service, with experiments showing promising results on both public and self-collected datasets.","tags":[],"title":"AI for Online Customer Service: Intent Recognition and Slot Filling Based on Deep Learning Technology","type":"publication"},{"authors":["巫义锐","Hongfei Guo","Cheng Qian","Wenpeng Wang"],"categories":null,"content":"","date":1625702400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1625702400,"objectID":"2d788e7335e3cfd5d5a14bd3ca9e8dd6","permalink":"https://hhudelta.github.io/zh/publication/yr2021/","publishdate":"2021-07-08T00:00:00Z","relpermalink":"/zh/publication/yr2021/","section":"publication","summary":"面对洪水发生频率低且机制复杂的问题，提出了一类深度神经网络模型( ET － LSTM) 。该模型通过构建特征增强模块提升了小样本情况下的洪水预报能力，通过结合时序感知模块的深度神经网络模型，构建洪水因子与径流量间的非线性关系，挖掘洪水因子间的隐含时序关联关系。首先利用一维卷积神经网络构建洪水深度特征表达; 然后，结合瓶颈( BottleNeck) 结构设计，通过特征通道间的信息交换，增强洪水深度特征的表达能力; 最后，构建时序无关和时序相关模块，分别提取深度特征中的时序相关与时序无关部分，进一步提升深度特征的时变表达能力，并在流域数据集上进行对比分析。结果表明: 该方法在模拟精度、相关性系数等指标上优于对比方法，能够更好地拟合真实径流量数据，提升洪水预报的准确性与预见期。","tags":[],"title":"基于特征增强与时序感知的洪水预报模型","type":"publication"},{"authors":["Guangchen Shi","巫义锐","Shivakumara Palaiahnakote","Umapada Pal","Tong Lu"],"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1625097600,"objectID":"029a60c80575cae8783cf12e1a7ec0d4","permalink":"https://hhudelta.github.io/zh/publication/icme2021/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/zh/publication/icme2021/","section":"publication","summary":"Few-shot segmentation has become a research focus for making predictions on unseen classes. However, most methods rely on pixel-level annotation, requiring significant manual effort, and the diversity in feature representation of same-category objects due to size, appearance, or layout differences poses additional challenges. To address these issues, the proposed active-reference network (ARNet) introduces an active-reference mechanism that supports accurate segmentation with cooccurrent objects in support or query images and relaxes the need for precise pixel-level labeling by allowing weak boundary labeling. Additionally, a category-modulation module (CMM) is applied to fuse features from multiple support images, selectively forgetting irrelevant information and enhancing key features. Experiments on the PASCAL-5i dataset show ARNet achieves a mean IOU score of 56.5% for 1-shot and 59.8% for 5-shot segmentation, outperforming the current state-of-the-art methods by 0.5% and 1.3%, respectively.","tags":[],"title":"ARNet: Active-Reference Network for Few-Shot Image Semantic Segmentation","type":"publication"},{"authors":["Jun Feng","Zhongyi Wang","巫义锐","Yuqi Xi"],"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1625097600,"objectID":"52ed2159d4f7d12e6b2e98ef0626ce5b","permalink":"https://hhudelta.github.io/zh/publication/ijcnn2021/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/zh/publication/ijcnn2021/","section":"publication","summary":"Intelligent flood forecasting systems provide an effective means to predict flood disasters. Accurate flood flow value prediction is a challenging task as it is influenced by both spatial and temporal relationships among flood factors. Popular deep learning architectures like Long Short-Term Memory (LSTM) networks lack the ability to model the spatial correlations of hydrological data, which hinders achieving satisfactory prediction results. Additionally, not all temporal information is equally valuable for flood forecasting. This paper proposes a novel Spatial and Temporal Aware Graph Convolutional Network (ST-GCN) for flood prediction, which extracts spatial-temporal information from raw flood data. Furthermore, a temporal attention mechanism is introduced to weight the importance of different time steps, leveraging global temporal information to improve flood prediction accuracy. Experimental results on two self-collected datasets demonstrate that ST-GCN significantly enhances prediction performance compared to existing methods.","tags":[],"title":"Spatial and Temporal Aware Graph Convolutional Network for Flood Forecasting","type":"publication"},{"authors":["巫义锐","Yuntao Ma","Shaohua Wan"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1622505600,"objectID":"77e78615780f7fbdc0c960fb0065de86","permalink":"https://hhudelta.github.io/zh/publication/spic2021/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/zh/publication/spic2021/","section":"publication","summary":"The goal of Visual Question Answering (VQA) is to answer questions about images. For the same picture, there are often completely different types of questions. Therefore, the main difficulty of the VQA task lies in how to properly reason relationships among multiple visual objects according to types of input questions. To solve this difficulty, this paper proposes a deep neural network to perform multi-modal relation reasoning in multi-scales, which successfully constructs a regional attention scheme to focus on informative and question-related regions for better answering. Specifically, we first design a regional attention scheme to select regions of interest based on informative evaluation computed by a question-guided soft attention module. Afterwards, features computed by the regional attention scheme are fused in scaled combinations, generating more distinctive features with scalable information. Due to the designs of regional attention and multi-scale property, the proposed method is capable of describing scaled relationships from multi-modal inputs to offer accurate question-guided answers. By conducting experiments on the VQA v1 and VQA v2 datasets, we show that the proposed method has superior efficiency compared to most existing methods.","tags":[],"title":"Multi-Scale Relation Reasoning for Multi-Modal Visual Question Answering","type":"publication"},{"authors":["Yuntao Ma","Tong Lu","巫义锐"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1609459200,"objectID":"395858670249b8dff9af3c4155fb6ce1","permalink":"https://hhudelta.github.io/zh/publication/icpr2020-1/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/zh/publication/icpr2020-1/","section":"publication","summary":"One of the main challenges in visual question answering (VQA) is properly reasoning relations among visual regions involved in the question. In this paper, we propose a novel neural network for question-guided relational reasoning at multiple scales in VQA, where each image region is enhanced through regional attention. Specifically, we introduce a regional attention module consisting of both soft and hard attention mechanisms to select informative regions of the image based on question-guided evaluations. Different combinations of informative regions are then concatenated with question embeddings across scales to capture relational information. The relational reasoning module extracts question-based relationships among regions, with the multi-scale mechanism enhancing the model's sensitivity to numbers and its ability to model diverse relationships. Experimental results demonstrate that our approach achieves state-of-the-art performance on the VQA v2 dataset.","tags":[],"title":"Multi-scale Relational Reasoning with Regional Attention for Visual Question Answering","type":"publication"},{"authors":["Pengyu Yu","巫义锐","Benze Wu"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1606780800,"objectID":"a95affc5313fa8458f10a150ce512e69","permalink":"https://hhudelta.github.io/zh/publication/hpcc2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/zh/publication/hpcc2020/","section":"publication","summary":"In order to ensure code quality, it's necessary to construct portraits for developers, which could analyze their behavior to provide personalized programming suggestions. However, most of the existing developer portrait algorithms only use global features and ignore local features extracted from log texts, which leads to the lack of comprehensive personality analysis. To solve this problem, the proposed method proposes a novel developer portrait model, which could describe developers' programming styles more accurately with both global and local information extracted from texts. The proposed model firstly collects the log data produced in the process of continuous integration development. Afterwards, the proposed method proposes the personality portrait model based on BERT-Capsule network, which successfully combines global semantic features and local emotional features. The experimental results show that the proposed BERT-Capsule model can effectively extract the contextual information and the local emotional information of the text, thus improving classification performance of the developer portrait model.","tags":[],"title":"A Novel Developer Portrait Model based on Bert-Capsule Network","type":"publication"},{"authors":["Pengyu Yu","巫义锐","Shun Zhao"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1606780800,"objectID":"419c25ba7c2ad5061466e34b292e79f9","permalink":"https://hhudelta.github.io/zh/publication/smartcity2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/zh/publication/smartcity2020/","section":"publication","summary":"With the rapid development of information technology, the scale of software products is getting larger. In order to ensure code quality, it's necessary to collect log information in the continuous integration development process, analyze the behavior of developers, build developer user profile models, and provide personalized suggestions to developers based on profile information. All these processes can be defined as goals of user portrait model. Essentially, user portrait model is designed to model users' coding behaviours based on a large amount of data. However, current methods often suffer from the problem of imbalanced data, which is the core challenge for a successful portrait model. In fact, most parts of log information refer to regular programmers, while only few samples correspond to programmers who are supposed to be improved by suggestions from the portrait model. To solve this problem, we propose to adopt SMOTE Algorithm to deal with the imbalanced log data, which is the core innovation of the proposed method. Experiments show the proposed SMOTE Algorithm based model could accurately classify programmers' personality types and offer suggestions.","tags":[],"title":"A Novel SMOTE Algorithm based Portrait Model for Programmers","type":"publication"},{"authors":["Xiaofang Li","巫义锐","Wen Zhang","Ruichao Wang","Feng Hou"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1606780800,"objectID":"c975954ff13d490d0ae9ab92f54be04d","permalink":"https://hhudelta.github.io/zh/publication/jrtip2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/zh/publication/jrtip2020/","section":"publication","summary":"Super-resolution is generally defined as a process to obtain high-resolution images from low-resolution inputs, which has attracted considerable attention from the image-processing community. This paper aims to analyze, compare, and contrast technical problems, methods, and the performance of super-resolution research, especially real-time super-resolution methods based on deep learning structures. Specifically, we first summarize fundamental problems, categorize algorithms, and analyze possible application scenarios. Increasing attention has been drawn to utilizing convolutional neural networks (CNN) or generative adversarial networks (GAN) to predict high-frequency details lost in low-resolution images. We provide a general overview of background technologies and pay special attention to super-resolution methods based on deep learning architectures for real-time processing, which not only produce desirable reconstruction results but also expand the possible applications of super-resolution to systems like cell phones, drones, and embedded systems. Benchmark datasets are enumerated, and the performance of the most representative approaches is compared to provide a fair view of current methods. Finally, we conclude the paper and suggest ways to improve the use of deep learning methods in real-time image super-resolution.","tags":[],"title":"Deep learning methods in real-time image super-resolution: a survey","type":"publication"},{"authors":["Haifeng Guo","Tong Lu","巫义锐"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1601510400,"objectID":"d3e0dbf8a1a4b50675fc5b2a51bcdaa7","permalink":"https://hhudelta.github.io/zh/publication/icpr2020/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/zh/publication/icpr2020/","section":"publication","summary":"Object detection based on convolutional neural networks is a key area in computer vision. The illumination component in images significantly affects detection performance, especially under low-light conditions. Although low-light image enhancement can improve image quality and detection performance, existing methods may negatively affect some samples, making it difficult to improve overall detection accuracy in low-light environments. This paper proposes a novel framework that combines low-light enhancement with object detection, enabling end-to-end training. The framework dynamically selects the appropriate enhancement subnetworks for each sample to improve detector performance. The approach consists of two stages: the enhancement stage, which enhances low-light images based on various enhancement methods and outputs corresponding weights, and the detection stage, where the weights provide information for object classification to generate high-quality region proposals, resulting in more accurate detection. Experimental results demonstrate that the proposed method significantly improves detection performance in low-light environments.","tags":[],"title":"Dynamic Low-Light Image Enhancement for Object Detection via End-to-End Training","type":"publication"},{"authors":["巫义锐","Xiaozhong Ji","Wanting Ji","Yan Tian","Helen Zhou"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1598918400,"objectID":"1510c4ed1b30ee610ac025a4696d7142","permalink":"https://hhudelta.github.io/zh/publication/ncaa2020/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/zh/publication/ncaa2020/","section":"publication","summary":"With the significant power of deep learning architectures, researchers have made much progress on super-resolution in the past few years. However, due to low representational ability of feature maps extracted from nature scene images, directly applying deep learning architectures for super-resolution could result in poor visual effects. Essentially, unique characteristics like low-frequency information should be emphasized for better shape reconstruction, other than treated equally across different patches and channels. To ease this problem, we propose a lightweight context-aware deep residual network named as CASR network, which appropriately encodes channel and spatial attention information to construct context-aware feature map for single-image super-resolution. We firstly design a task-specified inception block with a novel structure of astrous filters and specially chosen kernel size to extract multi-level information from low-resolution images. Then, a Dual-Attention ResNet module is applied to capture context information by dually connecting spatial and channel attention schemes. With high representational ability of context-aware feature map, CASR can accurately and efficiently generate high-resolution images. Experiments on several popular datasets show the proposed method has achieved better visual improvements and superior efficiencies than most of the existing studies.","tags":[],"title":"CASR: a context-aware residual network for single-image super-resolution","type":"publication"},{"authors":["巫义锐","Pengyu Yu"],"categories":null,"content":"","date":159624e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":159624e4,"objectID":"58344cb566400dd1b0613b9687aab0d9","permalink":"https://hhudelta.github.io/zh/publication/cybersci2020/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/zh/publication/cybersci2020/","section":"publication","summary":"Because of the differences in code styles and programming levels among developers, it is prone to code irregularities, poor readability, and security vulnerabilities. Although developers can see their problems in the test report, it is difficult to guarantee that they will not make mistakes on the same problem. In this article, we propose a way to build a programming level for developers. The proposed method explores the Stacking model for building a programming level for developers. First, cluster the problems that occurred during the development process, give weight to the category information, score the developer's programming level, and divide the user groups into different categories. Then use the word vector to extract the features of the code defect, generate a feature matrix, and finally pass the feature matrix to the Stacking classifier to classify the defect information and update the developer's programming level portrait. Experimental results show that it is effective in predicting defect code information. In addition, a comparative study with the state-of-the-art method shows that the method is superior to existing methods in terms of classification rate, recall, precision and F-measure.","tags":[],"title":"User Portrait Technology Based on Stacking Mode","type":"publication"},{"authors":["巫义锐","Yukai Ding","Jun Feng"],"categories":null,"content":"","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1586908800,"objectID":"c1cde949a0cb518dae0e2e1a91103c72","permalink":"https://hhudelta.github.io/zh/publication/ejwcn2020/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/zh/publication/ejwcn2020/","section":"publication","summary":"With a significant development of big data analysis and cloud-fog-edge computing, human-centered computing (HCC) has been a hot research topic worldwide. Essentially, HCC is a cross-disciplinary research domain, in which the core idea is to build an efficient interaction among persons, cyber space, and real world. Inspired by the improvement of HCC on big data analysis, we intend to involve related core and technologies to help solve one of the most important issues in the real world, i.e., flood prediction. To minimize the negative impacts brought by floods, researchers pay special attention to improve the accuracy of flood forecasting with quantity of technologies including HCC. However, historical flood data is essentially imbalanced. Imbalanced data causes machine learning classifiers to be more biased towards patterns with majority samples, resulting in poor classification of pattern with minority samples. In this paper, we propose a novel Synthetic Minority Over-sampling Technique (SMOTE)-Boost-based sparse Bayesian model to perform flood prediction with both high accuracy and robustness. The proposed model consists of three modules, namely, SMOTE-based data enhancement, AdaBoost training strategy, and sparse Bayes model construction. In SMOTE-based data enhancement, we adopt a SMOTE algorithm to effectively cover diverse data modes and generate more samples for prediction pattern with minority samples, which greatly alleviates the problem of imbalanced data by involving experts’ analysis and users’ intentions. During AdaBoost training strategy, we propose a specifically designed AdaBoost training strategy for sparse Bayesian model, which not only adaptively and incrementally increases prediction ability of Bayesian model, but also prevents its overfitting performance. Essentially, the design of AdaBoost strategy helps keep balance between prediction ability and model complexity, which offers different but effective models over diverse rivers and users. Finally, we construct a sparse Bayesian model based on AdaBoost training strategy, which could offer flood prediction results with high rationality and robustness. We demonstrate the accuracy and effectiveness of the proposed model for flood prediction by conducting experiments on a collected dataset with several comparative methods.","tags":[],"title":"SMOTE-Boost-based sparse Bayesian model for flood prediction","type":"publication"},{"authors":["巫义锐","Haohang Wang","Dabao Wei","Jun Feng"],"categories":null,"content":"","date":1584230400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1584230400,"objectID":"da5e7abb9dff40e39342cae64c8869c5","permalink":"https://hhudelta.github.io/zh/publication/hhu2020/","publishdate":"2020-03-15T00:00:00Z","relpermalink":"/zh/publication/hhu2020/","section":"publication","summary":"通过构建流域时空场景表征水文事件，提出一种创新的时空多特征流域场景模式库构建方法。对水文原始数据进行事件化分割，去除场景要素数据的时空冗余;基于要素关联关系分析，以多类型方法构造场景要素的对应特征;通过特征选择算法，选取场景关键特征，实现场景初始化;以初始化场景为特征空间，通过聚类提取场景模式，完成场景模式库构建。试验结果表明，创新的时空多特征流域场景模式库构建方法能高效提取水文事件中关键的时空场景数据，挖掘场景模式，形成场景模式库，可以为小样本条件下的水文事件预测提供准确高效的结果。","tags":[],"title":"时空多特征流域场景模式库构建方法","type":"publication"},{"authors":["巫义锐","Dabao Wei","Jun Feng","Xiaolong Xu"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1577836800,"objectID":"5a64e49a79fe72d799712d2d672b21f5","permalink":"https://hhudelta.github.io/zh/publication/scn2020/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/zh/publication/scn2020/","section":"publication","summary":"With the development of the fifth-generation networks and artificial intelligence technologies, new threats and challenges have emerged to wireless communication systems, especially in cybersecurity. In this paper, we offer a review on attack detection methods involving deep learning techniques. Specifically, we first summarize fundamental problems of network security and attack detection and introduce several successful related applications using deep learning structures. We categorize deep learning methods and focus on attack detection methods built on different kinds of architectures, such as autoencoders, generative adversarial networks, recurrent neural networks, and convolutional neural networks. We also present some benchmark datasets and compare the performance of different approaches to show the current working state of attack detection methods using deep learning structures. Finally, we summarize the paper and discuss ways to improve the performance of attack detection with deep learning techniques.","tags":[],"title":"Network Attacks Detection Methods Based on Deep Learning Techniques: A Survey","type":"publication"},{"authors":["Xiaoge Song","巫义锐","Wenhai Wang","Tong Lu"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1577836800,"objectID":"cde2811c694c220fc9822ddf3ac793a8","permalink":"https://hhudelta.github.io/zh/publication/mmm2020-1/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/zh/publication/mmm2020-1/","section":"publication","summary":"Benefit from the development of deep neural networks, scene text detectors have progressed rapidly over the past few years and achieved outstanding performance on several standard benchmarks. However, most existing methods adopt quadrilateral bounding boxes to represent texts, which are usually inadequate to deal with multi-shaped texts such as the curved ones. To keep consist detection performance on both quadrilateral and curved texts, we present a novel representation, i.e., text kernel, for multi-shaped texts. On the basis of text kernel, we propose a simple yet effective scene text detection method, named as TK-Text. The proposed method consists of three steps, namely text-context-aware network, segmentation map generation and text kernel based post-clustering. During text-context-aware network, we construct a segmentation-based network to extract feature map from natural scene images, which are further enhanced with text context information extracted from an attention scheme TKAB. In segmentation map generation, text kernels and rough boundaries of text instances are segmented based on the enhanced feature map. Finally, rough text instances are gradually refined to generate accurate text instances by performing clustering based on text kernel. Experiments on public benchmarks including SCUT-CTW1500, ICDAR 2015 and ICDAR 2017 MLT demonstrate that the proposed method achieves competitive detection performance comparing with the existing methods.","tags":[],"title":"TK-Text: Multi-shaped Scene Text Detection via Instance Segmentation","type":"publication"},{"authors":["Xiaozhong Ji","巫义锐","Tong Lu"],"categories":null,"content":"","date":1577145600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1577145600,"objectID":"cfe5914d59400a327dc7b2cd7190d20c","permalink":"https://hhudelta.github.io/zh/publication/mmm2020/","publishdate":"2019-12-24T00:00:00Z","relpermalink":"/zh/publication/mmm2020/","section":"publication","summary":"Deep learning models have achieved significant success in various vision-based applications. However, directly applying deep architectures for single image super-resolution (SISR) results in poor visual effects, such as blurry patches and loss of details, primarily because low-frequency information is treated ambiguously across different patches and channels. To address this issue, we propose a novel context-aware deep residual network with promotion gates, named G-CASR, for SISR. The G-CASR network consists of a sequence of G-CASR modules designed to transform low-resolution features into high-informative features. Each module incorporates a dual-attention residual block (DRB) that captures rich and varying context information through spatial and channel attention. A promotion gate (PG) is applied in each module to analyze the inherent characteristics of input data, enhancing contributive information while suppressing irrelevant data. Experiments on five public datasets (Set5, Set14, B100, Urban100, and Manga109) show that G-CASR outperforms recent methods like SRCNN, VDSR, lapSRN, and EDSR with an average improvement of 1.112 for PSNR and 0.0255 for SSIM. Additionally, the G-CASR model requires only about 25% of the memory cost compared to EDSR.","tags":[],"title":"Context-Aware Residual Network with Promotion Gates for Single Image Super-Resolution","type":"publication"},{"authors":["Yao Xiao","Minglong Xue","Tong Lu","巫义锐","Shivakumara Palaiahnakote"],"categories":null,"content":"","date":1567296e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1567296e3,"objectID":"d7f93e6d1bc04f06a03ea039eabb9847","permalink":"https://hhudelta.github.io/zh/publication/icdar2019/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/zh/publication/icdar2019/","section":"publication","summary":"The existing deep learning based state-of-theart scene text detection methods treat scene texts a type of general objects, or segment text regions directly. The latter category achieves remarkable detection results on arbitrary orientation and large aspect ratios of scene texts based on instance segmentation algorithms. However, due to the lack of context information with consideration of scene text unique characteristics, directly applying instance segmentation to text detection task is prone to result in low accuracy, especially producing false positive detection results. To ease this problem, we propose a novel text-context-aware scene text detection CNN structure, which appropriately encodes channel and spatial attention information to construct context-aware and discriminative feature map for multi-oriented and multi-language text detection tasks. With high representation ability of text context-aware feature map, the proposed instance segmentation based method can not only robustly detect multi-oriented and multi-language text from natural scene images, but also produce better text detection results by greatly reducing false positives. Experiments on ICDAR2015 and ICDAR2017-MLT datasets show that the proposed method has achieved superior performances in precision, recall and F-measure than most of the existing studies.","tags":[],"title":"A Text-Context-Aware CNN Network for Multi-oriented and Multi-language Scene Text Detection","type":"publication"},{"authors":["巫义锐","Yukai Ding","Jun Feng"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1561939200,"objectID":"7eb589a326edb0dec82c153f3c2c1ee9","permalink":"https://hhudelta.github.io/zh/publication/cpscom2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/zh/publication/cpscom2019/","section":"publication","summary":"Flood is a common disaster in our daily life. It's of great significance to improve the accuracy of flood forecasting, in order to help get rid of loss in both lives and property. However, there exists a uneven distribution of samples in factors of flood forecasting. Therefore, it's difficult to train a single datadriven model to describe the entire complex process of flood generation. In this paper, we propose a novel SMOTEBoost algorithm to perform flood forecasting with both high accuracy and robustness. Specifically, we firstly adopt a SMOTE algorithm to generate virtual samples, which greatly alleviates the problem of uneven sample distribution. Afterwards, we propose a sparse Bayesian model, which is trained with AdaBoost training strategy by improving its performance in over-fitting. At last, we carry out experiments on flood foretasting in Changhua river, which shows that the proposed method achieves high accuracy in prediction, thus owing practical usage.","tags":[],"title":"Sparse Bayesian Flood Forecasting Model Based on SMOTEBoost","type":"publication"},{"authors":["Yukai Ding","Yuelong Zhu","巫义锐","Jun Feng","Zirun Cheng"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1561939200,"objectID":"d146c0c72492168c6f543daf2a594845","permalink":"https://hhudelta.github.io/zh/publication/cpscom2019-1/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/zh/publication/cpscom2019-1/","section":"publication","summary":"In order to reduce the loss caused by flood, a large number of researches based on data, algorithms, machine learning and other technical means are used to realize flood forecasting. It will be a kind of flexible research method to realize the flood prediction of small and medium-sized rivers through intelligent models such as neural network. The area of small and medium-sized river basins is relatively small. Precipitation, soil moisture, evaporation and other factors can affect the timely total runoff prediction. However, not all the hydrological features is always valuable for flood forecasting, even at some time, noise of the factors will have larger interference on forecast process. Therefore, dynamic extraction of key feature vectors from various hydrological information plays an important role in flood forecasting. This paper proposed a flood forecasting model (STA-LSTM model) by using long short-term memory model (LSTM) and attention mechanism. We take the Lech river basin in Europe as the experimental basin and the results show that STA-LSTM performs well and has high research value with comparison of support vector machine (SVM), fully connected network (FCN) and original LSTM","tags":[],"title":"Spatio-Temporal Attention LSTM Model for Flood Forecasting","type":"publication"},{"authors":["巫义锐","Yuechao He","Palaiahnakote Shivakumara","Ziming Li","Hongxin Guo","Tong Lu"],"categories":null,"content":"","date":1559260800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1559260800,"objectID":"190f989a14bc356ae4606f091cf69ae6","permalink":"https://hhudelta.github.io/zh/publication/cit2019/","publishdate":"2019-05-31T00:00:00Z","relpermalink":"/zh/publication/cit2019/","section":"publication","summary":"Due to natural disaster and global warning, one can expect unexpected fire, which causes panic among people and extent to death. To reduce the impact of fire, the authors propose a new method for predicting and rating fire in video through deep-learning models in this work such that rescue team can save lives of people. The proposed method explores a hybrid deep convolutional neural network, which involves motion detection and maximally stable extremal region for detecting and rating fire in video. Further, the authors propose to use a channel-wise attention mechanism of the deep neural network for detecting rating of fire level. Experimental results on a large dataset show the proposed method outperforms the existing methods for detecting and rating fire in video.","tags":[],"title":"Channel-wise attention model-based fire and rating level detection in video","type":"publication"},{"authors":["巫义锐","Lianglei Wei","Yucong Duan"],"categories":null,"content":"","date":1555891200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1555891200,"objectID":"e9afaf0bf013cd56464c7ac9a9cf6490","permalink":"https://hhudelta.github.io/zh/publication/ci2019/","publishdate":"2019-04-22T00:00:00Z","relpermalink":"/zh/publication/ci2019/","section":"publication","summary":"With the rapid development of RGB-D cameras and pose estimation techniques, action recognition based on three-dimensional skeleton data has gained significant attention in the artificial intelligence community. In this paper, we incorporate temporal pattern descriptors of joint positions with the currently popular long short-term memory (LSTM)-based learning scheme to obtain accurate and robust action recognition. Considering that actions are essentially formed by small subactions, we first utilize a two-dimensional wavelet transform to extract temporal pattern descriptors in the frequency domain for each subaction. Afterward, we design a novel LSTM structure to extract deep features, which model a long-term spatiotemporal correlation between body parts. Since temporal pattern descriptors and LSTM deep features can be regarded as multimodal representations for actions, we fuse them with an autoencoder network to achieve a more effective feature descriptor for action recognition. Experimental results on three challenging data sets with several comparative methods demonstrate the effectiveness of the proposed method for three-dimensional action recognition.","tags":[],"title":"Deep spatiotemporal LSTM network with temporal pattern feature for 3D human action recognition","type":"publication"},{"authors":["Yisheng Yue","Palaiahnakote Shivakumara","巫义锐","Liping Zhu","Tong Lu","Umapada Pal"],"categories":null,"content":"","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1546905600,"objectID":"9d3a4b67775a4b0a672d61850a19a90f","permalink":"https://hhudelta.github.io/zh/publication/mmm2019/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/zh/publication/mmm2019/","section":"publication","summary":"Due to the introduction of deep learning for text detection and recognition in natural scenes, and the increase in detecting fake images in crime applications, automatically generating fake character images has now received greater attentions. This paper presents a new system named Fake Character GAN (FCGAN). It has the ability to generate fake and artificial scene characters that have similar shapes and colors with the existing ones. The proposed method first extracts shapes and colors of character images. Then, it constructs the FCGAN, which consists of a series of convolution, residual and transposed convolution blocks. The extracted features are then fed to the FCGAN to generate fake characters and verify the quality of the generated characters simultaneously. The proposed system chooses characters from the benchmark ICDAR 2015 dataset for training, and further validated by conducting text detection and recognition experiments on input and generated fake images to show its effectiveness.","tags":[],"title":"An Automatic System for Generating Artificial Fake Character Images","type":"publication"},{"authors":["巫义锐","Weigang Xu","Qinghan Yu","Jun Feng","Tong Lu"],"categories":null,"content":"","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1546905600,"objectID":"3bffa3422650abd8b0ddee955dd60c17","permalink":"https://hhudelta.github.io/zh/publication/mmm2019-1/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/zh/publication/mmm2019-1/","section":"publication","summary":"To minimize the negative impacts brought by floods, researchers pay special attention to the problem of flood prediction. In this paper, we propose a hierarchical Bayesian network based incremental model to predict floods for small rivers. The proposed model not only appropriately embeds hydrology expert knowledge with Bayesian network for high rationality and robustness, but also designs an incremental learning scheme to improve the self-improving and adaptive ability of the proposed model. Following the idea of a famous hydrology model, i.e., XAJ model, we firstly present the construction of hierarchical Bayesian network as local and global network construction. After that, we propose an incremental learning scheme, which selects proper incremental data to improve the completeness of prior knowledge and updates parameters of Bayesian network to prevent training from scratch. We demonstrate the accuracy and effectiveness of the proposed model by conducting experiments on a collected dataset with one comparative method.","tags":[],"title":"Hierarchical Bayesian Network Based Incremental Model for Flood Prediction","type":"publication"},{"authors":["巫义锐","Zhouyu Meng","Palaiahnakote Shivakumara","Tong Lu"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1541030400,"objectID":"732dbe3e3a4340fe99bd9fb21c697709","permalink":"https://hhudelta.github.io/zh/publication/mjcs2018/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/zh/publication/mjcs2018/","section":"publication","summary":"Deep neural networks (DNN) have shown significant performance in several domains including computer vision and machine learning. Convolutional Neural Networks (CNN), known as a particular type of DNN, have shown their promising potentials in discovering vision-based patterns from quantity of labeled images. Many CNN-based algorithms are thus proposed to solve the problem of object detection and object recognition. However, CNN-based systems are hard to deploy on embedded systems due to their computationally and storage intensive. In this paper, we propose a method to compress convolutional neural network to decrease its computation and storage cost by exploiting inherent redundancy property of parameters in different kinds of layers of CNN architecture. During the compression, we firstly construct parameter matrices from different kinds of layers and convert parameter matrices to frequency domain through discrete cosine transform (DCT). Due to the smooth property of parameters when processing images, the resulting frequency matrices are dominated by low-frequency components. We thus prune high-frequency part to emphasize the dominating part of frequency matrix and make the frequency matrix sparse. Then, the sparse frequency matrices are sampled with distributed random Gaussian matrix under the guiding of compress sensing. Finally, we retrain the network with the sampling matrices to fine-tune the remaining parameters. We evaluate the proposed method on several typical convolutional neural network and show it outperforms one latest compression approach.","tags":[],"title":"Compressive Sensing Based Convolutional Neural Network for Object Detection","type":"publication"},{"authors":["巫义锐","Yisheng Yue","Xiao Tan","Wei Wang","Tong Lu"],"categories":null,"content":"","date":1538352e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1538352e3,"objectID":"cdf525cb4a65e64a6bd23cb00da5e5a0","permalink":"https://hhudelta.github.io/zh/publication/icip2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/zh/publication/icip2018/","section":"publication","summary":"Classifying human chromosomes from input cell images, i.e., karyotyping, requires domain expertise and quantity of manual effort to perform. In this paper, we propose an end-to-end chromosome karyotyping method, which can automatically detect, segment and classify chromosomes from cell images. During detection, we explore Extremal Regions (ER) to obtain chromosome candidates in input images. During segmentation, we segment overlapping chromosome candidates by approximating chromosome shapes with eclipses. In classification, we first propose Multiple Distribution Generative Advertising Network (MD-GAN) to effectively cover diverse data modes and generate more labeled samples for data augmentation. Then, we finetune pre-trained convolutional neural network (CNN) to classify chromosomes with samples generated by MD-GAN. We demonstrate the accuracy of the proposed end-to-end method in detecting, segmenting and classifying by experiments on a self-collected dataset. Experiments also prove data augmentation with MD-GAN could improve classification performance of CNN.","tags":[],"title":"End-To-End Chromosome Karyotyping with Data Augmentation Using GAN","type":"publication"},{"authors":["Zhaoyang Liu","巫义锐","Yukai Ding","Jun Feng","Tong Lu"],"categories":null,"content":"","date":1537488e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1537488e3,"objectID":"fbb3bc2319a953139bb1b3ed9905fe54","permalink":"https://hhudelta.github.io/zh/publication/pcm2018/","publishdate":"2018-09-21T00:00:00Z","relpermalink":"/zh/publication/pcm2018/","section":"publication","summary":"To minimize damages brought by floods, researchers pay special attentions to solve the problem of flood prediction. Multiple factors, including rainfall, soil category, the structure of riverway and so on, affect the prediction of sequential flow rate values, but factors are not always informative for flood prediction. Extracting discriminative and informative features thus plays a key role in predicting flow rates. In this paper, we propose a context and temporal aware attention model for flood prediction based on a quantity of collected flood factors. We build our model on top of Long Short-Term Memory (LSTM) networks, which selectively focuses on informative factors and pays different levels of attentions to the outputs of different cells. The proposed CT-LSTM network assigns time-varying weights to input factors at all the cells of LSTM network, and allocates temporal-dependent weights to the outputs of each LSTM cell for boosting prediction performance. Experimental results on a benchmark flood dataset with several comparative methods demonstrate the effectiveness of the proposed CT-LSTM network for flood prediction.","tags":[],"title":"Context and Temporal Aware Attention Model for Flood Prediction","type":"publication"},{"authors":["巫义锐","Zhaoyang Liu","Weigang Xu","Jun Feng","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1533081600,"objectID":"764ac07cde336d6869a2ee8b007693ec","permalink":"https://hhudelta.github.io/zh/publication/icpr2018-1/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/zh/publication/icpr2018-1/","section":"publication","summary":"To minimize the negative impacts brought by floods, researchers from pattern recognition community utilize artificial intelligence based methods to solve the problem of flood prediction. Inspired by the significant power of Long Short-Term Memory (LSTM) networks in modeling the dynamics and dependencies of sequential data, we intend to utilize LSTM networks to predict sequential flow rate values based on a set of collected flood factors. Since not all factors are informative for flood prediction and the irrelevant factors often bring a lot of noise, we need to pay more attention to the informative ones. However, original LSTM doesn't have strong attention capability. Hence we propose an context-aware attention LSTM (CA-LSTM) network for flood prediction, which is capable to selectively focus on informative factors. During training, the local context-aware attention model is constructed by learning probability distributions between flow rate and hidden output of each LSTM cell. During testing, the learned local attention model assign weights to adjust relations between input factors and predictions at all steps of LSTM network. We conduct experiments on a flood dataset with several comparative methods to demonstrate high accuracy of the proposed method and the effectiveness of the proposed context-aware attention model.","tags":[],"title":"Context-Aware Attention LSTM Network for Flood Prediction","type":"publication"},{"authors":["巫义锐","Zhikai Li","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1533081600,"objectID":"35c690d4538e4497c4d088af246c12ad","permalink":"https://hhudelta.github.io/zh/publication/icpr2018/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/zh/publication/icpr2018/","section":"publication","summary":"Simultaneous Localization and Mapping (SLAM) is difficult to deploy in the embedded systems due to its high computation cost and stable input requirements. Building on excellent algorithms of recent years, we present Em-SLAM, a monocular SLAM method which is fast and robust in the embedded system. We present Em-SLAM in three stages comprising initial pose estimation, iterative pose optimization and correspondences, and mapping with nearest frame queue. During the first stage, we perform stable initial pose estimation based on the matched ORB features extracted around the selected key points. Regarding initial pose and corresponding key points as input, the second stage of Em-SLAM iteratively optimizes these inputs values by tracking key points in the new frames. At the last stage, we firstly determine keyframes with the help of the proposed nearest frame queue and then design a greedy search algorithm to find matched ORB features between keyframes, which are adopted for compact and robust map reconstruction. Due to the special designs for the embedded systems, Em-SLAM demonstrates a high accurate and fast performance on the embedded system for all SLAM tasks: tracking, mapping and loop closing. We evaluate Em-SLAM on he most popular datasets by comparing with one latest SLAM method.","tags":[],"title":"Em-SLAM: a Fast and Robust Monocular SLAM Method for Embedded Systems","type":"publication"},{"authors":["巫义锐","Weigang Xu","Jun Feng","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1533081600,"objectID":"f01e4b65defe22116a7e27ccaeaa3972","permalink":"https://hhudelta.github.io/zh/publication/icpr2018-2/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/zh/publication/icpr2018-2/","section":"publication","summary":"To minimize the negative impacts brought by floods, researchers from pattern recognition community pay special attention to the problem of flood prediction by involving technologies of machine learning. In this paper, we propose to construct hierarchical Bayesian network to predict floods for small rivers, which appropriately embed hydrology expert knowledge for high rationality and robustness. We present the construction of the hierarchical Bayesian network in two stages comprising local and global network construction. During the local network construction, we firstly divide the river watershed into small local regions. Following the idea of a famous hydrology model - the Xinanjiang model, we establish the entities and connections of the local Bayesian network to represent the variables and physical processes of the Xinanjiang model, respectively. During the global network construction, intermediate variables for local regions, computed by the local Bayesian network, are coupled to offer an estimation for time-varying values of flow rate by proper inferences of the global network. At last, we propose to improve the output of Bayesian network by utilizing former flow rate values. We demonstrate the accuracy and robustness of the proposed method by conducting experiments on a collected dataset with several comparative methods.","tags":[],"title":"Local and Global Bayesian Network based Model for Flood Prediction","type":"publication"},{"authors":["Sauradip Nag","Palaiahnakote Shivakumara","巫义锐","Umapada Pal","Tong Lu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1533081600,"objectID":"95887dd24cb44ba2c915ca884968ff72","permalink":"https://hhudelta.github.io/zh/publication/icfhr2018/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/zh/publication/icfhr2018/","section":"publication","summary":"Identifying crime for forensic investigating teams when crimes involve people of different nationals is challenging. This paper proposes a new method for ethnicity (nationality) identification based on Cloud of Line Distribution (COLD) features of handwriting components. The proposed method, at first, uses tangent angle of the contour pixels in each row and the mean of intensity values of each row for segmenting text lines. For segmented text lines, we use tangent angle and direction of base lines to remove rule lines in the image. We use polygonal approximation for finding dominant points for contours of edge components. Then the proposed method connects the nearest dominant points of every dominant point, which results in line segments of dominant point pairs. For each line segment, the proposed method estimates angle and length, which gives a point in polar domain. For all the line segments, the proposed method generates dense points in polar domain, which results in COLD distribution. As character component shapes change, according to nationals, the shape of the distribution changes. This observation is extracted based on distance from pixels of distribution to Principal Axis of the distribution. Then the features are subjected to an SVM classifier for identifying nationals. Experiments are conducted on a complex dataset, which show the proposed method is effective and outperforms the existing method.","tags":[],"title":"New COLD Feature Based Handwriting Analysis for Ethnicity/Nationality Identification","type":"publication"},{"authors":["Lianglei Wei","巫义锐","Wenhai Wang","Tong Lu"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1514764800,"objectID":"3067d7dd8a4c25351b6ff887fa2b5933","permalink":"https://hhudelta.github.io/zh/publication/mmm2018/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/zh/publication/mmm2018/","section":"publication","summary":"Understanding the meanings of human actions from 3D skeleton data embedded videos is a new challenge in content-oriented video analysis. In this paper, we propose to incorporate temporal patterns of joint positions with currently popular Long Short-Term Memory (LSTM) based learning to improve both accuracy and robustness. Regarding 3D actions are formed by sub-actions, we first propose Wavelet Temporal Pattern (WTP) to extract representations of temporal patterns for each sub-action by wavelet transform. Then, we define a novel Relation-aware LSTM (R-LSTM) structure to extract features by modeling the long-term spatio-temporal correlation between body parts. Regarding WTP and R-LSTM features as heterogeneous representations for human actions, we next fuse WTP and R-LSTM features by an AutoEncoder network to define a more effective action descriptor for classification. The experimental results on a large scale challenging dataset NTU-RGB+D and several other datasets consisting of UT-Kinect and Florence 3D actions for 3D human action analysis demonstrate the effectiveness of the proposed method.","tags":[],"title":"A Novel 3D Human Action Recognition Framework for Video Content Analysis","type":"publication"},{"authors":["Wenhai Wang","巫义锐","Palaiahnakote Shivakumara","Tong Lu"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1514764800,"objectID":"34fa680d86dc016bb0432b3fc655999a","permalink":"https://hhudelta.github.io/zh/publication/mmm2018-1/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/zh/publication/mmm2018-1/","section":"publication","summary":"Text detection in natural and video scene images is still considered to be challenging due to unpredictable nature of scene texts. This paper presents a new method based on Cloud of Line Distribution (COLD) and Random Forest Classifier for text detection in both natural and video images. The proposed method extracts unique shapes of text components by studying the relationship between dominant points such as straight or cursive over contours of text components, which is called COLD in polar domain. We consider edge components as text candidates if the edge components in Canny and Sobel of an input image share the COLD property. For each text candidate, we further study its COLD distribution at component level to extract statistical features and angle oriented features. Next, these features are fed to a random forest classifier to eliminate false text candidates, which results representatives. We then perform grouping using representatives to form text lines based on the distances between edge components in the edge image. The statistical and angle orientated features are finally extracted at word level for eliminating false positives, which results in text detection. The proposed method is tested on standard database, namely, SVT, ICDAR 2015 scene, ICDAR2013 scene and video databases, to show its effectiveness and usefulness compared with the existing methods.","tags":[],"title":"Cloud of Line Distribution and Random Forest Based Text Detection from Natural/Video Scene Images","type":"publication"},{"authors":["巫义锐","Wenhai Wang","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1509494400,"objectID":"0f50cd26e42d1a1fc8f2288b70c1ea2b","permalink":"https://hhudelta.github.io/zh/publication/icdar2017/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/zh/publication/icdar2017/","section":"publication","summary":"Text detection in video/scene images has gained a significant attention in the field of image processing and document analysis due to the inherent challenges caused by variations in contrast, orientation, background, text type, font type, non-uniform illumination and so on. In this paper, we propose a novel text detection method to explore symmetry property and appearance features of text for improved accuracy and robustness. First, the proposed method explores Extremal Regions (ER) for detecting text candidates in images. Then we propose a novel feature named as Multi-domain Strokes Symmetry Histogram (MSSH) for each text candidate, which describes the inherent symmetry property of stroke pixel pairs in gray, gradient and frequency domains. Furthermore, deep convolutional features are extracted to describe the appearance for each text candidate. We further fuse them by Auto-Encoder network to define a more discriminative text descriptor for classification. Finally, the proposed method constructs text lines based on the classification results. We demonstrate the effectiveness and robustness detection results of our proposed method by testing on four different benchmark databases.","tags":[],"title":"A Robust Symmetry-Based Method for Scene/Video Text Detection through Neural Network","type":"publication"},{"authors":["巫义锐","Zhouyu Meng","Shivakumara Palaiahnakote","Tong Lu"],"categories":null,"content":"","date":1506816e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1506816e3,"objectID":"4bd1fa6a6616c9ad1cabaad29ac473b2","permalink":"https://hhudelta.github.io/zh/publication/acpr2017/","publishdate":"2017-10-01T00:00:00Z","relpermalink":"/zh/publication/acpr2017/","section":"publication","summary":"Object detection is one of the fundamental challenges in pattern recognition community. Recently, convolutional neural networks (CNN) are increasingly exploited in object detection, showing their promising potentials of generatively discovering patterns from quantity of labeled images. Among CNN-based systems, we focus on one state-of-the-art architecture designed for fast object detection, named as YOLO. However, YOLO, as well as CNN-based systems are hard to deploy on embedded systems due to their computationally and storage intensive. In this paper, we propose to compress YOLO network by compressive sensing, which exploits in-herent redundancy property of parameters in layers of CNN architecture, leading to decrease the computation and storage cost. We firstly convert parameter matrix to frequency domain through discrete cosine transform (DCT). Due to the smooth property of parameters when processing images, the resulting frequency matrix are dominated by low-frequency components. Next, we prune high-frequency part to make the frequency matrix sparse. After pruning, we sample the frequency matrix with distributed random Gaussian matrix. Finally, we retrain the network to finetune the remaining parameters. We evaluate the proposed compress method on VOC 2012 dataset and show it outperforms one latest compression approach.","tags":[],"title":"Compressing YOLO Network by Compressive Sensing","type":"publication"},{"authors":["Hengduo Li","Jun Liu","Guyue Zhang","Yuan Gao","巫义锐"],"categories":null,"content":"","date":1504224e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1504224e3,"objectID":"523b39e36411e572fa44babb31179f51","permalink":"https://hhudelta.github.io/zh/publication/icip2017/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/zh/publication/icip2017/","section":"publication","summary":"With the development of depth cameras such as Kinect and Intel Realsense, RGB-D based human detection receives continuous research attention due to its usage in a variety of applications. In this paper, we propose a new Multi-Glimpse LSTM (MG-LSTM) network, in which multi-scale contextual information is sequentially integrated to promote the human detection performance. Furthermore, we propose a feature fusion strategy based on our MG-LSTM network to better incorporate the RGB and depth information. To the best of our knowledge, this is the first attempt to utilize LSTM structure for RGB-D based human detection. Our method achieves superior performance on two publicly available datasets.","tags":[],"title":"Multi-glimpse LSTM with color-depth feature fusion for human detection","type":"publication"},{"authors":["袁俐新","ShiJin Li","YaPing Jiang"],"categories":null,"content":"","date":1504224e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1504224e3,"objectID":"36c750cbcd5cfabbb29838c1c819b27f","permalink":"https://hhudelta.github.io/zh/publication/yuan-remote-2017/","publishdate":"2024-12-18T16:43:54.920255Z","relpermalink":"/zh/publication/yuan-remote-2017/","section":"publication","summary":"Due to the high intraclass variability and the low interclass disparity in high-resolution remote sensing (RS) image scenes, high-resolution RS scene classification is a challenging task. The performance of scene classification not only relies on discriminative feature representation but also needs appropriate classification strategies. In this paper, a scene preclassification strategy based on unsupervised learning is proposed, which divides scenes into two groups. The division method called dynamic-sphere division method (DSDM) is based on a dynamic-sphere division and an inside-sphere membership assessment. For the group with lower membership, after introducing the spatial location and scale of the scale invariant feature transformation (SIFT) descriptor to form a transaction, frequent itemset mining and an improved feature selection criterion are implemented to reduce the redundancy from the aspects of feature quantity and feature dimension, and a more discriminative structural feature histogram FMS-hist is finally obtained. Both the radius of the dynamic-sphere and the final optimal feature dimension are automatically selected according to the inflection point of the corresponding curves. Experimental results based on two representative data sets show that the proposed DSDM can select the suitable group, the proposed FMS-hist is superior to the bag-of-SIFT-based models. The holistic procedure can further enhance the scene classification accuracy.","tags":["Feature extraction","Dynamic-sphere division method with membership (DSDM)","Earth","feature mining (FM)","feature selection (FS)","Histograms","Image color analysis","Itemsets","preclassification","Remote sensing","scene classification","Visualization"],"title":"Remote Sensing Scene Classification Using a Preclassification Strategy and an Improved Structural Feature","type":"publication"},{"authors":["巫义锐","Tong Lu","Zehuan Yuan","Hao Wang."],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1483228800,"objectID":"0f68fafc00711a4e47195e056b6a1598","permalink":"https://hhudelta.github.io/zh/publication/tmm2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/zh/publication/tmm2017/","section":"publication","summary":"Sculpture design is challenging due to its inherent difficulty in characterizing artworks quantitatively; thus, few works have been done to assist sculpture design in the past decades in the multimedia community. We have cooperated with several sculptors on analyzing styles of different artists consisting of Giacometti, Augeuste Rodin, Henry Moore, and Marino Marini from which we find pose editing plays an important role in sculpture design. Motivated by this, we present a novel platform that allows sculptors to edit virtual three-dimensional (3-D) sculptures by a free way. The proposed platform consists of three modules, namely, sculpture initialization, sculptor-sculpture mapping, and interactive pose editing. In sculpture initialization, a virtual 3-D sculpture is first incrementally reconstructed from multiview images. Then, we define Laplace operator and its corresponding spectrum to describe the geometry information of the reconstructed sculpture. During sculptor–sculpture mapping, we apply spectral analysis on the low-frequency parts of the spectrum to search for candidate editing points on the surface of the sculpture. Next, body actions of the sculptor are captured by Kinect and further mapped onto editing points as a predefined configuration set. Finally, during interactive pose editing, a real-time Kinect-driven sculpture pose editing scheme is presented, which not only preserves geometry features of the sculpture but also allows instant changes of sculpture poses. We demonstrate that our platform successfully assists sculptors on real-time pose editing by comparing its performance with those of the existing sculpture assisting methods.","tags":[],"title":"FreeScup: A Novel Platform for Assisting Sculpture Pose Design","type":"publication"},{"authors":["Yiyang Zhou","Wenhai Wang","Wenjie Guan","巫义锐","Heng Lai","Tong Lu","Min Cai"],"categories":null,"content":"","date":1483142400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1483142400,"objectID":"064e8ea026f5512723157a4d18c6edb0","permalink":"https://hhudelta.github.io/zh/publication/mmm2017/","publishdate":"2016-12-31T00:00:00Z","relpermalink":"/zh/publication/mmm2017/","section":"publication","summary":"In this paper, we present a novel framework to drive automatic robotic grasp by matching camera captured RGB-D data with 3D meshes, on which prior knowledge for grasp is pre-defined for each object type. The proposed framework consists of two modules, namely, pre-defining grasping knowledge for each type of object shape on 3D meshes, and automatic robotic grasping by matching RGB-D data with pre-defined 3D meshes. In the first module, we scan 3D meshes for typical object shapes and pre-define grasping regions for each 3D shape surface, which will be considered as the prior knowledge for guiding automatic robotic grasp. In the second module, for each RGB-D image captured by a depth camera, we recognize 2D shape of the object in it by an SVM classifier, and then segment it from background using depth data. Next, we propose a new algorithm to match the segmented RGB-D shape with predefined 3D meshes to guide robotic self-location and grasp by an automatic way. Our experimental results show that the proposed framework is particularly useful to guide camera based robotic grasp.","tags":[],"title":"Visual Robotic Object Grasping Through Combining RGB-D Data and 3D Meshes","type":"publication"},{"authors":["巫义锐","Palaiahnakote Shivakumara","Tong Lu","Chew Lim Tan","Michael Blumenstein","G. Hemantha Kumar"],"categories":null,"content":" ","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1480550400,"objectID":"d5576294330dc86bf5fe3a494b39e66a","permalink":"https://hhudelta.github.io/zh/publication/tip2016/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/zh/publication/tip2016/","section":"publication","summary":"Text recognition in video/natural scene images has gained significant attention in the field of image processing in many computer vision applications, which is much more challenging than recognition in plain background images. In this paper, we aim to restore complete character contours in video/scene images from gray values, in contrast to the conventional techniques that consider edge images/binary information as inputs for text detection and recognition. We explore and utilize the strengths of zero crossing points given by the Laplacian to identify stroke candidate pixels (SPC). For each SPC pair, we propose new symmetry features based on gradient magnitude and Fourier phase angles to identify probable stroke candidate pairs (PSCP). The same symmetry properties are proposed at the PSCP level to choose seed stroke candidate pairs (SSCP). Finally, an iterative algorithm is proposed for SSCP to restore complete character contours. Experimental results on benchmark databases, namely, the ICDAR family of video and natural scenes, Street View Data, and MSRA data sets, show that the proposed technique outperforms the existing techniques in terms of both quality measures and recognition rate. We also show that character contour restoration is effective for text detection in video and natural scene images.","tags":[],"title":"Contour Restoration of Text Components for Recognition in Video/Scene Images","type":"publication"},{"authors":["Wenhai Wang","巫义锐","Shivakumara Palaiahnakote","Tong Lu","Jun Liu"],"categories":null,"content":"","date":147528e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":147528e4,"objectID":"0064a6eb713e33b1d2f08070be96e530","permalink":"https://hhudelta.github.io/zh/publication/pcm2017/","publishdate":"2016-10-01T00:00:00Z","relpermalink":"/zh/publication/pcm2017/","section":"publication","summary":"Detecting arbitrary oriented text in scene and license plate images is challenging due to multiple adverse factors caused by images of diversified applications. This paper proposes a novel idea of extracting Cloud of Line Distribution (COLD) for the text candidates given by Extremal regions (ER). The features extracted by COLD are fed to Random forest to label character components. The character components are grouped according to probability distribution of nearest neighbor components. This results in text line. The proposed method is demonstrated on standard database of natural scene images, namely ICDAR 2015, video images, namely ICDAR 2015 and license plate databases. Experimental results and comparative study show that the proposed method outperforms the existing methods in terms of invariant to rotations, scripts and applications.","tags":[],"title":"Cloud of Line Distribution for Arbitrary Text Detection in Scene/Video/License Plate Images","type":"publication"},{"authors":["Zehuan Yuan","Tong Lu","巫义锐"],"categories":null,"content":"","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1462060800,"objectID":"beeb18bd7dc2b2d492e3ff3f09b1cd09","permalink":"https://hhudelta.github.io/zh/publication/ijcai2017/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/zh/publication/ijcai2017/","section":"publication","summary":"We address the problem of object co-segmentation in images. Object co-segmentation aims to segment common objects in images and has promising applications in AI agents. We solve it by proposing a co-occurrence map, which measures how likely an image region belongs to an object and also appears in other images. The co-occurrence map of an image is calculated by combining two parts: objectness scores of image regions and similarity evidences from object proposals across images. We introduce a deep-dense conditional random field framework to infer co-occurrence maps. Both similarity metric and objectness measure are learned end-to-end in a single deep network. We evaluate our method on two benchmarks and achieve competitive performance.","tags":[],"title":"Deep-dense Conditional Random Fields for Object Co-segmentation","type":"publication"},{"authors":["Shijin Li","Hui Yu","袁俐新"],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1459468800,"objectID":"efa3a947b82eae01c2b0c1e0d7344f4f","permalink":"https://hhudelta.github.io/zh/publication/li-novel-2016/","publishdate":"2024-12-18T16:43:55.0093Z","relpermalink":"/zh/publication/li-novel-2016/","section":"publication","summary":"With the development of remote sensing (RS) techniques, the amount of RS images increases dramatically. It is a challenge to utilize those RS big data efficiently. Content-based Image Retrieval (CBIR) is a typical approximate similarity search problem, which needs to establish an effective index structure to reduce the time of retrieval. By analyzing the limitations of commonly-used indexing mechanisms in the current CBIR system, we propose a novel scheme that dynamically combines vantage point tree (vp-tree) indexes to CBIR by using spacing-correlation strategies to determine the vantage points. Borrowing ideas from feature selection, we have also put forward a new measure to adaptively online select proper vp-tree indexing in different feature spaces, the distance-contrast-based indexing validity index (DCIVI). And we then employ vp-tree index structure in each feature space, which can properly describe the content of the RS image by the chosen features. Experimental results on various typical land covers retrieval validate that the proposed method is effective and not only is the response speed increased by 70 100 times, but also the retrieval quality (in terms of precision and recall) is improved.","tags":["Feature extraction","Image color analysis","Remote sensing","content-based image retrieval","feature selection","high-dimensional indexing structure","Image retrieval","Indexing","remote sensing","Time factors","vp-tree"],"title":"A Novel Approach to Remote Sensing Image Retrieval with Multi-feature VP-Tree Indexing and Online Feature Selection","type":"publication"},{"authors":["巫义锐","Xianli Zhou","Tong Lu","Guo Mei","Sun Linbi"],"categories":null,"content":"","date":1448928e3,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1448928e3,"objectID":"a8b793e7a05333d523526ef5affb99c7","permalink":"https://hhudelta.github.io/zh/publication/icpr2016/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/zh/publication/icpr2016/","section":"publication","summary":"Imitation cartoon drawing is an important skill for cartoonists, requiring quantity of efforts on practising and guidance. In this paper, we propose EvaToon, an imitated drawing evaluate system, which automatically assigns judging scores and marks improper drawing regions. With our system, cartoonists can practise and get guidance by themselves. We have cooperated with several experts on developing such an evaluation system. Based on their guide, we present EvaToon in two stages comprising cartoon drawings analyzing and similarity evaluating. During analyzing, we first locate contour pixels with high curvature as interest points and then extract multi-scale features around interest points to hierarchically describe shape. During evaluating, we first match interest points between original and imitated drawing based on distance of features. After matching, we construct a regression tree to map high dimensional difference of matching features to scores and marks based on quantity of manually evaluated training examples. Finally, our system matches an input imitated drawing with the original one and predicts its scores automatically. We demonstrate the accuracy of our EvaToon system in matching and predicting and prove the capability of describing shape of our proposed features by experiments on a collected dataset of imitated drawings.","tags":[],"title":"EvaToon: A novel graph matching system for evaluating cartoon drawings","type":"publication"},{"authors":["巫义锐","Oscar Kin-Chung Au","Chiew-Lan Tai","Tong Lu"],"categories":null,"content":"","date":1427155200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1427155200,"objectID":"882c4902f6846b507abb1e136de5d829","permalink":"https://hhudelta.github.io/zh/publication/cagd2015/","publishdate":"2015-03-24T00:00:00Z","relpermalink":"/zh/publication/cagd2015/","section":"publication","summary":"Most existing handle-based mesh deformation methods require costly re-computation for every handle set updating, namely, adding or removing of handles on the mesh surface. In this paper, we propose a reduced deformation model that is independent of handle configuration, allowing users to dynamically update the handle set without noticeable waiting time. We represent the deformation space of a mesh as propagation fields defined by only the mesh geometry, independent of the handle set. We define the propagation fields as selected eigenvectors of the Laplacian operator and adopt the transformations of isolines sampled from the fields as the deformation descriptors. In this way, the deformation descriptors are pre-computed before handle specification. During interactive manipulation, constraints generated from the handles are incorporated into the deformation system in real time. Our method therefore supports incremental mesh editing where the user can freely define different handle sets to edit different parts of the shape without waiting for long re-computation. Our reduced model is scalable since the updating time per iteration is independent of the mesh size and the number of handles. We demonstrate the effectiveness of the proposed deformation method and compare its performance with related reduced deformation models.","tags":[],"title":"HIRM: A Handle-Independent Reduced Model for Incremental Mesh Editing","type":"publication"},{"authors":["巫义锐","Tong Lu","Zehuan Yuan","Hao Wang."],"categories":null,"content":"","date":1412121600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1412121600,"objectID":"36673f9c10e09df223888458f5365f12","permalink":"https://hhudelta.github.io/zh/publication/icme2015/","publishdate":"2014-10-01T00:00:00Z","relpermalink":"/zh/publication/icme2015/","section":"publication","summary":"Sculpture design is challenging due to its inherent difficulty in characterizing an artwork quantitatively, and few works have been done to assist sculpture design. We present a novel platform to help sculptors in two stages, comprising automatic sculpture reconstruction and free spectral-based sculpture pose editing. During sculpture reconstruction, we co-segment a sculpture from real scene images of different views through a two-label MRF framework, aiming at performing sculpture reconstruction efficiently. During sculpture pose editing, we automatically extract candidate editing points on the sculpture by searching in the spectrums of Laplacian operator. After manually mapping body joints of a sculptor to particular editing points, we further construct a global Laplacian-based linear system by adopting the spectrums of Laplacian operator and using Kinect captured body motions for real time pose editing. The constructed system thus allows the sculptor to freely edit different kinds of sculpture artworks through Kinect. Experimental results demonstrate that our platform successfully assists sculptors in real-time pose editing.","tags":[],"title":"FreeScup: A novel platform for assisting sculpture pose design","type":"publication"},{"authors":["LiMin Wang","巫义锐","Tong Lu","Kang Chen"],"categories":null,"content":"","date":131976e4,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":131976e4,"objectID":"2d857c3af8385fe472d6e195cbc6f9f0","permalink":"https://hhudelta.github.io/zh/publication/acmmm2011/","publishdate":"2011-10-28T00:00:00Z","relpermalink":"/zh/publication/acmmm2011/","section":"publication","summary":"In this paper, we present a novel approach for multiclass object detection by combining local appearances and contextual constraints. We first construct a multiclass Hough forest of local patches, which can well deal with multiclass object deformations and local appearance variations, due to randomization and discrimination of the forest. Then, in the object hypothesis space, a new multiclass context model is proposed to capture relative location constraints, disambiguating appearance inputs in multiclass object detection. Finally, multiclass objects are detected with a greedy search algorithm efficiently. Experimental evaluations on two image data sets show that the combination of local appearances and context achieves state-of-the-art performance in multiclass object detection.","tags":[],"title":"Multiclass object detection by combining local appearances and context","type":"publication"}]